                                 Muldis::D
                                TODO DRAFT
---------------------------------------------------------------------------

This TODO_DRAFT file is an overflow destination of sorts for the TODO file,
and it contains a large amount of brainstorming, some verbose, in the guise
of TODO items, but much of this is highly unlikely to be done without major
changes, and in any event it is considered inappropriate for release
distributions due at least to its great rate of growth and low signal to
noise.  This TODO_DRAFT file only exists in the Muldis D version control
repository, not in the packaged Muldis D distributions on CPAN, so it is
still available for interested parties to read but doesn't waste the
resources of everyone else.

----------

* https://langserver.org

* https://en.wikipedia.org/wiki/Arithmetic_progression

* https://docs.raku.org/type/atomicint

* https://github.com/minimaxir/big-list-of-naughty-strings

* Updates 2017-8-25:
- Further clarify the definition of Bicessable, or add another concept
instead if needed, that it is possible for the type to have pairs of
values where no other value can be found that is ordered between the members
of that pair, and moreover, except for eg singleton types, every value of
the type is a member of such a pair, or exactly 2 such pairs which it isn't
the first/last value.  This is relevant for interval-set normalization;
you can merge 2 nearby intervals iff there is no value between the nearer
endpoints; otherwise you can't merge them.

* Updates 2017-8-24:
- Rename Foundation Types concept to Foundation Root Types or similar,
leaving Foundation Types to describe any types that the Foundation spec
has specific knowledge of.
- Add Foundation routine that returns the name of the most specific
Foundation type of a given value.  These are meant to be on the order of
specificity that they correspond 1:1 with Muldis Service Protocol API
types and also the specificity that it is useful for implementation internals
to know that they are of those specific types for optimized handling.
For example, "Text" would be the Foundation name of valid Text values,
which doesn't include all Capsule whose label is Text.
Typically user code should be invoking this routine rather than asking
what the label of a Capsule is for system-defined types.
This has the same format of answer for eg Integer as for Fraction,
eg \Integer or \@foundation::Integer.
- Corresponds to Well Known Types implementation concept.
- Something we want from this, but perhaps not exactly this, is the
conception of more types eg all MDP types being conceptually base types
but without the extra ones actually being opaque.

* Updated conceptions 2017-7-2:
- Merge Function_Call into Function and Procedure_Call into Procedure and
Alias into both.
- An Evaluates/Performs now takes 2 separate things each, a Function+Tuple
or Procedure+Tuple respectively, not a *Call that combines both;
the full-grammar generic call syntax will probably become an infix rather
than a prefix, or both, similar to "if", like this:
    apply f to (29,95)
Eg:
    apply (\Function : (applies : \@plus)) to (29,95)
Or:
    f applied_to (29,95)
Eg:
    plus::(29,95)
Also:
    perform p with (x,y);
Or:
    p performed_with (x,y);
Eg:
    p::(x,y);
- Simply invoking a routine by name still involves wrapping that call in
another function, but to avoid infinite regress or otherwise keep it simple
to call built-ins etc, a Function/Procedure will gain a trait specificly
for this, perhaps called "aliases" and used like "negates" or "commutes" etc;
moreover, the full grammar syntax like foo::(...) will produce this.
- The action of partially applying a function will take a Function as input
and yield another Function taking fewer arguments where typically the latter
simply calls the former giving it some arguments from its own arguments and
giving it some others as constants; this may also use a new Function trait
or two used in combination with "aliases".
- Ooh, oooooh, MAYBE name the trait "applies" rather than "aliases".
- See https://medium.com/javascript-scene/curry-or-partial-application-8150044c78b8 also.
- More generally, we want some way to declaratively distinguish between
arguments that, when a function is applied to a collection say, are unchanging
for all invocations versus ones that change per invocation; the former are
the ones typically applied to first, and potentially an implementation could
rewrite the function body to JIT constant-fold those unchanging ones away.
Supporting this is part of the reason for this refactoring, besides making
other things nicer.

* Updated conceptions 2017-6-17:
- The primary feature group that Foundation doesn't natively have and which
is bootstrapped in pure Muldis D, is operator overloading plus any other
features whose primary reason to exist is to support operator overloading.
- Foundation DOES recognize the Package type but minus a few features;
it also DOES recognize the Function and Procedure types minus a few features.
- Foundation does NOT recognize the "floating" Local_Name but does the
other kinds; any reference it understands can only resolve to exactly 1
thing, which is known at compile time without requring any search traversals.
- Foundation does NOT recognize the "is_generalization" trait of type definer
functions, nor "composes" nor "provides_default_for".
- Foundation does NOT recognize the "virtual" and "implements" traits of
regular functions and procedures.
- Foundation does NOT recognize declarations of interface type definers
nor references to interface type definers in eg routine signatures etc.
- What Foundation does not recognize involves either search traversals to
resolve what to actually call or apply, or it involves single actual
declarations that effectively declare a multiplicity of things, an example
being that a function defined with a signature of an interface type causes
a function for every composing type to also be declared in their namespaces.
- The Plain Text parser function built-in to the Foundation only supports the
"foundational" subset of the grammar and not the full grammar, and it does
not create any new decoration nodes reflecting relevant actual code formatting.
A main beneficiary of the operator overloading features that Foundation lacks
is the non-foundational parts of the Plain Text grammar such as infix operator
calls, or prefix etc, especially bare symbolic ones, and these are typically
overloaded operators.
- The System package mainly differentiates itself from the Foundation in that
it declares all the interface types used conceptually by the Foundation types
and formally declares the latter's composition of the former, and also System
adds aliases or shims to the Foundation operators that exploit overloading,
and fleshes out the complement of related operators that didn't make sense
to explicitly declare at the Foundation level to keep the quantities down.
- The System package generally doesn't add anything except shims or trivially
simple new operators while the meat of their functionality is in Foundation.
- Other Muldis D packages named (probably) System::Plain_Text and System::Executor
are written in the Plain Text foundational subset such that Executor is
more or less the "main program" that the .NET/Perl/etc Foundation implementations
run and Executor+Plain_Text in turn parse the "System" package which is written
in the full grammar as well as any user code.  Executor would implement the
aforementioned Muldis D features that the Foundation lacks, the stuff
centered around operator overloading and interface types, translating any
code using those things to equivalent code that doesn't, for Foundation to run.
Plain_Text would parse/generate the whole grammar and would/can generate
decoration nodes reflecting the exact source code syntax used.  Basically
the higher-level language tricky bits are written in Muldis D so they only
have to be written once across all .NET/Perl/etc and are easier for users
to change to or swap out for something else if they want.

* When implementing, start with the low-level foundational procedural stuff and then
work the way up to functions and types a bit later; this means we can
actually start execute and test things with the minimum implementation possible.

* For now, treat Raku like it is the only Perl; just co-develop for Raku
and C# generally ignoring Perl; later on, when the Raku version
works, it can be back-ported to Perl.  Generally speaking the new "MUSE"
and other works are aiming to get in on the ground floor while Raku
database stuff is fairly immature and has no defacto standard, so it can be
the defacto standard, and help boost the fortunes of Raku in general.

* So Rakudo Raku especially as of the 2017-01 release loads "use" modules
lexically on purpose not globally like Perl, or like Raku did as a bug
thru 2016-12, because that prevented use of multiple versions in different
contexts etc.  Raku intended is much like Muldis D has been for awhile.
See http://rakudo.org/2016/12/17/lexical-module-loading/ for more.

* Some useful info re licenses and stuff: https://spdx.org/licenses/
and https://spdx.org/ and https://cry.nu/perl6/ecosystem-license-module-tagging/ .

* Consider the security consequences of any code including functional being
effectively Turning complete by being able to send a message to anything;
the functional code itself is very restricted and deterministic, but what
about if some message receiver process/service exists that isn't so
securely written and a specially crafted data processing job is given us by
someone who knows we may be causing side effects by sending a message and
exploiting some weakness in the recipient process?
- Perhaps a way to take care of this is for generic expression evaluation
syntax to suppress / not attempt message sending, and that this is only
permitted if an extra flag is explicitly given saying it is ok.
- Alternately or better yet, generalize that feature into a whitelist that
defaults empty saying which services may receive messages from function
calls, and this whitelist can be different with different senders.
- Having this also at procedure level means in general better explicit
supports for analogy to the plPerl, plPerlU duality in Postgres.
- That would be analagous to how an explicit extra flag is needed for a
package "entry" routine to automatically execute where otherwise it isn't.

* Add a language hint for declaring when an enumeration is starting and
when it is ending.  These would be invoked for example by parsers that are
converting non-trivial literals or collection selector syntaxes to the
values they represent, particularly strings or arrays or bags etc, which
would likely be implemented as first calling an empty collection singleton
followed by a series of insertion dyadics.  Likewise invoked by a MUSE API
routine converting a host collection data structure into a DBMS native one.
- Firstly, the hint declares that all Muldis D collection values
intermediate to the completed structure are transient and won't be directly
used again upon completion of the structure.  For example, once an array of
N+1 has been selected by adding an element to an array of N, the former
array value may be forgotten.  Declaring an enumeration start sets up that
prior condition and declaring an enumeration end obsoletes that condition,
so further ops on the value don't assume the pre-op value won't be reused.
- For safety all intermediate values actually will be retained until any
refs to them go out of scope, but the hints mean we will optimize their
storage structure to assume they are transient, such as by using a simple
linked list.  Moreover, no indexes will be produced during intermediate.
- When the hint appears saying the enumeration is done, an implementation
would likely do an eager internal compacting at that point, generating a
new simple array from the linked list with a copy of the elements and then
dereferencing the linked list which should all be garbage collected, though
if another reference actually still existed then that intermediate value
would still be safe.
- A value implementation would have a boolean probably saying that the hint
is in effect, copying that to each new value in the linked list, and then
later then rewritten array would declare the hint is not in effect.
- This feature may also be used for adding to an existing collection, not
just an empty one, in which case the compacting would normally just collect
the added values into a new array so the result in memory is the pre-hint
value structure plus 1 more value structure with all the new elements.
- The hint may also take metadata regarding the widest element type or
otherwise saying it expects all elements to be eg octets or characters, and
then the storage can use optimized versions for those from the start; but
declaring such metadata may also be a constraint such that if some other
value is given than declared the inserting process would throw an exception.
- For that matter, we could say that this "hint" as a whole is more of an
inherited constraint such that any attempt to refer to a value appearance
once another value has been derived from it will throw an exception.
- Maybe (or not) generalize or supplement this with a "transient"
hint/flag/constraint that can be used on any expression to declare that its
result would just be used once and then could be discarded.  This would
come in pairs such as to start that condition, which is then inherited by
results, and to stop that condition.  In a sense this is almost like an
assertion as well that can be enforced at compile or runtime perhaps.
Perhaps normal grammar could just cause either state, such as a Labelling
and an Embedding for example.

* A useful key differentiator of Muldis D (and Raku) from probably most
other languages is declaring use of modules by their identity rather than
by where they live, an example of the latter is paths in the filesystem.

* Explicitly add ordered indexes such that an indexing (serializing) function would yield
an Array of Integer which is then ordered as per a String, or alternately
each Array elem could be an Array likewise, thus defining a tree; the rools
are such that an Integer is always ordered before an Array at the same position.
- Behind the scenes this can be flattened into a plain Array of Integer
using appropriate one-way encoding where eg a zero means the following
element was originally an Integer and a one means the following was
originally the first element of an Array, or alternately we keep the
Foundation a simple String and any such complexity must be boosted to upper
levels, is probably best.
- Alternately, or in addition to this normalizing/serializing function, we
still need an in_order() dyadic because we can't assume that many or even
most data type values can be serialized in a meaningful way, such as Fraction.
So an in_order() is required anyway, and a serializer can be optionally
provided assuming we can key/cache by that.
- All ordered indexes would necessarily have to be designed as partial
indexes with an associated predicate such that they only index collection
members that are passed by the predicate; they're only a total index if the
collection as a whole has a constraint requiring that all collection
members pass the predicate; in practice the predicate would be one and the
same as the type constraint on each of the in_order() arguments, or this
can be required.

* Have support for constraint predicate metadata on values, which includes
stuff related to key inference I suppose.  This can effectively support
stronger typing or constraints at the expression level such that attempts
to perform some operations would fail if the result would violate the
constraints.  Mainly this feature is meant for things like assuring that
all elements of a collection pass a predicate / are members of a type so
that some indexing operations can be total and such.

* Implementations should fold/normalize any source code that defines indexes
etc so that where possible we can detect when independently written source
code has identical behaviour and thus recognize that two things are the same
predicate or same in_order() or same serializer etc, and thus increase the
odds we actually use or reuse existing indexes than making redundant ones.
- This can be done at several levels including both per host implementation
or in a bootstrapped interpreter or both at the same time but the key thing
is that over the lifetime of a program / the relevant data structures, this
folding is deterministic and unchanging, or at least ensure that two functions
with different behaviour never normalize to the same thing.
- Examples of normalizing include commuting commutable arguments or renaming
labels to something generated etc.

* Implementations can cache the source code of any predicates, that source
being the identity of the predicate, that each given Muldis D value
satisfies, associated with the value as metadata.  Once a value is known to
satisfy a predicate, the predicate source is associated with it, so the
next time one asks the same question of the value, we can just see if the
predicate source matches a cache entry, and if so then don't need ask again.
- Optionally cache all questions asked regardless of answer, and record
boolean of the answer, so we also don't have to keep asking questions known
to be false; we could say that in practice repeated 'no' are less common?
- For efficiency, an implementation Memory object could maintain a tree or
graph of predicate sources where supertype/subtype relationships are known,
and then each Value object only has to point to a single leaf or few; in
that sense we effectively do have TTM's "most specific type" concept but
that these are automatically generated and hidden from the user, and the
identity of each type is its expression source code.
- Each Value could still mention several types directly, but in theory
using the graph should allow shorthands where naming one indirectly names
several others, so its a shorthand.
- The Memory object type graph would generally be such that each node is
defined as parents along with "and"/"or" connectives, typically "and"
chaining a parent type with an extra restriction, while "or" mainly comes
about when types are defined using "or".
- The Memory object graph would likely be rooted in, first pick the
foundation type, next pick the Capsule key if applicable, each probably a
dictionary lookup, and then it goes from there.
- The memory object graph can be a version 2 optimization, and for version
1 we could hash all tests with the Value maybe, still to decide.
- Generally speaking the triggering of a type cache usage is by the value
in question being an argument to a boolean operation, either directly or
via use in an argument test or a collection index-related operation.
- While having all these MSTs can mean a combinatorial explosion of types
in theory, the likely actual scale of this is more of an issue for users
having to manually name them, but for an automatic behind the scenes
system, where we likely would only see a few hundred of these at most, in
theory the caching should work.
- For this graph, each final predicate should be factored into pieces as
small as possible, particularly split on "and" and "or" type logic,
which fundamentally take the form of if-then-else, particularly any resulting from
explicit selection-of-where and interface composition, though those would
have been reformed into functions by the time the internals see them.
And so the graph would have a leaf or something node for each smallest-possible
piece and other nodes would connect them with 'and' or 'or' etc, where these
connectors should probably be n-ary sets maybe or otherwise be sorted.

* Replace ::= with "let" or "be" or "let ... be" and replace ::?= with "note".
- Actually, keep ::= as is (or keyword is "naming", or perhaps "label" is more accurate).
- Still use "note", and also use "decor", for annotation and decoration in turn.

* While intended meaning of ::= as a macro definer works for most cases,
it clearly doesn't account for various things with Reference values.
- When one does "x ::= new" to create a variable and give it a name for
referring to, this actually is expecting different behaviour because by normal
rules every use of "x" would actually create a new variable, same as having
"new" in its place would, and this isn't what we actually want.
- Therefore we actually do need a different kind of lexical name or
declaration syntax such that x can be used multiple times and keep its
meaning of referring to the same variable, that is, not like a macro.
- Perhaps a thing to do is reintroduce sigils in a manner of speaking;
eg :&foo is a lexical name for a variable itself, or automatic dereferencer;
also :~foo say is a format for a label for a statement such as for control flow.

* Don't have _Expr or _Stmt in type names unless needed for disambiguation.

* Lets say (\Labelling : (foo,bar)) is what "foo ::= bar" desugars to,
and that   (\Embedding : foo) is what "foo" in a generic context desugars to.
- The "bar" in the above example could either be an expression or a statement,
we reuse the syntax and desugared types for both, and within a routine both
of those kinds of labelled things share the same label namespace.
- The key thing to emphasize here is that what ::= declares is effectively
a macro, and the semantics of ::= are always that wherever the label appears,
it is as if the full expression or statement that it applies to were written
there in full instead, meaning that any occurrance of the \Embeddeing Capsule
can be replaced with bar and program semantics are identical.
- And so we generally need to deal with variables using a different mechanism,
where "deal" means binding a lexical alias to a variable, or using the
current value of a variable in an expression, or updating a variable, etc.
When a "bar" refers to a variable, the "foo" is expressly NOT guaranteed to
be deterministic in the general case.
- Lexical aliases for variables share the same namespace as the ::= macros,
but they are NOT labels for macros.  In some sense, operations with
lexical aliases are eager, while operations with the macros are lazy.
- Given a lexical alias for a variable, we need syntax to select the Reference
value corresponding to that variable.  Also, given a Reference value, we need
syntax to bind a lexical alias to the same variable.  Also, what variable
an alias binds to should be changeable over time.
- It should be possible to anonymously select a Reference value, meaning
create a new variable, and use that same Reference in multiple places, but
also it should be possible to reuse the same code snippit for anonymously
selecting a Reference value such that each usage produces a different
Reference value.  Actually, requiring use of a lexical alias as an intermediary,
meaning not anonymous per se, could handle the first case, while regular ::=
and no use of the lexical alias could handle the second case.

* As far as balancing the fact that ::= results should only need to be
calculated once despite semantically being embedded and that they might
have different results for different inner statements, and they need to be
evaluated only conditionally, I think I figured out a design that could
actually allow Muldis D code to be more in the way of compiled and less of
an interpreted requirement, maybe, or maybe these thoughts are simply about
making the interpreter logic simpler.
- Implementation-wise about some "x ::= y", we can use a lexical variable x
which starts out undefined; when we want to use the value of x we then
evaluate the y and assign the result to x, and any other places in the same
statement that x is referred to we just use it, thus avoiding the
recalculation; when the statement ends and we transition to the next
statement, the lexical variable x is made undefined again.  This is the
default simple logic, but a simple optimizer can check if y refers to any
actual variables, and if it does not (or we are inside a function), then x
is not undefined and can be safely reused.  A more complex optimizer could
deal with, in the case variables are referenced, did they change meanwhile,
which could only in general be guaranteed if a consistent lock was on the
variable.

* Maybe :. and :.? (both infix) as special syntax for tuple attr accessor / exists,
and :< plus :> (both prefix) for capsule wrapper/asset and :>. plus :>.? (both infix) to combine
capsule asset tuple attr accessor if asset is a tuple.
- Check that none of the above aren't already in used by eg Attributive.
- Make all of the above high priority operators, higher than nearly everything.

* Do something about the special postfix syntax :> which can't simply be
replaced with a postfix regular operator call.

* Lets say the contract system is modelled as a directed graph where each
node represents a contract, which corresponds to a set of values, and each
arc represents a direct definer relationship and each path represents a
possibly indirect definer relationship; for simplicity we are only thinking
about defining what values comprise the contract and we ignore stuff like
default values, which can be overlayed over the otherwise complete system.
- The identity of every contract is the source code that declares it, which
may optionally have an identifier label on it, but regardless it is
referred to somehow either by a label or by being embedded; this is also
like how an expression is identified.
- Contracts come in two varieties, which are nominal and structural.
- A nominal contract must have a label for identifying it and its
definition must not be declared with the label, but rather its definition
is always separate and refers to the label externally.  When other
contracts declare that they compose a nominal contract, they are extending
the definition of the contract; perhaps even the term "extends" should be
used rather than "composes", since that is what is actually happening, akin
to tuple extend.
- A structural contract is identified by its source code and not by its
label, in contrast to the nominal, and it declares for itself its complete
value set, in terms of set operations on other contracts' types, and its
meaning can NOT be extended by some other contract it doesn't explicitly
define itself in terms of.
- In the contract graph, a nominal fundamentally has arrows pointing TO it
while a structural fundamentally has arrows pointing FROM it.

* Rework the interface/virtual/etc thing.
- Formally make virtuals and implementers their own standalone concept,
expressed as they are now essentially.  By default, a virtual is declared
in terms of any type, and an implementer in terms of a regular subtype by
constraint of said.  But formally speaking the relationship is nominal,
the implementer refers to the virtual it implements by name, and an
invocation on the virtual only ever dispatches to implementers that refer
to the virtual by name.
- An interface is defined largely in terms of a list of routines that it "provides",
this being a list of qualified names that resolves to actual specific
routine declarations, which may be either virtual or non-virtual, and this
is analagous to declaring the public methods of a (super)class.
- A type that composes an interface also declares a list of routines that it
"provides", each of which may or may not be an implementer of a virtual
from the interface it is composing; the composing type may refer to routines
declared for the composed interface, in which case it is saying that it
doesn't implement that one itself, and expects its own composers to do so.
- Some terminology may need further changes.

* Note, SQL has about 825 reserved words according to
https://www.drupal.org/docs/develop/coding-standards/list-of-sql-reserved-words
so its not just a few dozen; in contrast Muldis D has maybe 2 dozen.

* Note, see https://wiki.haskell.org/Keywords which shows like our
Plain_Text.pod the Haskell keyword list includes both alpha words and symbols.

* A thing of interest, PL/Container: https://github.com/greenplum-db/plcontainer

* Note, many languages have the "let" keyword, which derives from maths or
is popular in functional languages, for declaring either functions or scopes,
such as "let foo = some_expr" (C#, Swift, others) or "let (...) ..." etc (Haskell).

* Or do this ... somewhat reminiscient of HTTP/etc requests/responses actually:
    #!foo
    Muldis_D Plain_Text 'https://muldis.com' '0.300.0'
    meta script_encoding 'UTF-8'
    (\Package : (...))
- A SQL version:
    #!foo
    SQL SQLite 'http://sqlite.org' '3.15.1'
    meta script_encoding 'UTF-8'
    select * from someone;

* Consider parser unit predicate more like this:
    #!foo
    Muldis_D;;Plain_Text;;(
        authority : "https://muldis.com",
        version_number : "0,201,0,-9",
        script_encoding : "UTF-8",
    );
- The quasi-tuple-format list, or actual tuple format?, can be free to
omit elements not relevant at a particular stage, such as encoding, or that
are chosen not to apply, such as Unicode normalization etc.
- Strictly speaking one can declare UTF-8 or any encoding without declaring
a repertoire, as technically UTF-8 is just a mapping between 2 integer arrays,
using it on its own doesn't have to imply the repertoire is Unicode, or something.

* Actually drop all double-semicolons and single-semicolons between sections:
    #!foo
    Muldis_D Plain_Text  `magic cookie`
    (w:x,y:"z","a":"b",...)  `rest of predicate`
    ...  `subject`
- The above 4/5 are each separated by <sp>, except the shebang is determined by UNIX rules instead.
- The magic cookie must be simple ASCII alpha barewords.
- The predicate remainder must be a pair of parens with simplified Tuple lit syntax;
actually, it MUST be 0..N named pairs where each key/asset of a pair must be
either an alpha bareword or a plain double-quoted string, contents escaped
per name/text literals but no leading backslash, no trailing comma needed this time for single pair.
- The subject can be anything at all, it is everything that follows the closing paren of predicate.

* At Foundation level, a Function has the "accepts" trait and not the
"matches" trait; the idea is that "accepts" in general takes all the same
formats as "matches" but is more strict; "matches" is more applicable in an
environment with operator overloading, while "accepts" is more for an
environment that isn't necessarily overloaded, maybe.
- Actually no, at the Foundation level a Function is still just its body
plus some hint bits like commutative etc; any matches/accepts concepts are
just integrated into its body.
- At Foundation level, allow some special value results, eg when input is
not valid UTF-8, rather than dying.  Also for divide by zero etc I suppose.

* Notes for 2016-11-01:
- Support Package types at the Foundation level after all.
- Canonical unit of code given to "evaluate" or "perform" at the Foundation
level is now a Package with a entry-routine that is either a Function or a
Procedure, similarly to a Package being the unit for passing work to a remote
process to execute.  In either case this Package should be self-contained
and a Package is used in order partly for code to be well factored.  But
when used as an index generator it is still serialized to produce its hashing
identity for the index.
- While Foundation supports/wants Packages, it is still strictly fully-qualified
entity names, no floatings, and also 100% structural typing, no interfaces
etc, so an interpreter/compiler layered on top still would desugar any interface
defs to not having such.
- At Foundation level, Packages still have identities, but we might consider
supporting the concept of pseudo-or-actual-anonymous packages, which can not
be dependencies of other packages, and can be suitable for main programs or
particularly generated packages; on the other hand we might want to have
generated UUIDs or something for those, see also what .NET does.
- So, the main sugar then that Foundation doesn't support and is layered on
top is any support for overloading polymorphism, floating names, entity declarations
representing types, etc, probably aliases; Foundation has Package,Function,Procedure.
- Make Foundation version of Packages just have the attributes
{foundation, entry, materials} and maybe "identity"; not "using" or "floating";
"identity" is not actually used per se but it is expected or allowed in
contexts where packages are kept in depots and should be identified therein.

* Part of selecting a Reference value is, optionally, selecting a type
constraint and default value for it.  Perhaps type definitions don't need
to include default values in the general case, and these can instead just
be set per variable or in particular selection functions.

* Notes for 2016-10-28:
- Declare a predicate function in Foundation for every type that is
considered special enough to name either to the main
runtime/interpreter/compiler or Plain_Text parser or Perl etc interfaces
etc.  A type is in this list if and only if it is either a Foundation type
or it is a system-defined Capsule type whose wrapper is just an Attr_Name
rather than being package-qualified, or maybe a few special cases.  So, we
DO now declare Text, Blob, etc in Foundation this way.  Part of the idea
also is to make the System package more of a luxury than a necessity.  The
Foundation still does NOT declare eg Text-specific operators in general,
one still does longhand with Capsule pack/unpack plus Foundation Array ops.
- Structural vs nominal vs duck vs algebraic typing... metamodel...
- Low level MD is 100% structural (?) while High level MD adds duck and
algebraic and or its kind of hard to know what are right terms to use.

* Call variants Less Muldis D / Less Muldis D Plain Text and More Muldis D
/ More Muldis D Plain Text.  The Less is analagous to Not Quite Perl.
- http://pmichaud.com/2010/pres/yapcna-nqp/slides/slide8.html
"NQP's goal is regularity more than DWIM"

* Something interesting / could be informative: http://reactivex.io

* Low level expression node types:
- Opaque(x) - Expression would simply evaluate to the value of x, which has
already been completely folded.
- Args() - Expression would simply evaluate to the argument list for its context.
- Evaluation(x,y) - Expression would evaluate the context-defining sub-expression
or function defined by x, such that the value of y is its args.
    - x may optionally be the name of a Foundation function.
    - x may optionally be a special 'self' name indicating a recursive call.
- Performance(x,y) - Statement would perform the context-defining sub-statement
or procedure defined by x, such that the value of y is its args.
    - x may optionally be the name of a Foundation procedure.
    - x may optionally be a special 'self' name indicating a recursive call.
- If-Else-Expr(x,y,z)
- Guard-Expr(x)
- If-Else-Stmt(x,y,z)
- ... name, naming, anno, deco, ...
- ... sel for array, set, bag, tuple ...

* Notes for 2016-10-21:
- The concept of the "hosted data" class of languages could just go away,
or be strictly limited to representing data and not code.  That is, no one
would declare eg Perl Hosted Data as a command language.
- What we do for a VM interface like MUSE is that everything native is
converted into a "Value" object with relatively little indirection, and THEN, one
does the prepare/execute with a Value object where an optional input to
prepare/execute is the spec of the language already represented by a
Muldis D data type value (either Text or something else), so the language
name then is orthogonal to what exact eg Perl types the values were represented
as prior to conversion to Value.
- So all finer details of Perl types to Muldis D / MUSE types are strictly
on resolving to eg integers, texts, tuples, etc.
- If a distinct Hosted Data source type exists, it is possibly more like a concrete
parse tree that might vary by host.
- Use Function/Procedure dichotomy, not Command; use keywords evaluate/perform to run them.
- The External type is what it is for when one wants to pass regular Perl/etc
values including sub refs etc through the API unadulterated, eg if the DBMS
has a mechanism for callback hooks these may be options for passing to them.
- Similarly to MD Plain Text or SQL, one can parse JSON or XML etc via MUSE
as a language option.

* For consistency and understandability, use "-" with ".." to mean open
endpoint rather than "^"; so: {x..y, x-..y, x..-y, x-..-y}.  Regardless of
the change, one still has to say eg "-5..(-2)" because we don't special
case a leading "-" on a digit to indicate a negative as far as parsing
precedence goes.

* Notes for 2016-10-16:
- Rewrite part of Foundation.pod declaring the Foundation types/routines so
it looks more like System.pod, but providing pseudo-source-code (conforming
to Muldis D Plain Text with foundational clause) that may illustrate more
concisely than descriptive text the definitions of the types/routines that
Foundation understands.
- Add new expression/statement node types / PT keyword yada_yada_yada which
is for indicating programatically that code is missing at that spot and
should be provided later; such code would parse without error but would
throw an "unimplemented" exception at runtime.  This keyword chosen on
purpose as unlikely to clash with something users would want to define.
A key point is that the new Foundation.pod pseudo-code would use this
copiously, anywhere from as given definitions of whole functions to
definitions of the portion of a function besides that which validates args.
- Every Foundation entity is a function or a procedure, period; or that is,
it has no interface concept of a type/contract/interface/etc, those being
strictly higher level things that would desugar to a "function"; also the
Foundation function/procedure is actually just an expression/statement with
an args context; any "matches/excepts" is explicitly defined in the body.
- The declared Foundation entities would have matching names with their
correspondents in the System package where applicable; for example, FDN would
have a "function" just named "Integer" which takes a tuple of degree 1
and returns a Boolean.  Niladics for default/special values are different funcs.
- Regular package entities would be allowed to alias Foundation entities,
maybe, if that makes sense.
- ... TODO note paradigm shift...

* Notes for 2016-10-13:
- General rule of thumb, every context-free type should be defined as a
Capsule with its own wrapper, and every named type should be thusly
context-free unless there is a good reason for it not to be.
- Context-free means a value of the type can appear anywhere and it is
trivially easy/cheap to know that the value is of the type, even when we don't
have the actual type definition available, such as at the Foundation level.
- In contrast, context-dependent types, such as subtypes of Tuple or Array,
should generally only be used in very specific contexts, such as used as a
direct component of a context-free type, and thus defining this as a type
at all is merely for the purpose of code factoring, more or less.
- Idiomatically, a context-dependent type should be named in a namespace
beneath the context-free type it exists to serve, such as Package::Identity
to go with Package.  Generally every type directly in System:: or
System::Source_Code:: should either be an alias of a Foundation type, or it
should be an Interface, or it should be a context-free type.
- Interfaces and virtual routines are conveniences for factoring code in
context of the full Muldis D environment, but at the Foundation level
neither exists and values should all be declaring what they are and how to
interpret them strictly in a structural manner, an appropriate nested
combination of specific Capsule wrappers and specific attribute names.
- Nesting is also used for general-purpose proper subtypes.

* Simple rules for expression folding, particularly at MDPT parse time:
1. Each "opaque literal" parse node turns into 1 constant value node.
2. Each "collection selector" parse node is folded into 1 constant value
node iff every one of its child parse nodes can be folded into 1 constant value.
3. Folding does NOT occur across a naming expression, or annotating or decorating etc.
4. Folding does NOT occur for a node that is an invocation of a material.
5. Folding might occur across special operators specifically recognized by
the MDPT grammar and directly correspond to a Foundation op, examples being
>:< etc can be folded if both inputs can be folded.
6. We do not fold across special references like 'args' or any reference to a name.
- Following these rules can give the best tradeoff between preserving the
user's original structure of the code and storage/execution efficiency as
well as keeping parsing quite simple and easy.
- Further folding can be done at runtime or cross-compilation time, but the
parsing of MDPT into the "canonical" source form would respect the above rules.

* Magic numbers in the form of niladic routines for VMs that the Muldis D
reference engine implementations conform to; testing for the declaration
requires some kind of introspection capability, such as Perl's can(), or
for languages lacking such, some nearest analogy:
    - provides_Perl_API_for_VM_effective_at_Muldis_D()
    - provides_C_Sharp_API_for_VM_effective_at_Muldis_D()
    - etc
- A Perl / C# / etc library that supports multiple independent API specs
that broadly support this scheme may have a distinct such niladic for each
one, say for example if a standards body not specific to Muldis D prefers a
magic number that doesn't have "Muldis D" in it, while not preventing us
from still using that string.
    - For example:
        - provides_Perl_API_for_VM_effective_at_MUSE()
- A fundamental feature is that there is no common hard dependency (such as
a specific library) for providers of an API, not even as the means for
declaring provision of the API; for example, declaring provision of the API
must never require composing a particular "interface" or "parent class" or
"role" etc.  This is in stark contrast to how the legacy Perl DBI works,
where to use any DBD module one MUST also use the "DBI" module.  However,
API providers may individually choose to have common hard dependencies such
as a DBI-alike.  No mandatory common dependencies means each provider is
truly mutually independent of the others, like with SQL or HTTP or etc.
- See also Perl module Data::CapabilityBased for a related concept,
"Ask your data not what it is, but what it can do for your program".
- Optimizing similar within Muldis D itself means having a foundational
syntax analagous to that for routine calls, where one can ask, does a
routine matching this name chain and argument signature exist?  Then common
virtual routines can invoke that rather than is_a when more appropriate;
that is, we ask like "can_evaluate \plus::(x,y)" etc.

* Notes for 2016-10-08:
- Specifically within possibly multi-level/:: identifiers, iff the first/only
chain element is one of these keywords NOT QUOTED then it indicates the
namespace for the chain as a whole (quoting them makes them not-keywords):
    - foundation - namespace for referencing named Foundation entities
        foundation:: - any meaning?
        foundation::FDN__Integer_plus - how to refer to Foundation func Integer-plus from anywhere
    - used - a root namespace for referencing entities in the linked versions of all used packages
        used:: - any meaning?
        used::foo - root folder of the used package linked under that short name
        used::My_App::main - possible long name of main proc from POV of the My_App package
        used::MD::Integer - possible long name of System Integer from POV of some other package using it
    - package - the innermost/only package definition declaring the referant
        package:: - the root folder of that package
        package::foo - a folder or material immediately inside that
        package::Integer - an absolute reference by anything in the System package to the Integer type
    - folder - the folder that is the immediate parent of the referant (material)
        folder:: - the folder itself, which might be the same thing as package::
            - this obsoletes 1::
        folder::foo - the referant
            - this obsoletes 1::foo
        folder::"" - how virtual routines are typically referred to by their implementers
    - material - the material, which lives directly in a folder, whose definition includes the referant
        material:: - how to refer to one's own material anonymously, eg for a recursive call
            - this obsoletes 0::
        material::foo - how to refer to a specific trait of one's own material
            - open question - how to deal with explicitly defined vs inherited or defaulted
        material::where - unary func testing membership in a type
        material::default - niladic func giving default value of a type
    - lexical - the innermost lexical context function or procedure, eg func or proc body
        - name change?
        lexical:: - how to refer to the innermost self anonymously, eg for a recursive call
            - also obsoletes 0::
        lexical::foo - an option to refer to a named lexical expr or statement
    - relative - for a relative context - IS THIS STILL EVEN USEFUL?
        relative::1 - refer to one's parent
        - or don't support at all, and instead let unquoted 0::, 1:: etc mean eg "\\c<0>":: like in regular Attr_Name
    - floating - for a floating context
        floating:: - this by itself is meaningless
        floating::foo - look for 'foo' in the various floated paths
        floating::foo::bar - likewise
- Note that as of 2016-10-10 the 'self' package does NOT have a user-defined
local alias that shares the 'used' namespace; instead 'used' is reserved for
OTHER packages and a package must use the 'package' keyword to ref itself.
- Note that a "floating" list must be fully-qualified, with either package:: or used:: for each entry.
- Note that without an explicit such keyword, leading :: eg ::foo always takes 'used' eg used::foo;
otherwise, without an explicit such keyword, 'floating' is assumed.
- Now, simple prefix/infix/postfix are all allowed to be multi-level,
and it is explicitly the presence or absence of a trailing :: that differentiates
a simple monadic prefix eg "foo::bar (...)" from an n-adic postcircumfix eg "foo::bar::(...)".
    - TODO, maybe reconsider this?
- Specifically within some lexical context as runtime information:
    args - for referring, as a value, to the argument list of the innermost lexical context
    callers - for referring to runtime information about the current call stack etc,
        - about who called us this time, not where we were declared

* Notes for 2016-10-04:
- Paradigm:
- All Muldis D source code is a tree (or graph) of expression nodes, whose
union type is Expression.
- Muldis D code is composed wholly of nested lexical contexts,
each with its own "args", each with its own namespace for "foo ::= ..." etc.
Between each lexical context is a lexical boundary.
Crossing a lexical boundary denotes an effective delay in execution of the
inner context relative to the outer context.
- A lexical context is declared by either of several kinds of expression
nodes, those that declare a function or procedure body or something similar.
- These nodes / syntaxes declare a nested lexical context, the ... in question:
    package (x:y,...)     yields Package selector node
    function (x:y,...)    yields Function selector node
    procedure (x:y,...)   yields Procedure selector node
    \(...;...)            yields Function_Body or x selector node
    \[...;...]            yields Procedure_Body or x selector node
... since we want the inner code to be written normally within such and be
treated as a constant of sorts to the outer context.
- In contrast, nodes like these do NOT declare a lexical context:
    \Package >:< (...)     yields Capsule selector node or func call node
    \Function >:< (...)    yields Capsule selector node or func call node
    \Procedure >:< (...)   yields Capsule selector node or func call node
... since these are generally constructing code dynamically from named nodes
or variables declared in the outer scope.
- A Muldis D Plain Text source code string must, at its outer level,
aside from leading declarations to help understand the syntax,
consist of a "package (...)" node; if one wishes to declare multiple
packages in the file/string, they must be nested inside this single one.

* Notes for 2016-10-01:
- Major syntax changes involving generic expressions, which can be anywhere,
and material declarations.
- Have a set of alpha keywords or source code attr names for the common
declarations or actions that presently use symbolics, so they're easier to
understand in casual reading and also frees up symbols for package-def ops.
- Now basically every keyword symbolic contains colon or backslash, or semicolon or comma.
- keywords:
    - expr "evaluate x" where x is a function call spec, results in the result of executing the function
        - what any function call syntax desugars to
        - also want eg can_evaluate eg "can_evaluate \plus::(x,y)"
    - stmt "perform x" where x is a procedure call spec, causes the procedure to execute now
        - what any procedure call syntax desugars to
        - also want eg can_perform eg "can_evaluate \read_Text_from_STDIN::(x)"
    - expr "returns x" (nee "<-- x") marks which expr in a semicolon-list is the aggregate result
    - expr "fetch c" (or "=: c") where c is a Reference results in the current value of the variable that c points to
    - stmt "c store e" (or "c := e") where c is a Reference results in the variable it points to getting new value e
    - stmt "c apply f" (or "c =:= f") is a shorthand for "c := f(=:c)"; f() is a unary function or part-applied >1
    - expr|stmt "x then y else z" (optional "if" noiseword) is short-circuiting ternary expr/stmt,
        where x is a predicate expr and {y,z} are either exprs or stmts
    - expr "x :? y :! z" (nee "x ?? y !! z") is non-short-circuiting ternary expr
    - expr|stmt "w when x1 then y1 when x2 then y2 ... default z"
        (optional "given" noiseword) is short-circuiting n-ary expr/stmt,
        where {w,x1,x2} are exprs and {y1,y2,z} are either exprs or stmts
    - expr "w :?? {x1 : y1, x2 : y2, ...} :!! z"
        (nee "w ??? {x1 : y1, x2 : y2, ...} !!! z") is non-short-circuiting n-ary expr

* Note that the latest / 2016 Sep 28 (and maybe prior) TTM IM prescriptions differ from Muldis D in ways including these:
- IM 5: A supertype should declare an example value of a supertype that isn't part of its subtype.
- IM 7: The term "overlap" is the complement of "disjoint", having at least 1 value in common, period.
- The term "proper subtype" only means "proper subset" for TTM "scalar" types, otherwise it means "subtype".

* Note that Muldis D isn't alone with using the GPLv3+ for a specification rather than code;
https://rfc.zeromq.org/spec:42/C4/ (Collective Code Construction Contract) does that too.

* Note that http://kripken.github.io/emscripten-site/ may be useful with LLVM or JavaScript.

* Note see also http://bedrockdb.com .

* Note see also https://code.google.com/archive/p/northwindextended/ .

* Note see http://tdan.com/data-architecture-comn-sense-the-missing-logical-layer-in-data-management/20569
and http://www.tewdur.com for something related to multi-model DBMSs.

* Generalize the Foo::Bar syntax so it implies drilling through nested
Tuples, and also special case support for nested tuples so that the same
tree branches can be recursed into more than once.  Examples
    (
        x : (
            "" : -4,
            a : 3,
            b : 5,
        ),
        z : 6,
        y : (
            a : 5,
            b : 0,
        ),
        x : (
            c : 2,
            d : 9,
        ),
        x::e : 17,
        x::f : -3,
    )

* Related to above, see if we want to somehow merge uses of . and :: or
otherwise allow :: as a tuple attribute accessor in general that is
recognized at the Foundation level.

* Notes for 2016-09-xx:
- Major syntax change for functions and other materials.
- Specify this block starting in terms of the source code types and then
see what Plain_Text syntactic shorthands are useful, or do both together.
- New terse forms when only the body is declared are \(...) for functions
and \[...] for procedures, instead of (function : (...)) etc.
- New general forms are Tuple literals, but with a leading keyword that
binds with the tightest precedence like with terms; example like this:
    function (
        traits : (
            returns : \Integer::(),
            matches : (\Integer::(), \Integer::()),
            is_associative : True,
            identity : 1,
        ),
        body : \(args.\0 plus args.\1),
        annotation : ...,
        decoration : ...,
        args : (),
        context : ...,
    )
- Above subject to change.
- For embedded cases involving a partial application, one could do this:
    function (args : (...,), body : \(...))
... instead of this:
    (...,) \--> \(...)
... if they wanted to for illustration, but the latter brevity often wins.
- Actually the above is more like a selector at runtime for a Function
value, which also subsumes the attributes of a Function_Call value, "args"
being any partial application (niladic tuple by default).
- So eg that also means "\Function wrap (...)"
- Any \(...) means the (...) gets its own 'args' context and is evaluated
later, and it mirrors \foo::() on purpose as both return a nee-Function_Call
for giving to higher-order functions.
- Where there is no leading \, the expression instead gets evaluated in the
context of the expression containing the "function", which includes the
outermost tuple.
- The "body" is the function proper, and recognized by the Foundation.
- The "args" and possibly "context" are also used by the Foundation.
- The "traits" are other things that are potentially recognized by the
Foundation and might be used by it for enhancing performance but often they
are made use of instead by the higher level runtime written in Muldis D and
running over top of the foundation, just as implementing polymorphism and
the concept of types is.
- It stands to reason that the related "package ..." is similarly like
"function ..." actually an expression node and that a source file
containing one directly is implicitly the body of a niladic function whose
args is (), and so material declarations are actually expression nodes more
or less, so then perhaps the general case of a source code string being
parsed should actually be a function body whose result is typically but
doesn't have to be one or more Package values, that is, an Array of
Packages value, this being shades of Perl, but presumably this outer-level
function body isn't allowed to immediately invoke any non-Foundation routines,
no matter what package they are declared in, any package-defined things
can only be called delayed with \.  In this case, parsing would begin assuming
the latest Foundation etc version supported by the implementation, and then
change when an explicit Package declaration along with its declared expected
Foundation version is encountered, or otherwise the process restarts several
times potentially, same as with the syntax declaration that precedes this.

* Use =: as the "fetch" operator to get from a variable, which is visually
flipped from the := "store" operator; the former is a unary expression,
the latter is a binary procedure.  So & and * etc are left free.

* Note: TTM Multi-assign can just be an ordinary (system-defined) procedure
call now, same as regular assign, it just takes collections of references
and things to assign to them, more or less.

* File this note ... func call to foundation also needs context because
context package may require different version of foundation.

* Notes on 20160906:
- Lets say that the actual variable a Reference points to contains several
pieces of user-visible data that can be independently kept:  The regular
current value of the variable, the declared type or currently applied or
"expected-to-satisfy" constraint of the variable (a predicate function
call), the declared default value of the variable (a niladic function
call), perhaps other things.  Each of these would need Foundation routines
for reading or writing them.  Or alternately we say there's just one piece
of data, which is a Tuple, whose attributes are those mentioned things; if
we do this, then more of the complexity is boosted to user space and is
customizable.
- Consider emphasizing Foundation more as being a specification of the API
for a virtual machine or runtime for Muldis D, or otherwise have separate
documents for those details versus the language overview that Foundation
also serves the function of; eg, concepts like contracts are more of an
overview than a foundation thing. Foundation is still a good name for the
VM / runtime descriptor.

* Notes on 20160816:
- Effective immediately, Muldis D should be implementation-driven, meaning
that I should now focus on writing one or more host language
implementations of the Foundation etc.  I should do so using reasonably the
least amount of code possible to speed the way so code written in Muldis D
Plain Text would execute, and thus with the language bootstrapped we can
save implementation of as much as possible such that as much as possible
reasonably can be written in Muldis D itself.
- The host language version of the Plain Text parser only needs to be able
to handle UTF-8, the encoding of the System files, and then any support for
other encodings can just be implemented by Muldis D code.  And so on.
- Start by making a simple Perl script that extracts a .mdpt file from a
.pod file like with System.pod etc; it may just process STDIN to STDOUT.

* Notes on 20160820:
- The Foundation should be formally simplified to know practically nothing
about the "normal" Muldis D source code types, or the "normal" Plain_Text
grammar, or even a lot of core concepts, and instead natively have an
alternative and much simpler model.  Use the Perl ecosystem as influence,
where say the Foundation is akin to vanilla Perl and the Muldis D proper
is a distinct separate language implemented over top of it, like a
combination of Perl plus Moose plus more syntax plus a new runtime etc.
Or Foundation is like MoarVM and Muldis D proper is like Raku or NQP.
- See also MoarVM etc bootstrap types like BOOTArray, BOOTInt, BOOTCode,
VMString, NQPArray, QUAST::Op, BOOTCompUnit, etc.
- The Foundation just knows about values, variables, functions, procedures
and moreover its concept of the latter is a lot simpler, more like that of
Perl.  Foundation knows nothing of constants or contracts or interfaces
or operator overloading or multi-level namespaces etc and also
types are just a convenient fiction that it doesn't formally know about.
There are just values which happen to fit into 7-8 or so disjoint groupings
and nothing knows about their distinction but internals.
- Formally the Foundation provides a set of functions and procedures, each
of which accepts a subset of all values and/or results in such, with no
concept of type; bad input will throw an exception simply because the
values are unacceptable etc.
- At the Foundation level, like in Perl, functions and procedures have no
traits other than their bodies, and the bodies will directly take their
"args" input Tuple and either work with it or throw an exception.  A
function body is just an expression that can be factored into named parts;
a procedure body is just a sequence of statements plus such expressions.
- Similarly to its builtins concept, the Muldis D runtime builtin to each
host language implementation of the Foundation expects that all
user-defined code consists of just the same kind of simple functions and
procedures as its native ones, that only have bodies and not other traits,
though the list of statement/expression node types is likely the same
because it is already reduced / noninclusive of all Plain_Text etc options.
- In order to be extensible, any (eg Tuple-based) data types the Foundation
knows about only requires that certain attributes be present, but it allows
other attributes to exist which it simply ignores.
- All wrappered/Capsule types at the Foundation level use a simple
Attr_Name as the wrapper, most likely named "FDN__Foo" or such.
- The Foundation probably would also know about a simplified concept of a
Package as well just as a container to keep named pieces of a refactored
routine in, but generally speaking all user-defined functions and
procedures intended to be evaluated directly by the host-language Runtime
would be anonymous, and any higher-order whatsits would be taking or
returning the entire source code, with typically nothing being called by
name anywhere, except for builtin routines.  The low-level packages might
also be anonymous, or alternately they would likely best be proper-subsets
of full Muldis D packages with full identity names, so they can live in the
same compilation unit repositories as whatever.
- A simpler analogy to Plain_Text (or a proper subset calling itself
Plain_Text plus foundational indicator) would be the only thing the host
language parser knows, and it parses directly into the Foundational
simplified package/function/procedure/the-end.
- And so, the main Muldis D runtime AND plain text parser would be written
in Muldis D itself, or specifically the proper subset the Foundation knows.
OR, what we may actually have here is a single self-contained Muldis D
package written in said subset which consists of a "main program" runtime
plus parser etc, and it is actually THIS which is what is used to parse and
interpret/compile/etc the normal System.mdpt and other full grammar code.
- Probably the Foundational grammar/mini-plain-text would be limited to
ASCII, although strings would still be "any integer", and the bootstrapped
Muldis-D-mini code that would parse the regular System.mdpt would implement
UTF-8 whatever.
- At the simpler Foundational level, anything which counts as optional
hints for performance eg indexes is realized as optional explicit arguments
to the operators at hand, eg ones that output a Bag, and the effects of the
hints would then propagate internally as useful; hints are of-course
(presumably) write-only in the interest of portable determinism.
- Maybe call the thing that parses/etc System.mdpt for the first time
Bootstrap, especially if it is used as a compiler that converts the more
detailed Muldis D code into that simpler variant, which then the
Foundation's built-in just runs directly in place of that Bootstrap.  In
theory/practice this Bootstrap could run another main program written in
full Muldis D, namely "System" plus "System::*" plus things which may implement
compilers/etc of various kinds.  Users could choose whether their compilers
are hand-written in the full Muldis D or in the minimal variant, same as
users of MoarVM/etc can write in either NQP or full Raku.
- Maybe "System::Bootstrap" is what this bootstrap package should be named.

* Notes on 20160819:
- Have some explicit concurrency management features out of the gate, such
as where multiple in-DBMS processes may be using the same variables at once
keeping in mind that some variables are normally shared, some are normally
private or lexical, some change statuses between those.
- Each in-DBMS process has the concept of a process id and/or one or more
transaction ids, the representation of each of which is also a Reference or
otherwise a black box to / not seen by Muldis D code.
- Each variable has associated with it a record of interested parties and
it potentially has multiple versions where the versions are in a tree
rather than a graph; the identity of a version is a combination of the
singular logical location/identity of the variable plus a process and/or
transaction id.  Each version has the appearance of a value, itself
readonly; if any process/context has a repeatable read lock on a variable,
then assigning to it will create a version.  A process/context creating a
write lock on a variable will typically create a privileged path through
the tree such that writing is then only allowed by that process or locks
for such only by others who are seeing the value it set in contrast with
some other version.  ... Something something multiple persisting versions...
- Have 2 procedure traits each of which is an expression taking args as
input and resulting in a Tuple each of whose attributes is a Reference; the
first trait says we want repeatable-read-consistency on all of the
corresponding variables for the duration of the procedure; the second trait
says we may want to update/assign-to any of the corresponding variables and
so we should get a reserved or write lock on them for the duration of the
procedure; in either case, invocation of the procedure will block until all
of the requested locks have been granted, prior to executing any
precondition or main body blocks.  If the procedure can't get all of the
locks it requests, it will throw an exception.  There can also be blocking
or non-blocking versions.  What happens may vary by implementation.
- UPDATE: The primary trait of interest is "synchronizes" which takes the
expression denoting a list of Reference values whose variables we want to
have exclusive access to, possibly for writing.  Only a single in-DBMS
process may be synchronized on a variable at a time, and so any other such
claims in progress must complete or rollback before it is granted;
moreover, when one holds a sync on a variable then reads from it will get
the newest and canonical version, based on which they can decide to update
it or not with a newer value, which then becomes the canonical one.  During
this time, other in-DBMS processes may see an earlier committed version of
that variable with read-only access, which is fine as long as they don't
assume it is the latest.  This is essentially serializable isolation.
- Another trait "snapshots" takes the list of References expression and
obtains a repeatable read lock on all of them.
- Without either of the above traits in force, variables could potentially
change between each innermost statement of a procedure stack, similar to
read committed isolation perhaps, though within each individual inner
statement there is repeatable read isolation, or something.
- That all being said, at the Foundation level the synchronize/snapshot/etc
mechanism needs to be a regular procedure call, meaning it can also be
called directly as a procedure rather than requiring a declarative, meaning
we have to use a scope guard or such.  How this could work say is the
Foundation provides a proxy procedure that takes an argument specifying the
normal procedure to execute plus another argument with a Tuple of Reference
that it should synchronize or whatever; then, the proxy will apply and free
the locks respectively around the procedure call.  This is somewhat
low-level and doesn't know about a lot of Procedure details, it is meant to
be invoked by a runtime written in Muldis D ... or alternately by users.

* Notes on 20160815:
- Muldis D would likely end up as a highly recursive compile-execute cycled
environment such that every executing routine has data lexically available
to it that includes its own source code, the package it was defined in, and
all the packages that one uses, moreover the act of invoking a routine is
actually taking the source code of the thing being invoked and interpreting
it directly like a data processing operation.
- Typically a routine invocation is on a named package entity declared in
the same package or as the caller or a package that one "uses", in which
case the interpreter would walk said packages to find the called routine's
source and invoke it, which might be by way of a virtual/implements
relationship; at other times, a routine or package might be generated at
runtime, not the least of which by or for a higher-order function, and then
invoking that is essentially the same code-walking interpretation process
as when calling a named package entity after it is "found".
- We want two similar Package-like types; the regular "Package" is the
normal source format that can persist and is non-recursive, its "uses" are
just identifier strings of the other packages; the alternate "Package_Tree",
say, only manifests at runtime and has the actual sources of all "used"
packages as its attributes; the latter is the result of a compilation of
sorts, where a current package and all dependencies are pulled in from
whatever repositories they live so the actual execution cycle doesn't have
to worry about that; typically a procedure would do the gathering into a
Package_Tree, and then execution could potentially be by a function.
- Whenever a routine effectively changes its own source code, that is, when
an analogy to a "data definition" operation happens, this generally results
in the current interpreter, still being the old code version, spawning
another interpreter to carry on in its place, where the latter is running
the new code version.  So we may end up with multiple nested interpreters
to an arbitrary recursive depth, although any interpreter can be structured
in such a way to instead exit with a left instruction for its own caller to
call the new version in its place.
- It is possible that other languages such as Lisp do something like this,
or I recall the act of defining routines in a Lisp shell to claim it had
increased its execution level, or something or other.
- This design can permit a high degree if isolation, where individual
in-DBMS processes can arbitrarily change the executing code without
impacting each other, aside from where they choose to share their results
with commits.
- A Function_Call value would have as its context attribute a Package_Tree
value provided to its selector at runtime, rather than a Package value.
(Providing it at compile time / inlining it probably bad / self-recursive.)
- An interpreter procedure would likely by necessity use a collection-typed
variable to keep the package sources in definitively after fetching from depots.

* At Foundation level, a general routine for calling conceptual Foundation
routines would take an extra optional argument or several which provide
optimization hints including ones for indexing; at Foundation level things
are more explicit as arguments.

* In theory there should be disjoint interval types for successable vs
not-successable member types.  Partly this is for distinguishing certain
fundamental conceptual differences such as the interval being sparse (a set
of points with intervening space) or dense (a solid line between
endpoints).  In practice this also affects value identity as the mere
presence or absense of a successor function being defined in scope can
affect how an interval or interval set is normalized, eg closed vs open or
unions are contiguous or not.  Similarly a lot of API options just exist
for one and not the other, such as counting and enumeration.  In some ways
the importance of having the 2 types is like having distinct Set and Bag
types, in that Set are Bag plus extra behaviour, as is the case that
discrete is like continuous with extra behaviour.  Note that adding this
distinction doesn't mean necessarily that we have a cross-product of types
between this and interval set/bag, or 6 instead of 3; rather, the
distinction could just be on the individual interval and then the sets/bags
simply require all member intervals to be of the same kind, perhaps.  Note
that this is where the name Range vs Interval can be used, as elsewhere done.
- Or, keep things merged as Interval etc after all since extra
normalization of integer intervals etc is simply more DWIMmy for interval
identity if = is meant to say "consists of the same members".  Intervals
over integers etc can then simply be a subtype by constraint maybe?

* Note that another interesting use of list comprehension or successable
types is generating mandelbrots etc, each iteration is the next value.

* See from the list of 100 or so functions in http://ramdajs.com/0.21.0/docs/
to provide a checklist or reminder of ones I might want.

* Notes on 20160704:
- Most Tuple ops are either of 2 kinds:
    - Predicates that ask about their headings like same/overlap/etc;
    several of these would be implemented directly by Foundation.
    - Generally non-commutative ops for projecting, substituting, extending,
    or combos of those, including projections of left-only,common,right-only etc.
- There are generally no ops for combining tuples where the anames overlap.
- Relational ops eg join should generally be built in Muldis D code using similar
logic to how Set::Relation works; first analyze how headings do or don't
overlap, then split tuples along the same lines with overlapping vs not and
then catenating appropriate pieces.
- Basic ops to help joins etc should be Homo ops both cross-join and
equi-join etc which generally result in homos of tuples of tuples etc,
and some of these would be implemented in Foundation, same as nest/un.

* Allow syntactical alternatives for Plain_Text such that one can write it
in a way that resembles the native form of Muldis D code as much as possible;
for examples:
- Allow declaring packages like this:
    package (
        identity : ...,
        uses : ...,
        materials : (
            ns1 : (
                ns2 : (
                    f1 : (function ...),
                    p1 : (procedure ...),
                    s1 : (selection ...),
                ),
            ),
        ),
    );
... and so in that case any entities in the same namespace must be declared
contiguously rather than in arbitrary places as might help documentation.
- The grammar for the above would provide the 2 options
"package {...}" or "package (...)" where the latter (...) is a strict subset of
<Tuple> syntax.
- Or, the "package" and "materials" can independently be of either format,
in which case, such a "materials (...)" would be a bit like a function body
inside wherein "x::y::z ::= foo" is like the named expr format.  THIS BEST.
- Or, mostly just have the old version but add materials {} wrapper analagous
to a routine body part of the package, and optionally allow any given namespace
level to include its constituents like with binder, or have a hybrid where
one can do the nested or directly addressed while allowing them to overlap.
- Allow users to write array/set/bag/tuple literals that contain only other
literals, that is, any time that something would be represented as a
Value_Expr or whatever its called, the corresponding Plain_Text code would
not have any sub-expressions that are function calls or named expression
references etc.  Analagously, it is like stitching Perl strings where a
string interpolation feature isn't used, so any runs of collection elements
that are just literals are represented as a collection-typed Value_Expr
which is then args to functions to combine sub-collections with ones derived
from function calls or whatever.
- Use of the "foundational" indicator would force these more like the real
code alternatives to be used.

* Don't be afraid for the Source_Code types, eg the expr node options, to
be more comprehensive or overlap on purpose with Foundation routines, for
example that certain basic Tuple operations would be canonically be expressed
in code with their own expr node type rather than a call-fdn-func node;
nevertheless, Foundation funcs for such things would still exist even if
chicken and egg means they can't all actually be used in some code, but
they can still be used by implementation routines.

* How to bootstrap Tuples:
- In Plain_Text, don't need any new syntax or keywords, we just use the
generic (...) selector syntax, so () will make a nullary Tuple, (x:y) or
(x,y) etc to make the other basics.  From there everything in Plain_Text is
just defined in terms of Foundation func/const calls.
- Foundation does provide functions for mapping String to Attr_Name and
for selecting a unary tuple etc with a name from a String etc, but generally
this is only used by the parser etc.
- Extracting Tuple names or assets can just be done by calling Foundation funcs.

* Don't use terminology like "multiple-assignment" assuming that TTM has a
specific meaning about it involving the resolution of targets that might
overlap behind the scenes; what Muldis D actually has can be more aptly
described as "simultaneous assignment" or such.

* For now, rather than providing general case of assignment LHS expressions,
just provide simply simultaneous multiple-variable assignment.
- Provide this very specific feature for procedures, where
we provide terse multi-variable assigning syntax in procedures.
- This would be special syntax with its own node type probably.
- Resembling Tuple literals, or something.

* IMPORTANT: See the long block a few points down citing ACID/CAP in its
first line, in particular the message-passing "request submitted" etc and
what leads to it.  More notes...
- It seems best to conceive that by default or fundamentally there is no
magic in Muldis D for providing the illusion of working with large datasets
that wouldn't normally be held in memory in a typical program as if they
were in memory.  In particular this means not representing entire databases
or database relvars in terms of lexical variables of routines that
conceptually might run outside an actual DBMS such as in a regular
application.  Rather, only routines that conceptually run inside the DBMS
would do this.  So any lexical variable or expression node in a routine
would conceptually represent data structures living in the local memory
space of the process where the routine runs.
- For working with large data sets conceptually living in an external DBMS,
Muldis D would feature the paradigm of sending an inter-process message
consisting of code/instructions intended to be run by the recipient.
- Generally each message would take the form of a Package value intended to
be interpreted (loaded and invoked) in the context of the recipient.  This
could be as simple as a plain routine call, or also include definitions of
things or carry associated data.
- A message is a request for the recipient to do something for the caller,
but the recipient is free to accept or decline the request.
- Generally message passing is asynchronous and involves callbacks; a sent
message includes an instructions on how to respond to the caller, such as
what kind of response is desired and who to send that response to.  An
initial response may be "heard your request" and later ones may relate to
the outcome.
- A remote and a local process can maintain a session such that multiple
messages are passed back and forth and the outcomes of several messages are
treated as a single transaction by the invoked side; indeed some messages
may involve transaction controls.
- Ideally a message would denote a self-contained transaction as applicable.
- Each side maintains a queue for messages.
- As processes within the same VM also pass messages between themselves,
there should be at least 2 message passing systems, one that is implicitly
local and lightweight, assuming both parties share the same memory space,
and the other system which is implicitly remote and heavyweight, which
involves marshalling.  Typically there would be local processes that are
proxies for the second system and they are invoked by most code via the
first system, so most code doesn't have to know for sure the locality, but
just act as if they are talking to something that MIGHT be remote.
- For version 1 all that actually needs to work is the local/lightweight version.
- The paradigm naturally translates to working with distributed systems,
where the actual work might be divided up or replicated, transparently to
the original requestor.
- Typically a "main program" interacts with the user and sends messages to
do most of the grunt work, whether in a straight Muldis D or mixed-language program.
- A Package should have an optional trait that names its "main routine" if
there is one; this would obsolete the need for a "when loaded" trigger.

* See also http://openmp.org/wp/
"The OpenMP® API specification for parallel programming"
- C# version: http://www.osl.iu.edu/research/mpi.net/
- Spark is a Java version of MP

* The concept of "two-phase commit" is somewhat related to the above.
http://rajeevrastogi.blogspot.ca/2016/06/savepoint-andpreparetransaction.html

* Explanation given on 20160606:
1.  For some context, Muldis D executes in a multi-process virtual machine,
each one analogous to either a normal program process or thread, or to a
distinct DBMS client connection, or to a DBMS autonomous transaction.  Each
process can spawn other processes which constitute independent parallel
transactions. Processes can communicate with each other via message
passing.
2.  Muldis D is fundamentally an ordinary general purpose programming
language and does NOT have a "built-in persisting ACID DBMS" in the same
sense as a typical SQL DBMS.  Rather, it fundamentally provides in-memory
data structures that are immutable and transactional, effectively, in a
manner consistent with a purely functional language.  Structures are
graph-like built add-only and if code throws an exception then aborted
structures are never visible to users.  No automatic durability is
included.  Instead, Muldis D provides ordinary access to the file system or
STDIN/OUT etc or other general purpose ways of talking with the outside
world that any general purpose language has, and the user-level code is
responsible for actually writing databases out and reading them back using
whatever method they want.  Muldis D provides built-in routines to
serialize data or code (Package values) and parse it back again, so
combining that with a write-to-file and read-to-file is one of the simplest
ways to persist a database, which otherwise is just in memory.  For faster
durability, new packages can just be deltas of prior ones, which they then
name as dependencies.  So, Muldis D is also a language you write a DBMS in,
not just a query language for that DBMS; or in other words, a default
persistence aspect is also written in itself, the language is self-hosted
there.
3.  For normal recommended usage scenarios, the mechanics of how a database
persists or otherwise managing one too large to fit in memory is abstracted
away by the message passing mechanism.  One or more processes ("server")
are actually responsible for managing a database including persistence, and
other processes ("client") representing users communicate what the users
wish to be retrieved from or modified in the database.  A message generally
takes the form of a Package value declaring an entrypoint procedure, which
is executed/interpreted in the context of the receiving process.  A message
also constitutes a transaction for the receiver, it defines a
self-contained unit of change to apply or of what data to fetch and return
or both.  A message is completely non-interactive with the sender, it must
include any conditionals on whether or not to do something based on the
then-current state of the database etc; the sender then is informed that it
either succeeded/committed or failed/rolledback with another message sent
back.  Similarly there are no open-ended transactions, they only last as
long as the server takes to process the message.  Transactions use
serializable isolation and it generally shouldn't matter what order
messages are actually applied to a database or there isn't a guarantee it
is in the order received by the server.  Message passing is asynchronous so
a client doesn't necessarily block waiting for its request to complete.
There is also an optional security model where clients/servers don't have
to trust each other, and a server could just say "denied" rather than doing
the request.
4.  And so, in such client/server setups, the Muldis D code running in
client/user processes see ACID or whatever they should be seeing as if it
were built-in to the DBMS.  How the server actually implements that is
hidden away and as such the Muldis D code running in server processes could
be written to do things a variety of different ways.  For standalone
reference implementations of Muldis D they will basically just do the file
serialization I previously mentioned, as the simplest thing that works.
For other implementations such as over SQL DBMSs or other kinds of data
stores the Muldis D code running in the server process could be translating
the Muldis D code comprising the message (a Package value) into analogous
SQL or whatever and then sending it off to the SQL DBMS, thus essentially
being a proxy for it.  Or if the Muldis D implementation is on a
distributed system the server process may be farming out the message to a
multiplicity of other computers.  All of this should be relatively easy to
do considering Muldis D's homoiconic and generally pure functional nature.
But from the client/user perspective they just made use of "the database"
via the message passing, as typical application code does with a SQL DBMS
today.

* Moved from Plain_Text.pod on 20161010:
    <pkg_entry> ::=
        <entry_point_kind> <sp> by <sp> <entry_point_rtn_name>
    <entry_point_kind> ::=
        performance | value | retrieval | replacement
- TODO.  Note that "entry" is the "entry point" for a program.  It is NOT automatically
invoked when loading a package, but is informative for an explicit post-load call.
- TODO.  Each entry point kind represents a distinct kind of routine or
distinct expectation for how it would be used, mainly in the context of a
Package being passed as a message between VM processes.
A "performance" is an ordinary main program and has no parameters.
A "value" is a constant, such as when the Package is a response message
or alternately represents a serialized database.
A "retrieval" is a function that takes a current database value as its main
input argument and results in some report derived from said.  This performs
some pure functional read-only usage of the database and is implicitly run
inside a repeatable-read isolation level or something.  It is a direct
analogy to a SQL SELECT query issued to a server.
A "replacement" is a function that takes a current database value as its main
input argument and results primarily in a derived database value (and also
in a result value for the client as desired).  This performs some pure
functional read-write usage of the database and is implicitly run
inside a serializable isolation level or something.  It is a direct
analogy to a SQL stored procedure call or set of change statements.
This list of entry point kinds is subject to change.

* ADDED ON 2016-08-08:
- Muldis D should have a core security model including jails and feature
white lists, inspired by the likes of Postgres trusted vs untrusted
languages, Unixen chroot or irrevokable privilege reduction, or Mac OS'
per-application filesystem containers or per-process feature whitelist.
- For example, Muldis D Foundation features would be split into groups, so
that say the base privileges only let one invoke pure functions and can't
do anything besides return a calculation on input, and granular optional
privileges could allow things like seeing or modifying "the database", or
using STDIO on already provided filehandles, or accessing limited parts of
the filesystem, or accessing the whole filesystem, or running other
programs, or accessing routines provided by the host language as applicable.
- Generally permissions are granted at the in-DBMS-process level, when the
process is started, and a process may only be granted a subset of the
privileges that the process which started it has.  A process may not seek
to elevate its own privileges; it must pass a message to another process
with higher privileges to operate on its behalf if desired.  The ability to
invoke other processes is also whitelisted, which can include, can only
invoke processes with or without certain privileges.
- Conceptually each process can start with either a clean slate / no
packages loaded, or it can start with a fork of the parent, starting with
all of the same packages loaded and read-only memory shared, generally we
want process starts to be a cheap operation in principle, like safe threads.
- Loading a library is something that would work for a process under the
tightest privileges assuming its source is provided as input to the
process; a functional-only process can't "go out and get" the package.
- As such, the fundamental paradigm of package use actually does have to be
logically a pure interpreter model, and not a compiler model, although an
implementation can choose to compile behind the scenes for performance.
- A main point of all this is security and resilience against attackers
injecting code either as data or as filesystem files where libraries live, etc.
- Similarly, it should be optional as to whether the "entry" routine of a
Package gets executed or not upon using the package.
- Using packages expected to just be data and not code should be handled
appropriately, including optional load config files.
- See also http://ledgersmbdev.blogspot.ca/2016/08/postgresql-plperl-and-cve-2016-1238.html
and http://ledgersmbdev.blogspot.ca/2016/07/notes-on-security-separation-of.html
for some inspiration for this train of thought.
- Orthogonal to this, there should be some way of specifying resource
limits or priorities somehow on processes, so that even say pure functional
ones can be prevented from using unexpectedly high amounts of CPU or memory
such as for a denial of service attack.  This could be a "later" thing,
though the ability to see or kill in-DBMS processes should exist early on
at least for debugging purposes.

* The concept of using sequence generators for surrogate keys, eg auto-increment,
should be strongly discouraged in principle for various reasons.
- The generators are inefficient users of DBMS resources and are bottlenecks
both for acquiring values and for DBMS replication, while in practice the actual
serial nature isn't important to the data and could otherwise be something else random.
- Something like a UUID is better suited and each record can be calculated indepedently.
- Where sequence generators are used, typically the state should be maintained
outside the main database and not be tied to its transactional mechanism.
- Surrogate key geneation should typically be done application side so the
database can just work with the records as if everything was using natural keys.
- Handling of surrogate key generation should be done similarly to handling random numbers.
- There should be some procedure (because it is officially non-deterministic)
that takes a list and applies new generated values to it; a function can be
used iff the values are in an array (including a tuple array) AND the sequence
generator is deterministic, eg a serial, and that works best if the DBMS
has a "reserve n" or "increment by n" in its native sequence generator.
- Making sure sequence generation is untied from record inserts makes it
easier for users to choose from various means to generate said sequences.
- See also https://www.depesz.com/2016/06/14/incrementing-counters-in-database/

* Actual random number/data generators should be procedures that produce an
array of the random values, and these can then be applied to other records
using an array-join like thing that joins on common indexes.

* Consider using the greek-letter-looking symbols often used to denote some
relational operations like selection, projection, and rename.  However note
that typical usage of those in maths has the relation as the right-hand
argument and the attr names etc are on the left (as subscripts), though the op name leftmost.
In particular, we don't yet have a symbolic rename or restriction.

* Note re parsing:  After splitting nonquoted by character class, seek
semicolons and parenthesis first.  Every material/etc declaration has a
semicolon after it / doesn't cross a semicolon.  All bracketing characters
are paired within a material.  Try counting brackets in inter-semicolon
spaces to try and detect pairings complete vs unbalanced start vs unbalanced end.
No need to distinguish [{( from each other, just opening from closing,
initially, though on second pass pay attention to detect errors.

* Consider making the catalog an explicit Structural/Tuple variable, and
the act of registering/activating a package is nothing more than adding it
to the variable.  Similarly, as functions no longer necessarily have to be
named and could be passed around by value, the main purpose of this catalog
is to provide an easy way for code that is otherwise anonymous and passed
around to be factored into named reusable parts, which are then typically
looked up in said catalog.  As such, conceptually routine/type lookup or
composes/implements resolution takes place in the user / Muldis D layer,
by (possibly) Muldis D code that is interpreting or compiling Muldis D code,
in which case the Foundation corresponds to the CPU instruction set.

* Muldis D makes catalog stuff explicit.  All data definition is data
manipulation, producing values that are source code, canonically values of
the Package data type.  Nothing can invoke code just because it exists, one
has to explicitly register/activate it, by calling a routine passing the
Package value, which then makes its code invokable; one can also call a
routine to unregister a Package.  Orthogonal to the executable status, a
Package can be saved to disk or loaded from disk, as regular data, and
that's where you get persistence. Executing or persisting a Package is
optional.  This is the fundamental paradigm.

* Muldis D generally takes concerns related to either ACID or CAP and
hoists them into the user space so Muldis D programs or databases can more
explicitly define what behaviours they want.
- Muldis D itself neither assumes nor leaves implementation-defined
particular consistency or independence or availability or concurrency or
synchronicity or partitioning or durability or atomicity or versioning etc
concerns, but rather expects user code to be explicit about what of those
it expects or account for implementation variability.
- Muldis D is around 99% pure functional and expects all things of
significance to be encoded explicitly as values or as introspectable
functions, whose unified common representation is Package values.
- A complete "database" state is represented as a Package, and each Package
is logically a node in a graph that defines a "database" state either
completely self-defined or defined in terms of a function applied to the
"database" state(s) defined in one or more other Packages.
- There can be anywhere from zero to many peered Muldis D DBMS processes,
either on the same physical machine or spread out over a network, and each
one has in memory a set of zero or more Package values; each process can
exchange Package values with other processes, or store them to or retrieve
them from process-external storage, of pass them to or from client
applications etc.
- In the general case, the DBMS processes are unique and don't necessarily
have the same concept of what the "current" database value is, and they
interact with each other asynchronously.  A client application should think
of using the DBMS like using a queue; change requests go in one end and the
results of those change requests come out the other.  A client should not
assume that any change they made has stuck until they get the appropriate
response out of the queue.
- Internally, idiomatically have a DBMS process responsible for managing
the official database state (which may or may not delegate), and then clients
are represented by other DBMS processes, which send messages to the first one.
Perhaps there is less reliance on tied variables in this case?
- The DBMS processes can operate at different levels of trust, where they
may or may not trust or accept what their peers say.  Interaction between
them could conceivably be about a chain of claims or he-said-she-said
rather than just cells within a brain that implicitly trust each other, or
alternately they could implicitly trust.  But regardless, with multiple
potential writers making changes at once, there would be policies for
merging their change attempts, which may be clean or less so.  A version
control system like git is a good model to look at.
- Best practice for normal DBMS users is each change request ultimately is
encoded as a function that will take some prior database value and applies
various changes conditionally thereby deriving an updated database value.
Due to concurrency issues, the actual database value seen for input to such
a transformation function may not be the same database value seen by the
client application that composed the change request in the first place, and
so the function needs to be appropriately resilient to variations in the
input, such as other records added/changed/removed/present by some other
user, and either make its own changes in a way that appropriately maintains
the changes by the other users, or to fail gracefully so telling the client
to please try formulating a new function to try again.
- The key here is to not leave it up to the DBMS to decide how to resolve
possible conflicts, eg 2 users updating the same record, rather each user
defines their update request to explicitly say how to act in the presence
of a possible update by the other, and the DBMS makes no guarantees of
whose function runs first.
- Each DBMS in the general case would just respond to a client request with
"request submitted" but the client should not assume anything about the
state of the database until they're separately told their request was
processed and to read the result.  The DBMS could also block on that result
instead as the implementation chooses.
- Things like talking to other processes or with filesystems etc could be
implemented either in Muldis D itself for maximum user control (using eg
sockets exchanging octets), or hidden from the users in
implementation-specific ways, basically both options would be available, eg
for abstracting away where DBMS processes are and how they are talked with.
In any event, typically a specific DBMS process would be a user's proxy,
running their session, for the group at large.
- Using possibly distributed implementations should benefit from this design.
- Package names in the general case could be UUIDs or such, ones representing
database versions, and we can leave human-readable names for libraries etc.

* FIRST PRIORITY:
- Make the core have basic Unicode knowledge after all.
To be specific, make it know what the range of valid code points is, and
implement UTF-8 encode and decode in core.
- So core has support for the Unicode UTF-8 encoding only, which is a
proper superset of ASCII, that the internal "maximal" Text is supposed to
match anyway, and it has no endian-ness variations.
- Core does not know about any other encodings, whether UTF-7,16,32 etc or
any UCS or any Latin or whatever; those are still left to other modules.
- Core does not know anything about Unicode normal forms or character
properties; it can't normalize NFD or NFKD etc; it can't do upper/lower/etc;
its really all that stuff where the actual Unicode complexity is.
- Since UTF-8 encode/decode isn't actually a complexity burden on its own,
it is safe for the Core package itself to have Unicode, so we can merge in
the Core::Aliases package into Core.
- Core may still want to exclude any Unichars that can change under
normalization and just have those always represented by a single code point,
which already happens I think.
- We still retain ASCII names for everything system-defined.

* See https://tools.ietf.org/html/rfc3629 for the RFC defining UTF-8, 2003
version (latest?).  "UTF-8, a transformation format of ISO 10646".

* See https://en.wikipedia.org/wiki/UTF-8 for bunches of useful info on how
to decode/encode UTF-8 including about patterns and valid and invalid octet
sequences etc.  See also about "overlong" etc.

* Terminology note string vs seq, from Wikipedia article on "substring":
- A substring of a string S is another string S' that occurs "in" S. For
example, "the best of" is a substring of "It was the best of times". This
is not to be confused with subsequence, which is a generalization of
substring. For example, "Itwastimes" is a subsequence of "It was the best
of times", but not a substring.

* See also http://www.finitio.io data language by Bernard Lambeau.

* See also http://setl.org/setl/ .

* Nice piece of work, this:
http://www.theregister.co.uk/2016/01/13/stob_remember_the_monoids/

* In more recent Perl, functions with a prototype of () might inline automatically.
https://metacpan.org/pod/perlsub#Constant-Functions

* Lessons of JSON::Typist might be useful https://rjbs.manxome.org/rubric/entry/2101 .

* A module SQL::Functional exists as of 2016 Jul 31 or so.

* Quoth the raven:
"In Haskell all operators are as-if user-defined.
And therefore even library-supplied operators
(like arithmetic and comparison)
can be overloaded, redefined,
and their fixity changed within a module."

* See this on how to release a Raku module including metadata examples,
we can adopt such things:
https://doc.raku.org/language/modules
aka
https://docs.raku.org/language/modules.html

* Valuable to read: https://doc.raku.org/language/faq
- Including the start about versioning.
- Including https://docs.raku.org/language/nativecall .
- And https://github.com/niner/Inline-Perl/ .
    use Inline::Perl;
    use DBI:from<Perl>;
    - "Raku' use statement allows you to load modules from other languages
    as well. Inline::Perl registers as a handler for the Perl language."
    - In theory that's how I'd best make Muldis D callable from Raku.

* Note that a Perl "version" object internally uses the sequence of
integers scheme, but the maturity is indicated separately:
    bless( {
      'original' => 'v1.2.3_4',
      'alpha' => 1,
      'qv' => 1,
      'version' => [
        1,
        2,
        3,
        4
      ]
    }, 'version' );

* The RPerl (Restricted Perl) project looks interesting, it compiles a
subset of low-magic Perl to C++/etc for a 7-fold or hundred-fold performance
improvement.
- http://rperl.org/faq.html
- "Perl 11 is the philosophy that Perl should be made modular ("pluggable")
on the 3 primary levels of source code lexer/parser, compiler/AST, and
runtime/interpreter/VM. Perl 11 is not an actual version number Perl
release like Perl 4, Perl, and Raku. One of the goals of Perl 11 is to
work toward unification of Perl and Raku, thus 5 + 6 = 11. There are
multiple independent projects under the Perl 11 umbrella, one of which is RPerl."
- http://perl11.org

* https://en.wikipedia.org/wiki/Primitive_recursive_function

* https://en.wikipedia.org/wiki/Ackermann_function

* https://proofwiki.org/

* https://en.wikipedia.org/wiki/Relational_operator

* http://www.dagolden.com/index.php/2633/no-more-dirty-reads-with-mongodb/
See write concern level and read concern level etc.

* https://www.depesz.com/2016/05/04/picking-task-from-queue-revisit/
SKIP LOCKED for implementing queues.

* http://bonesmoses.org/2016/04/29/pg-phriday-derivation-deluge/
About different Postgres forks that exist with a focus on scaling.
Also: https://wiki.postgresql.org/wiki/PostgreSQL_derived_databases

* http://www.depesz.com/2016/03/23/waiting-for-9-6-support-parallel-aggregation/

* "PostgreSQL does not provide any infrastructure for capturing sequence changes [but is trying]".
http://blog.2ndquadrant.com/pglogical-1-1/

* http://www.cybertec.at/2016/06/postgresql-instance-level-encryption/
A patch for Postgres gives it full-cluster encryption at file block level.

* https://www.commandprompt.com/blog/pgconf_2016_kicking_the_donkey_of_postgresql_replication/
- Linux kernels 2.6 (EOL) and 3.8+ are best.
- Don't use Slony except for complicated or obscure setups.
- If you think you need multi-master you likely don't understand the problem.
- Replicate from master to 1 slave then to other slaves (implied by Y diagram).
- Pages of how-tos / checklists for setting up built-in replication features.

* https://developers.slashdot.org/story/16/07/14/1349207/the-slashdot-interview-with-larry-wall
This has lots of good quotes, including following.
- "First, let's clear one thing out of the way. The heart of Perl is not
some particular syntax, but a way of thinking about how a language works
together with itself holistically. Mechanisms we use in natural languages
come into play, such as metaphor and simile ("A class is just a kind of
package, a method is just a kind of function."), as does the idea that your
program should talk about the problem you're trying to solve, not just
describe its own structure (given/when/next instead of switch/case/break).
If you like the way Perl works inside, I think you'll come to like the
way Raku works inside even better."

* In a sense I reinvented something Raku has, which is a family of operators
including {'.','.?','.='} etc but the right-hand side of those is a method.

* https://6guts.wordpress.com/2016/02/09/a-few-words-on-perl-6-versioning-and-compatibility/

* https://github.com/agentm/project-m36 another TTM project.

* http://search.cpan.org/dist/Specio/ - Type constraints and coercions for Perl
Experiment that may be a common factor for Moose/Moo/etc in the future.
"At it's core, a type is simply a constraint. A constraint is code that
checks a value and returns true or false. Most constraints are represented
by Specio::Constraint::Simple objects. However, there are other type
constraint classes for specialized kinds of constraints."

* https://metacpan.org/pod/Type::Tiny is recommended meanwhile for something
working across Moose/Moo/etc, latest version from 2014.

* https://en.wikipedia.org/wiki/ISO_31-11
on mathematical signs and symbols for use in physical sciences and technology

* https://en.wikipedia.org/wiki/ISO_80000-2
on Quantities and units — Part 2: Mathematical signs and symbols to be used in the natural sciences and technology

* http://www.r2d3.us/visual-intro-to-machine-learning-part-1/

* An introduction to the new more modern Apple File System.
https://developer.apple.com/library/prerelease/content/documentation/FileManagement/Conceptual/APFS_Guide/

* See https://github.com/timo/iperl6kernel and its inspiration
http://ipython.org/ipython-doc/rel-0.13.1/development/messaging.html .

* https://www.sqlite.org/draft/sessionintro.html
SQLite Session Extension included with version 3.13.0.
This is about packaging a changeset of arbitrarily many table updates
useful for patching or undo, analagous to "patch" or a version control system.

* On how SQLite was and is based on Postgres specifically:
- http://www.pgcon.org/2014/schedule/events/736.en.html - SQLite: Protégé of PostgreSQL
- http://use-the-index-luke.com/blog/2014-05/what-i-learned-about-sqlite-at-a-postgresql-conference
- Richard Hipp gave keynote at PgCon 2014.

* Note: A "covering index" is an index that contains the data you want to
select, that is all the tuple attributes you want to select are in the index itself.

* https://en.wikipedia.org/wiki/Kotlin_(programming_language)
Kotlin makes a distinction between nullable and non-nullable datatypes. All
nullable objects must be declared with a "?" postfix after the type name.
null-check must be performed before using nullable value. Kotlin provides
null-safe operators to help developers:
?. (safe navigation operator) can be used to safely access a method or
property of a possibly null object. If the object is null, the method will
not be called and the expression evaluates to null.
eg: foo ?. bar() ?. baz()
?: (null coalescing operator) often referred to as the Elvis operator:
- ACTUALLY, other languages like Smalltalk and Swift have similar features.
- See also https://en.wikipedia.org/wiki/Pyramid_of_doom_(programming) .

* Interesting article on methods for in-database queues.
http://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5

* https://pyrseas.wordpress.com/2013/10/02/multisets-and-the-relational-model/

* Add one or more features to assist in making some otherwise fatal
actions non-fatal, while staying in pure functions.
- Example use case, make it easy to define Interval type that is as
forgiving as Set, such that one can test membership of a value in the
Interval and let it just result in false when the ranges are defined over
another type, this instead of throwing a type-mismatch exception because
there is no implementation of in_order(Foo,Bar) for the combo in question.
- An option is to have a simple way to declare a fallback for a virtual
function, something that is chosen if no specific composing type handles
the interface in question.  This gets into the realm of overrides.
- Another option is to add either of several expression node types.
    - Example, "foo x bar" that says if foo throws an exception such as no
    matching type sig for a routine arg, return bar rather than dying.
    - Example, variants of generic routine calls analagous to generic
    Excuse routines:
        - args -->? rtn results in true iff rtn matches something that accepts args as valid input
        - args -->! rtn results in a particular Excuse in situations where
        prior would be false and results in the normal result if prior would be true
        - actually just make first a builtin/expr node type, latter should be function
    - Above is just for input type mismatches, not failure to math a rtn name.
    - To avoid over-reaching, those wouldn't neuter indirect call failures
    like a "foo>bar" call, probably just use on a direct in_order() call.
    - Actually, -->? also useful in constraint defs eg also in Interval
    where one might require "\%(lhs, rhs) -->? \@in_order"
    which is possibly more applicable than asking if they are the same type.

* Postgres and MySQL (and in theory the standard) have distinct built-in
types for representing byte arrays and bit arrays respectively; the latter
are called BIT [VARYING] while the former are called BYTEA or BLOB.
- Calling the latter Blob is obvious.
- The former I could call Bits.
- MySQL has precedent where long blob literals can be split like 'XX'<cr>'XX' etc.

* Of some interest: https://hackage.haskell.org/package/HaskRel

* From "Webscale's dead; long live PostgreSQL!"
by Joshua D. Drake
https://www.youtube.com/watch?v=HtVLQD38yvU
- PgConf US 2015.
- "Webscale is a marketing term for everything professionals have been doing
for decades.  Creating highly scalable systems."
- Amazon literally charges twice as much for the same thing as otherwise.
- Make replication / hot standby asynchronous unless you know what you're
doing, else a loss of one slave makes everything wait.
- PgPool valuable, transparently make writes to master, reads from slaves.
- Minimum Postgres version that is reasonable to use is 9.2,
not just a lot faster, but like Prius vs Farari.
- Connection pooling important, Pg connection opening is expensive.
- BDR - Bi-directional replication without million dollar licence fees and
support contracts, with ACID compliance, proper controls, scaling to 48 nodes.
- Banks like / ok with "eventually consistent".
- Postgres XL if you just want sharding.
- Lists about 33 things that there are foreign data wrappers for.
- You can process credit cards inside Postgres, but don't.
- Presenter hates PL/pgSQL, "so does everyone", write functions in other
languages, eg Perl, Python, V8, etc.
- Using DO lets you have scripts run in the database without living there.

* http://bonesmoses.org/2015/12/11/pg-phriday-postgres-xl-and-horizontal-scaling/

* http://sqitch.org - Sqitch is a database change management application.

* https://metacpan.org/release/Time-Moment
More appropriate than Perl DateTime perhaps or as a model for Muldis D.

* Next to C# and Perl, probably Javascript should be a top priority for
porting Muldis D to, for the purpose of providing easy user demos of the
langauge, where users can try it out in their web browsers simply and
without installing anything.  The UI could just be a pair of editable text
fields, one for an input program package, prefilled with an example and
other examples available, and another field with the result.

* There are formally independent versioning namespaces for MD,MDF,MDPT,MDSL
so items in each don't need to explicitly say which they are.
- Consider potential of multiple foundations, that is multiple languages.
- All fully-qualified versions now just have 3 main parts: base name, authority, vnum.
    - This is all the VERSIONING in D.pod should generally be saying, and
    in particular it can dig into the meaning of authority and vnum which is shared.
    - Detail of the first part is more left up to each component spec to explain.
    - Generally VERSIONING can be chopped down quite a bit, not try to be so flexible for eg N parts.
- Merge first 2 parts of grammar name, and add a first part for foundation, so...
- Syntax (per file):
    Muldis_D_Plain_Text:"https://muldis.com":"0.300.0"
- Behaviour (per package):
    Muldis_D_Foundation:"https://muldis.com":"0.300.0"
- Vocabulary (per package):
    Core:"https://muldis.com":"0.300.0"
    Core::Aliases:"https://muldis.com":"0.300.0"
    Core::Math:"https://muldis.com":"0.300.0"
    ...
- Requiring explicit 'Muldis_D' as its own token is largely redundant; each
grammar or whatever will match or not holistically, and a lot of grammars
are for emulating things without such declarations anyway.

* TODO:
- Consider renaming FDN entities to remove the FDN__ prefixes; these aren't actually
needed as Foundation entities live in a separate namespace with different
call syntax.  Allow corresponding FDN/FDC names to more closely match where
helpful, though type prefixes probably still valuable to keep in FDN.
That being said, FDN prefixes are still valuable from general documentation
standpoint, unless all references prefix with foundation type foo or core type foo etc.
- Favor the names users would most
often actually use as the initial names, at least for types; routines can
probably stay as they are; in particular, symbolics will always be aliases or shims.
- Flesh out Foundation type/op docs to be more self-defined; in particular,
use the old Catalog_Types.pod as inspiration for documenting the code-defining
subtypes, where their structure is defined using just words and not MDPT;
it is okay to have more duplication between MDF and MDC docs.
- Generally make MDF more pleasant to use directly, even if most users won't,
or at least possible to understand completely in isolation more or less.
- MDF types still need to be italicized as formally they have no names that
are referenceable in code, since MDF has no system-defined contracts,
all contracts are user-defined.

* Dependency relationship like this:
  ---------------------------------
  |  logical MDC  |  MDPT parser  |
  ---------------------------------
  |              MDF              |
  ---------------------------------
- So MDC and MDPT are siblings that each depend on MDF.
- MDF is the semantics without external physical syntax.
- MDPT is the external physical syntax without necessary but with implied semantics.
- MDC is a toolkit built over MDF, logically with no physical syntax,
but that just so happens to be written in MDPT when canonically externally presented.

* Formally redefine "Muldis D" as an aggregation of language components
that are quasi-independent but designed to work together.
- These components each have their own specification documents.
    - MDF - Muldis D Foundation
        - Defines the fundamental paradigms and features and semantics of
        the language, including what's in Overview.pod and Foundation.pod.
        - Defines the fundamental types and operators that would normally
        be implemented by each language host / DBMS.
    - MDPT - Muldis D Plain Text
        - Defines a concrete syntax for writing Muldis D code in.
        - Parser can be implemented by a package written in Muldis D but
        would also need to be implemented by host languages at least for
        bootstrapping; the latter would be the initial normal way.
    - MDC - Muldis D Core
        - A regular package written in Muldis D that defines what normal
        users would consider the language core types and routines that are
        always available.  The package is probably just named "Muldis_D".
- MDF makes no assumptions about MDPT or MDC, but MDPT and MDC each make
some assumptions about MDF.  That being said, the MDF type/routine docs
assume people have read the MDC docs as the latter explain more of commons.
- Each of {MDF,MDPT,MDC} has its own version number series, that only
increments when that specific part changes.  Code written in Muldis D would
separately declare which version(s) of all 3 it is compatible with.
    - Required MDPT version is declared once at the file/char-string/etc
    level and not per package within, as it is irrelevant for the latter.
    We should probably rename this "Muldis_D:Plain_Text:..." from
    "language_name" to "syntax_name" or "syntax_indicator" or some such.
    - Required MDF version is declared once per every package.
    - Required MDC version is declared once per package that uses it.
- The Muldis D spec aggregation also has its own version number series
which increments when any of its component specs increment.  The aggro is
more of a meta-spec which partly says which versions of the components are
expected to be compatible with each other, and also talks about the wider
ecosystem including matters of versioning, third party versions, and so on.
- In some respects, MDF is the only pure spec, where all implementation
code is distributed separate from the spec.  MDC is the opposite, it is a
completely coded implementation that is documented.  MDPT crosses into both
worlds, or more likely is also a pure spec, albeit one that regularly makes
mention of MDC for contextual purposes even though it doesn't depend on it.
In fact MDPT really is best to be a pure spec, and then separate impls
whether as Muldis D packages or in the host can each be separate from it.
- Each system-defined Muldis D package also has its own version num series,
such as Math or whatever.
- The Muldis::D CPAN distro groups together everything in the aggregate for
convenience sake to users, but it could conceivably be split up.  Probably
the best time to do a split up is around the same time we declare 1.0 of at
least one component, but this is subject to change.
- The line is also blurred when bringing in reference implementations, as
far as what is bundled with what when distributed.

* Just do mapping of Integer:Text etc all high level, eg an integer literal
represented by an array of ascii code points for each digit/hexit/etc can be
converted with a simple map(-).map(*).reduce(+) or a loose analogy.
- We don't need low-level text to num etc, or any such can override rather
than being part of Foundation API, same as overrides for floats etc.
- This code only runs at parse time in typical cases, having many num
literals is rare in non-dump code.
- Have generalized concept of a string/array-of-int representing positions
in a numeric literal, little-endian so value is reduce of `ary[i] * base ** (i+1)`
and each array element is in 0..1, 0..9, 0..15 etc after subtracting 31 or
whatever and reversing array element order.
- Users can write these various intermediate values directly if they want.
- Same concept reusable to map binary numbers, so eg 0..255 elements or
-128..127 as applicable, same for big-endian and little endian but array
is or isn't reversed before the map-reduce.

* On implementations:
- Tuple - low cardinality
    - flat list, rewrite with each change
- Array - high cardinality
    - tree/graph with new root node to represent each distinct derived value
    - each object representing a 'value' has
        - (root) branch node that is an ordered list of either leaf nodes or
        same-structured branch nodes (recursively)
            - a leaf node is one of these:
                - a single element representing a 'value'
                - a sequence of same elements represented by 1 copy plus multiplier integer
                    - prior case could be just this with multiplier of 1
                - an ordered list of non-big integer elements
                    - to tightly implement strings/blobs/texts
        - a count of child elements for quick access, so count() is fast
        - ?optionally extra indexes on value like with Bag?
    - the structure could also be used simply to segment very large strings or lists
    - maybe have low-level indicator that we want to ensure elements all unique
- Bag - high cardinality
    - tree/graph with new root node to represent each distinct derived value
    - structure possibly exactly the same as Array but elems not considered ordered
    - definitely support extra indexes on value
    - should have low-level indicator that we want to ensure elements all unique
- Provide implementation hints analagous to locks that say things like
whether a particular value is only intended for a single use, and so the
next time it is input to a particular operation only the result and not the
input needs to be kept.  For example, when adding a number of elements to
a collection we can effectively just mutate the one we have rather than derive another.

* enum is the successable - pred/succ not defined as 'by one'
    - Raku specifically defines pred/succ as adding/subtracting 1, and --/++ are wrappers for it
    - maybe I should consider renaming my Enumerable routines to prev/next?
- succable - succ/inc/++/pred/dec/--/zero/one/neg_one
    - given ints is closed within ints
    - ditto rats and above
    - formally mathematics only defines successor for whole numbers, not fractions, decimals, or negatives
- addable/plusable - opposite, plus, minus, absolute, etc, also ++/-- defined as 'by one'
    - Raku has operator precedence category named Additive
    - given ints is closed within ints
    - ditto rats and above
- multiplicable/timesable - multiply, divide both kinds, remainder, gcd, lcm
    - Raku has operator precedence category named Multiplicative
    - given ints can also produce rationals
    - given rationals is closed within rationals
    - ditto algebraic complex and above
- exponentiable/powerable - powers and roots (not logs)
    - given ints can also produce rationals
        - iff exponent is negative
    - given rationals can also produce algebraic complex
        - iff base and exp both negative, or something
- tetratable - leave that one to ::Math

* On classifying collections ... or numbers:
- homogeneous vs heterogeneous
    - homo for Array, Set, Bag, Interval, etc
    - het for Tuple, etc
- discrete vs continuous
    - can collection values be enumerated/iterated or not
        - if so, is there a canonical order for the iteration or not,
        that is, do we have a canonical mapping to a Positional;
        if not, iteration in a procedure context order is implementation-dependent/quasi-random
    - discrete for Array, Set, Bag
    - continuous for Interval
    - maybe make Continuous a type rather than Discrete, or do both and
    expect they wouldn't both be used but in some strange but not impossible situation
    - lets say that while Discrete can be bunches of Any in general case,
    Continuous must entirely be Orderable composers, but in particular must
    be the same Orderable composer, sort of, as defined by the existance of
    an is_order routine; Continuous requires that every pair of defined
    endpoints must have is_order defined that matches them
        - in particular Interval at least would have that constraint
    - if one wants to mix and match types in a Continuous, they'll have to
    define another type that is a Discrete collection of Continuous elements,
    that is a set of sets
        - this is also how relations with interval elements would work
    - so uh we DEFINITELY want Positional to be orthogonal to Discrete/etc rather than a subtype,
    maybe of Homogeneous too, in which case we're saying Homogeneous is more, uh...
    - actually we want Text to not have the concept of elements, only of substrings
- ordinal numeric point vs bucket semantics
    - date is more bucket
    - datetime is more point
- allows duplicates vs doesn't
    - allows for Array, Bag
    - doesn't for Set
    - ostensibly doesn't for Interval, but might be some variant that does
- integral weights vs real weights
    - integral for Array, Set (=1), Bag
- members addressable by position or not
    - yes for Array
    - no for Set, Bag, Interval
    - but what is "position"?  does it have to be an integer with a fixed step size?
        - is a Text considered positional, since doing it properly uses an abstract CharPos or whatever, see Raku
        - is Textual and Blobby considered subtype of Positional or orthogonal to that?
        - in some sense a Textual is continuous rather than discrete when looked at abstractly, like a line being sliced
        - What Would Raku Do?
        - maybe Positional should be orthogonal to Discrete, same as Setty is
    - have Position interface to go with Positional interface, they're a pair
        - composed by Integer to go with Array
        - composed by Text_Pos? to go with Text, Blob_Pos? for Blob
        - don't need StrLen analogies as regular ol' Interval over Position does it
    - apparently Raku 'Blob' is a role now, which does (disjoint) Stringy and Positional
        - "The Blob role is an immutable interface to binary types, and offers
        a list-like interface to lists of integers, typically unsigned integers."
        - Blob is composed by Buf: "A Buf is a mutable sequence of (usually unsigned) integers."
        - so if I go with that, I don't need String, I can just call that Blob or something,
        they both just mean array of integers basically
        - Raku Str does Stringy
            - Str: "String of characters"
            - Stringy: "Common role for string types (such as Str) and types that can act as strings (such as Cat)."
- positional are dense vs not
    - array values are fixed length
    - require use of [insert,catenate,replicate,etc] to increase the length of an array
        - those 3 are all low-level ops
    - ary.n etc accessors can only replace elements, not add or remove them
    - Array is formally dense
    - Array can be implemented in a sparse fashion using something like run-length encoding
        - when several identical consecutive elements, store element once with a multiplier integer
        - analagous to conception of a bag, but ordered
        - replicate is low-level to easily exploit this implementation possibility
        - if one wants to insert ary[1_000_000] elem first conceptually,
        they first say something like ary.insert_replicate(null-marker,999_999)
    - Array possible implementation is a list of lists, where each inner list
    is either a regular list of items each stored 1:1, or it is an interval
    that just says how many copies of an item
        - length of whole array is sum of logical lengths of sub-arrays
        - this is loosely analogous to a B+ tree or something
    - Arrays overall have several likely-used implementation format alternatives
        - dense regular array of plain integers, representing octets or code points
        - dense regular array of arbitrary objects
        - tree-like structure for example as previously described
- members have probability of being in collection vs being absolute yes or no
    - absolute yes for Array, Set, Bag, Interval
- finite vs infinite member count
    - finite for Array, Set, Bag
    - infinite for Interval
        - note, there is also the concept of multi-dimensional intervals, eg a plot of land
- quantities of units or not
    - not for Array, Set, Bag, Interval
    - yes for Quantity, ... see also general case that supports eg years+mons+days etc ... is this symbolic math?
        - on one hand, Quantity should just be built over,
        on the other hand, do we consider Blob, Text to implement these APIs
        in full?
        - note, Text/Blob are not Quantity, their numbers are nominal or ordinal,
        emphathetically not cardinal like in a quantity
- can test if an individual value exists in collection or not with boolean result, including exists within range
    - yes for Array, Set, Bag, Interval
- can ask multiplicity of an individual value in collection with numeric result, even on continuous collections (overlapping ranges)
    - yes for Array, Set (=1), Bag, Interval (=1)
- what members exist defined by a general predicate expression, like with a type definition, or not
- can add an individual value to, or remove an individual value from, collection, or not
- can ask if collection is a super/subset of another, or not
- can ask if collections are disjoint, partially overlap, or one is contained wholly in another, or not
- can collections be unioned, intersected, added, differenced, etc
- can collection be filtered with 'where' or not
    - in general case, expands its membership predicate with an 'and'
- can collection be transformed with 'map' or not
    - in general case, expands its membership predicate with an expression transform, or something?
        - but normally the 'map' is in terms of individual elements, so may not work there
- can collection be 'reduced' or not
- note C# uses enumerator/enumerable for what Raku calls iterator/iterable
- ?fuzzy logic vs not?
- for numbers, single dimension/real versus multi-dimension/complex/etc
- ?for numbers, individual value vs range?
- note, sequence generators in general are related but are optionally built over this,
not part of it, because they have a state related to the ad-hoc iterating;
the only sequence generators "built-in" are for Enumerable types like integer
where the previous value is all you need to know to get the next one
- On Raku numbers:
    - "Int objects store integral numbers of arbitrary size. Ints are immutable."
    - Rat does Rational does Real does Numeric
    - Rat has 64 bit denom, arbitrary numer; FatRat is also arbitrary denom.
    - "Rational is the common role for numbers that are stored as pairs of
    numerator and denominator. It is parameterized by the types of the numerator and denominator."
    - Real: "Common role for non-Complex numbers."
    - Numeric: "Common role for numbers and types that can act as numbers."
    - Num does Real etc.
    - "A Num object stores a floating-point number. It is immutable.
    On most platforms, it's an IEEE 754 64-bit floating point numbers, aka "double precision"."
    - Complex does Numeric.
    - Complex: "Represents a number in the complex plane."
- In Raku, purpose of Cool role is to be either a number or a string as needed,
so basically like the Perl scalar; numbers and strings compose it.
- Various math readings say that Fractions is the complement to Integers
where both are Rationals; likewise, Rationals and Irrationals complement
under Reals, and Reals plus Imaginaries are disjoint subsets of Complex.
- See also "complex number venn diagram" and "algebraic number"
- See also https://en.wikipedia.org/wiki/Algebraic_number and links at bottom.
- real algebraic number is a subset of real and superset of rational, eg sqrt-2
- algebraic number is subset of complex where both parts are like the prior,
something about integral roots of rational coefficients
- See also http://foldoc.org/algebraic%20data%20type which is a related
concept of a sort, a "sum of products type", which is defined in Haskell
like how we may define a "union type".
- transcendental number is the real or complex that are not algebraic, eg pi
- So http://mathworld.wolfram.com/Mantissa.html sez
"For a real number x, the mantissa is defined as the positive fractional
part x-|_x_|=frac(x), where |_x_| denotes the floor function.
For example, for x=3.14159, the mantissa is 0.14159."
- See http://mathworld.wolfram.com/Floating-PointNumber.html and stuff
and http://mathworld.wolfram.com/Floating-PointArithmetic.html and stuff.
- Now that it comes to it, it would seem that how I should be representing
floating point numbers all along is to include in the value a count of
significant figures.
- Regarding a floating-point number, which seems more of a computer concept,
the leading part is called the "significand" and stuff...
- So lets make a Float have 4 parts, generalizing an IEEE float:
    - r - radix (integer >= 2)
    - p - precision (integer >= 1), in positions in the particular radix, eg in bits or digits
    - s - significand (integer in -(r^(p-1))..(r^(p-1)))
    - e - exponent (integer)
- We normalize so exponent and significand balance out, such that exponent
has precision subtracted from it to account for significand being an integer
rather than a fraction or something.
- So a Float's numeric value is: (s*(r^(-p)))*(r^e)
    - eg, {r=2,p=2,s=3,e=0} = 3/4 = 0.75
- But if we do that, the question arises of what should we do if asked to
do math between values with different radix or precision.
- Alternately we could skip the precision component and that problem goes away:
    - s - significand (integer)
    - r - radix (integer >= 2)
    - e - exponent (integer)
- So a Float's numeric value is: s*(r^e)
    - eg, {s=3,r=2,e=0} = 3
- But then there's the question of how valuable said type is in practice.
- Perhaps the simple solution is just to boot Float from core, let extensions
do it; the generic Fraction can handle it anyway, albeit taking more space
with values unlikely to appear in normal usage.
- Extensions have lots of choices, eg any of the above, or even a symbolic, etc.
- A polynomial always has a rational result for rational variables/constants;
"A polynomial is an expression that is the sum of a finite number of non-zero terms,
each term consisting of the product of a constant and a finite number of
variables raised to whole number powers."

* Useful info: https://en.wikipedia.org/wiki/Rounding .

* See also http://docs.oracle.com/javase/8/docs/api/java/math/MathContext.html
which includes Java using a similar design to what I do, C# does also.

* Its important to note that fixed-scale (even unlimited precision)
rationals are in a strange place mathwise in that they are not closed under
multiplicative or higher math operations that unlimited-scale+precision
fractions are, rather they are only closed under additive operations.  So
they are somewhere in a triangle whose corners are integers and fractions
and floating-point numbers.  In practice, dealing with them on one hand is
supposed to be like integers or quantities, given what they are used for,
but in the general case it is like dealing with floats but with different
truncation rules.  Because of this, while one might conceivably want to put
a generic fixed-scale fraction type in core, it shouldn't be due to the
variety of possible behaviours and hence variety of required operators to
deal with them, such as how to deal with repeated operations.
- On the other hand, perhaps it could be done if the rules are like this:
    - Scaled is a pair of integers like Fraction but with different
    normalization rules.
    - Scaled are NOT coprime and the actual num/den pair is significant
    for value identity.
    - Scaled has a fixed step size per each distinct denominator, which is
    simply incrementing or decrementing its numerator.
        - We can haz add a new operator pair specific to this case.
    - Math with Scaled+Fraction inputs are not defined, one must convert
    first; this ensures deterministic results when adding say 2/4 fraction
    with 2/4 Scaled, do you get 2/8 or 4/16 Scaled, we avoid that problem.
    - Math with Scaled+Integer exist in very limited instances, equal to
    how a few Fraction have Integer, specifically those related to division
    or exponentiation; while for Scaled we conceivably could add
    multiplication to that list, its probably better that we don't, and
    require users to explicitly convert the integer to scaled first.
    - Additive operations have the constraint that denominators must already
    match, and its a fail if not; so additives are all closed within a denominator.
    - Multiplying Scaled by Scaled - result denominator is product of input
    denominators with no normalization; inputs don't have to match.
    - Fractional dividing Scaled by Scaled will also alter the denominator,
    or that is it will do the same as multiplying but without normalizing.
    - Whole divide type operations including remainders will have rules
    like additive operations, requiring denominators to be equal or fail.
    - Exponentiation will follow appropriate rools also.

* Err on the side of fewer/larger non-core packages rather than more.
- The following are generally still part of the Muldis D Plain Text language
core spec, but some might be better separated / done by the experts, eg Spatial.
- UPDATED 2016-08-15...
- Core - the core that everyone should use
    - all the boolean logic stuff
    - just numbers that are unlimited-sized and rational
    - just the simpler math ops especially ones that don't deal in irrationals mostly
- Core::Source_Code
    - declare all the main Package/Function/etc types for persistable/canonical
    source code living in depots/files, but that are not for normal user data
    - mainly behavioural but also some annotative or decorative
- Core::Runtime
    - declare an interpreter program that is a suitable main program
    implementing Muldis D at runtime, that which walks over source code and
    causes it to be performed as if it were processing data, when ordinary
    Muldis D routines are invoked etc
    - this would be bootstrapped by a simpler version for each host language
    - this version would still use the native host implementations of
    Foundation types and routines
    - this version would invoke Core::Plain_Text etc for its functionality
    - alternative Runtime-like modules could do cross-compilation or
    remote federation or whatever
- Core::Syntax::Plain_Text
    - parsers and generators for Plain_Text code that respect decorations
    - probably only has to map Text values and not Blob, so caller Runtime
    can alternatively try to parse Blob source as each encoding and then
    ask Plain_Text to try and extract a language name plus script declaration
    and see if they match up; the caller can just say scan the first few
    hundred bytes for this purpose with each attempt
    - other Syntax modules can map with both plain text and other structures
    defined in Muldis D, and possibly even some host data (via Externals),
    though the last might best be done by the host itself just because
- Core::Math - all numeric (and logic) types and ops not in Core
    - particularly everything not in Core that are common to GP languages
    - fixed precision or scale rationals/ints
    - exactly match hardware numbers
    - scientific notation
    - IEEE floats matched exactly
    - complex nums
    - nonexact math ops, especially listwise ones
    - algebraic symbolic nums
    - transcendental symbolic nums
    - symbolic nums defined by a generic function
    - trig functions
    - probability
    - significant figures
    - fuzzy logic
    - doesn't use Unicode
- Core::Text - all character types and ops not in Core
    - where knowlege of non-ASCII repertoires and encodings go
    - Unicode knowledge such as normal forms and encodings
    - Latin encodings
    - subset types for eg Unicode, Latin, etc
    - disjoint types that bundle in national/locale-specific orderings
    - might be further divided into Core::Text::Unicode for normalization
    or folding or non-UTF8 knowledge etc plus Core::Text::Latin etc
- Core::Stream - read or write generic filehandles
    - name subject to change
    - read or write STDIN, STDOUT, STDERR etc
    - read or write arbitrary filehandles
    - read or write arbitrary stream-like interfaces
    - this is all procedural code more or less
    - fundamentally all streams are in terms of integers or bytes or etc
    - border of Muldis D with the general system has no concept of characters,
    just bytes etc
    - this module itself doesn't know anything about character encodings,
    rather layers over top / other modules do that; aka doesn't use Unicode
- Core::Temporal - all standard temporal types and ops
    - continuum - a namespace
    - epoch
        - an epoch by itself is just a nominal or a namespace
        - place or for-what is intrinsicly tied to time also
    - instant - quantity of time relative to an epoch
    - duration - quantity of time
    - knowlege about calendars and timezones and what have you
    - these are types of Quantity
    - ops to read the system time
        - specific type of result is implementation-dependent, composes some interface
        - the key thing is that result is WHAT WE ACTUALLY KNOW from the
        system, which may for example be a UNIX_Time wrapped-integral value
        - result explicitly gives calendar, time zone, etc, where known, so
        much as is needed to qualify the actual timestamp payload;
        a timestamp without this info is treated as a 'generic floating
        timestamp from local clock' or some such
    - ops to 'sleep' etc
    - doesn't use Unicode
- Core::Spatial - typical complement of spatial/geographic types, ops, etc
    - what SQL/etc databases usually support
    - lines, polygons, etc
    - doesn't use Unicode

* These things should not be part of the Muldis D Plain Text language spec
but can be other libraries written in the language best by third parties.
- Something with a complement of units and ops used in physical sciences.
- Something with more elaborate business or banking related stuff.

* For a generic sequence generator, where they are useful, advancing is one
operator that returns the generator state, and getting the generated value
is a separate function on that state value.
- Idea, perhaps seq gen ops could also emit values via write-only
message-sending where that may be useful, which may be often.
- A generator is expressly not like an Enumerable because the the former can
go forwards/nextward only, and it expressly isn't expected to be able to go
backwards; moreover, a generator has a concept of a 'first' value, kind of;
actually to be more accurate.
- Make Generator an interface requiring composers to implement these 2 methods:
    - next_state(Generator) --> Generator
    - nth_next_state(Generator) --> Generator
    - curr_asset(Generator) --> Any
- Exactly one of nth_/next_state can be virtual and the other defined over
it, but for general efficiency purposes we would likely make nth_next_state
the virtual so composers can jump more efficiently as applicable.
- For a type composing Generator, its own generic selector routine is one
and the same as the generator initializer.
- Fundamentally the only requirement of a Generator value is to be a state
that tells next_state() everything it needs to know to produce both the
next state and, indirectly, all later states.
- A generic Generator composer, that is a single type capable of
representing a variety of different generated sequences in one type, would
in general have a Identity_Identifier to a next_state-alike function plus the
necessary state-alike to seed it with, that is it is a wrapper.
- But in the general case a Generator composer would be a distinct type for each
generated sequence, and the function would be implied by its composer type.
- We likely would make any Enumerable types also compose Generator just
because; that is, Enumerable itself will compose Generator, and next_state()
plus current_asset() would alias to succ() and return-argument respectively.
- A Generator could be considered a lazily implemented sequence potentially,
but that = is not expected to work on it like it would with an Array, where
equality means the exact same value list is produced and expects such normalization.
- A Generator composer could optionally take hints at initial selection
time on how it intends to be used, eg about how many values it is likely to
be asked for (eg 10 vs a million), so then if it needs to make different
decisions earlier on in order to support later iterations, eg if it is
calculating digits of pi and needs enough precision up front, it can.
- Need to look up Raku Iterator and Iterable again for guidance.
- Lookup "list comprehension" again also for guidance.
- IT APPEARS THAT LIST COMPREHENSION IS NOT THE SAME THING per se;
that is, set-builder notation seems to start with a list,
then perform a map combined with a filter to produce another list,
whereas the generator I defined so far is either infinite or the stopping
condition is defined in the function.
- That being said, list comprehension examples usually start with a range,
such as 1..10 or 1..inf, to define the source list; that is a list or an
iterator, so at least that part is definitely related to the generator, and
the generator can subsume the other stuff too.
- Lets just say the general case of list comprehension is a higher level
construct that can be built from a variety of interchangeable components or
combinations, rather than being a more fundamental feature itself;
this is similar to 'summarize' et al.
- REGARDLESS OF ALL OTHER FEATURES, WE WANT A LOW-LEVEL THAT JUST PRODUCES
A LIST OF 0..N CONSECUTIVE INTEGER ELEMENTS WHICH CAN BE EASY INPUT TO A
COMPREHENSION FUNCTION CHAIN ETC, also likely used for rank etc.

* NEW: In Plain_Text, special case the chars '-','.','/' only when they are
in front of or between digit chars, in which case they are part of the
literal and not a routine call; otherwise they are a routine call.

* Have special syntax-agnostic but for plain text decoration that specifies
at what character position/range each source code element was at relative
to the Text/Array parsed into the Package.  Debuggers can use this in
conjunction with just the original Text value to show a context snippit or
calculate at the time the relevant line number etc.

* This looks intresting: https://en.m.wikipedia.org/wiki/Tuple_space

* Interesting that SQLite is now a core feature in MS Windows 10;
shared by Richard Hipp with the SQLite mailing list on 2015 Nov 10:
http://engineering.microsoft.com/2015/10/29/sqlite-in-windows-10/

* Effectively we have a master system catalog variable after all, in a
manner of speaking, behind the scenes in the Foundation.  This variable is
a Structure with an attribute per registered package, possibly a hierarchy
of base:auth:vnum etc.  When a package is registered/"compiled" etc a field
is added to this variable; unregistering removes one.  One can read fields
to introspect code for either interpreting or compiling or implementing
various things like "isa" etc.  Unlike normal variables, this one is
directly visible to functions, because we don't want to explicitly pass it
as an extra function argument, or something.  Or its not so much a variable
as an implicit extra argument passed in addition to 'source'.  For that
matter, we also want an implicit extra argument visible to each material
that says what its fully qualified name is, eg, what \@0 resolves to.
Maybe also one with current call stack information.
- ACTUALLY, ANY OF THESE ARE NO LONGER KEYWORDS; EG "args" IS THE ARG LIST NOW.
- Perhaps we shall say that every routine (and type 'where') actually has 2 arguments:
    - source (a keyword) is what is explicitly passed in by calling code
    - context (a keyword) is implicitly passed extra information, a Structure:
        - self - a Identity_Identifier to / fully qualified name of the routine, eg \@0
        - catalog - the variable listing all the registered/invokable packages;
            one can lookup self in this to get info on where are we now
        - ?callers - information on the present call stack of the routine;
            or strictly speaking maybe the ability to read this in a function/type
            is bad because it means the function is not deterministic on its
            source; at the very least though, an alternative to this in
            functions is a write-only/debugging feature that says, write or
            track a stack trace of where we are now, whatever it is
    - generally we need to distinguish compile time info vs runtime, and only
    the former is available to functions but via 'source'; self/catalog is
    compile time, assuming each new package register conceptually recompiles
    everything, while call stack is runtime; likewise, "the database" is in
    the general case runtime / is not a constant in the registered packages,
    and we don't want functions to see it outside of 'source'
    - see also what Raku does again re context
- Generic debugging should be handled like with compiled languages, where
in order to step through a program or see trace information, one must
explicitly compile it that way, which in the Muldis D case that generally
means the MD source code is transformed to insert extra expression/etc nodes
which publish messages.  The ability to trace is NOT built-in to the runtime
because that would make everything slower when not needed.
    - Normally all we need is a new root expression node in the root of each
    function, since nearly every expr node calls a function anyway.
- Have generic keyword like 'publish' or 'trace' which takes 2 inputs, the
name of a channel and the value to write to it.  A third input says whether
or not to include context info like a stack trace, so it isn't done if it
doesn't need to be, this being something functions can't see as a value.
- Channels can be optionally listened to by various parties for what they want to see.

* See also http://doc.raku.org/routine/warn and things it references.
Raku has "note" which generally should be used instead of "warn" if you
just want to print to STDERR; warn is like almost ready to throw exception but didn't.
There are also "quietly" blocks alternative to "try" blocks.

* Idea, a package version number is just implemented as a string like an
unqualified identifier, where each integer is a code point like with the
names of positional field names.  Thus all parts of a fully qualified
package name or a Identity_Identifier are just field names / strings.

* Quoth I: "I think of 1st normal form as when you don't have multiple
relation attributes with the same meaning. That is, common examples of 1NF
violations are when you have say phone_1 and phone_2 fields or email_1 and
email_2 fields in a people relvar. This is what "multi-valued" or "single
values of a domain" etc refer to in practice as far as the prescription on
eliminating such to achieve 1NF. The problem was never about array-valued
attributes or complex types or what have you."

* Don't use Identity_Identifiers as args.\0 etc in types the Foundation needs to know
about, such as anything Plain_Text has syntax for or anything likely to
have an internal special representation for or any type defining source
code.  Save Identity_Identifiers for things only a bootstrapped system would see.
- Identity_Identifiers as they are versioned by package version can cause problems at
the Foundation level just because.

* We really should have the Voidlike interface or some such rather than
just a Void selection type.  Then make the various built-ins that otherwise
possibly return Void instead return a specific composing type that is more
specific to the reason for being Void, eg No_Such_Field etc.  It is
analogous to Exception types.  Tests should then be for isa Voidlike rather
than = Void etc.  That being said, we want these to be treated like Unit,
essentially just being a name without extra info, unlike Exception which
probably has extra info.  Or something.  See also ?! coalesce op, which can
also be named 'coalesce'.  While loosely resembling exceptions, these are
not in that sense as one throws exceptions while one returns these values
and moreover these values are part of the declared function result type
while exceptions are not so declared.  It could be argued that the main
purpose of Voidlike, or Void, is to be an intermediary in implementing a
ternary operator like "tup .! atnm ?! val_if_missing" eg it lets one
overload that syntax while still having custom reasons for can't-do-it.
- Anyway, voidlikes should be direct restrictions on Structure without
being members of other types, so all regular operators fail on them.
- How about declaring say Excuse is a selection type of Structure where
.>0 is \Excuse and .>1 is any \foo; there would also be a special
Plain_Text syntax, eg \!foo eg \!No_Such_Field etc.  Then there's no
interfaces here, its just a selection type loosely like Text.  Or better
yet .>1 is a Identity_Identifier, or Excuse is itself isomorphic to Identity_Identifier.  One
can/does still declare subtypes of Excuse or
constants/singletons so to use specific ones in signatures etc.
- Actually, Excuse should just be an interface; as ?! etc can still be a
non-abstract function, it doesn't need an implementation per value.
- Note, Excuse is declared as a regular type, not as a source code type.
- Use it as one would have used null/void/etc.
- A ?! mnemonic is "why not?".  Perhaps "orelse" instead of "coalesce"?

* SAS is another programming language to get operator name ideas from.
One newly discovered thing, ^ is logical not, ^= is not-equals.

* Consider that cross-compiling Muldis D to the host language might not
actually be that valuable in practice for a default reference
implementation, considering how much would still have to be done in
wrappered ways anyhow, such as any operator calls or implementing lazy
evaluation, and that a straight interpreter based design may actually be
the best thing to default to.  Especially with host languages that don't
support runtime invocation of the compiler.  Also that would be more like
how SQL DBMSs work.  There can still be alternate implementations that
cross-compile though, such as for better performance.

* Quoth a perl6-language post:
"Keep in mind that what Raku calls a "type object" isn't quite the
same as class objects in other languages -- a Raku typename is
really an undefined instance of a class.  In other words, the
identifiers C<Int>, C<Rat>, C<Array> etc. refer to instances of
those classes just like the literals C<3>, C<4/5>, and C<[1,2,3]> are
instances of those classes.  They share the same method spaces."

* THIRD TRY:
- Amend the SECOND TRY by eliminating the 4th position 'maturity' and
instead say that versions with an alpha suffix eg -dev, -alpha, -beta, -rc
mean pre-production while no suffix means production.  This indicator is
still just boolean (presence or absense), the actual letters are for humans.
- The patch moves to the 4th position from the 5th, and does NOT reset to
zero when transitioning from pre-production to production.
- Commit numbers/ids from the version control system can be a further
suffix; they should not be used as the patch number, too much information.

* SECOND TRY:
- This is a proposed scheme for software projects, particularly those that
feature a programmatic API (programming languages, libraries, DBMSs etc)
would use.  It is a semantic versioning scheme such that the numbers have
meaning.  There is a sequence of non-negative integers arranged greatest to
least; semantics are such that an infinite trailing list of zeros exist;
normally, incrementing a position will truncate/zero all subsequent ones.
- Parts are: [automata.branch.feature.maturity.patch]
- Incrementing a part has nothing to say about the size of the change, just
on how the change should be treated.
- Incrementing automata primarily means a distinct product from the user's
perspective that happens to share the same marketing name as and be similar
to a number of other such distinct products.  Versions with different
automata have no expectation of cross-compatibility; their APIs might have
no useful common subset; they might not be able to read or write each
others' data files, either user data or config data.  Users wanting to use
both versions might install them in parallel and invoke them separately
such as for different data files.  Incrementing automata should also, if
applicable to the project, have a completely separate namespace both in API
and in used filesystem locations, so that the two versions may coexist and
be used in the same project as if they otherwise had no relation.  As an
exception to this, an automata of 0 is meant to indicate a new product
under experimental development and versions sharing an automata of 0 may be
arbitrarily mutually incompatible in ways that otherwise should only happen
between different automata; incrementing automata to 1 should indicate that
this period is over.  Analogies in SQLite are 2.0 vs 3.0 (both data files
and API), in Postgres are 9.3 vs 9.4 (with respect to data files), in Perl
are 5.0 vs 6.0 (ignoring emulation), etc.
- Incrementing branch primarily means a distinct island of stability from
the user's perspective, such that as long as they only upgrade versions
within a common branch, it is unlikely they would suffer any breaks except
perhaps in edge cases, or any breaks they are likely to be able to adapt to
without much difficulty in such a way that their updated code would still
work with all versions within that common branch.  From a production
standpoint, each branch is likely to be a distinct maintenance branch in
the version control system (though the newest 'branch' may be its trunk),
where each of those would possibly but not necessarily have its own series
of further releases.  A branch is free to be completely
backwards-compatible with previous branches.  Analogies in SQLite are 3.7
vs 3.8, in Postgres are 9.3 vs 9.4 (with respect to API), Perl are 5.16 vs
5.18, in Mac OS are 10.8 vs 10.9, etc.
- Incrementing 'feature' primarily means a new batch of one or more
features or changes of significant size or complexity have been made within
a common branch such that a pre-production release (experimental or alpha
or beta or release-candidate etc) is likely desired to occur for those
changes prior to a production release.  In this context, a shared 'feature'
groups together the versions that are essentially the same but for
development status.  Or, 'feature' should be incremented whenever a change
is made that is known to break backwards-compatibility.  'Feature' is the
finest granularity for which dependencies on particular versions should be
declared, except for where one wants to declare avoidance of a bug.
Analogies in SQLite are 3.7.0 vs 3.7.1, in Perl are 5.8.0 vs 5.8.1, in Mac
OS are 10.9.0 vs 10.9.1, etc.
- Incrementing maturity primarily means that a 'feature' has transitioned
from being some pre-production status to production status.  Unlike the
other parts, maturity is always just 0 or 1, with 0 meaning pre-production
and 1 meaning production.  Note that a 'feature' is not required to have
both pre-production and production maturity versions under it.  Analogies
in Perl are 5.15.0 (pre-prod) versus 5.16.0 (prod) or 5.16.0-RC1 vs 5.16.0.
- Incrementing patch within a common 'feature' primarily means different
things depending on whether the maturity is pre-production or production.
Between 2 production releases of the same 'feature', an incremented patch
means there should only be non-API-affecting internals changes for fixing
bugs or for security or performance, or minor API changes that are a
consequence of bug or security fixes.  Note that if it is known or
anticipated that any users may be depending on buggy behaviour, the fix for
the bug should be made with an incremented 'feature' instead; likewise with
a security fix that disallows previously allowed user actions.  Between 2
pre-production releases of the same 'feature' any arbitrary level of
changes are allowed of the same kind that one would otherwise increment
'feature' for.  Analogies in SQLite are 3.8.11.0 vs 3.8.11.1, in Postgres
are 3.4.0 vs 3.4.1, in Perl are 5.16.0 vs 5.16.1, in Mac OS are some
security patches, etc.
- To be clear, all versions considered for production must have a 1 in the
4th position.  For example, the first feature-complete reasonably-stable
production release would be version 1.0.0.1 (or 1.0.0.1.0) whereas version
1.0 (1.0.0.0.0) is suitable as a release-candidate version number.

* FIRST TRY:
- This is a proposed scheme for software projects, particularly those that
feature a programmatic API (programming languages, libraries, DBMSs etc)
would use.  It is a semantic versioning scheme such that the numbers have
meaning.  There is a sequence of non-negative integers arranged greatest to
least; semantics are such that an infinite trailing list of zeros exist;
normally, incrementing a position will truncate/zero all subsequent ones.
- Parts are: [parallel-major.parallel-minor.sequential-incompatible.sequential-compatible].
- Incrementing a part has nothing to say about the size of the change, just
on how the change should be treated.
- Incrementing 'parallel' means a distinct product from the user's
perspective that happens to share the same marketing name as and be similar
to a number of other such distinct products.  Sometimes each parallel-minor
would install into a distinct directory from other parallel-minor and be
used at the same time as each other.  Each parallel-minor typically means a
distinct maintenance branch that can have further versions sharing the
parallel-major-minor for an arbitrarily long time.  Analogies in Postgres
are 9.3 vs 9.4, in Perl are 5.16 vs 5.18, in Mac OS are 10.8 vs 10.9 etc.
- Inspired by Perl, parallel-minor even numbers specifically mean
production release while parallel-minor odd numbers mean pre-production
(snapshot/experimental/alpha/beta/etc) release; each odd N is the
pre-production series for the even N+1 series.  Typically in version
control the pre-production releases (except when stabilizing for a release)
would come from trunk, and then production releases would come from a
branch named for the production version number, eg 5.18 comes from a 5.18
branch while 5.17 come either from trunk or from an earlier version of the
5.18 branch.  Notably, all releases have distinct numbers, there is never a
text portion of the version such as the word "alpha" or whatever.
- A parallel-minor is free to be completely backwards-compatible with prior
parallel-minor, or not; the fundamental purpose of these is to distinguish
releases of versions not considered ready for production.
- The parallel-major is only incremented arbitrarily if one wants to put
extra marketing emphasis on what would otherwise be a parallel-minor; for
example, Postgres 8.4->9.0 instead of 8.4->8.5.  For practical purposes,
there is no need to ever increment parallel-major.
- As a major exception, a parallel-major of 0 is meant to indicate a new
product under development that is not considered feature-complete, while a
parallel-major version of 1 or greater should be considered
feature-complete.  Parallel-major zero versions can still reasonably be
considered production versions, as in safe for production, but its just
declaring that major features are known to be missing.  Many projects may
just have versions 0.x and 1.x and never a 2+.x.
- So 0.1 is the first pre-production release version, 0.2 is the first
production version declared to be feature-incomplete, 1.0 is the first
production version declared to be feature-complete, 1.1 is the first
pre-production release following 1.0, and so on; version 0.0 is the
pre-development version, when creating a new project without any code yet.
- Incrementing 'sequential' means a point release within a single product
from the user's perspective, such that upgrading typically means replacing
the older version with the newer one; while replacement isn't mandatory,
there usually isn't a reason to keep both around at once aside from testing
during the transition.  All 'sequential' within the same parallel-minor
typically come from the same maintenance branch.
- Loosely speaking, all 'sequential' under the same 'parallel' are expected
to be compatible with each other, or backwards-compatible in particular, so
anything working in an older version also works unchanged in a newer one;
however in practice there is no guarantee, and all a common 'parallel'
means is they are from the same maintenance branch in practice.
- The sequential-incompatible is incremented when making changes with a
'parallel' that are not necessarily backwards-compatible and should be
treated with greater caution by users.  In particular, this at least must
be incremented whenever there is a known break in backwards-compatibility,
which normally would also be documented for users to know about, but
incrementing this may also happen just when substantial internal changes
happen for which the author is not confident it doesn't break something.
This is also incremented even if the change is purely for security
purposes, but would be disallowing a previously allowed action.
Analogies in Mac OS include 10.9.1 and 10.9.2 etc.
- The sequential-compatible is incremented when making changes that are
known to be completely backwards-compatible with the prior 'parallel';
often these are bug fixes, but they can also be feature additions that
don't change the behaviour of existing uses.
- To clarify, whether a bug fix means an -incompatible or -compatible
increment depends on whether it is believed that any users may have been
relying on the incorrect behaviour (thus it was an unintended feature); if
they were, increment -incompatible; if they were not, increment -compatible.
- Generally speaking, it should be safe for users to upgrade to the latest
sequential-compatible at any time for their current sequential-incompatible,
and it should also be possible for them to upgrade to the latest
sequential-incompatible within a 'parallel' with only relatively small if
any adjustments to their usage.
- It is generally valid to use non-consecutive numbers when incrementing if
one wants to, but later versions should be higher numbers than earlier ones.
- Likewise, as an option, one can choose to say just use a build number or
commit number etc as their sequential-compatible value; they don't have to
reset it to zero when incrementing the other numbers, the main semantic
inference logic should still work regardless.
- There is no obligation to have an actual maintenance branch for each
even-numbered parallel-minor; one could decide to have short turnaround
between say version 1.2 and 1.4 and not actually provide further 1.2.x
versions at all, but at the same time the opening is there to retroactively
do more 1.2.x later in a branch added at that time.
- As to release candidates, that depends.  When a 'parallel' is about to
hit production status but hasn't yet, a release candidate can be in the
same manner as alphas/betas etc by being one of the last x.N.y.z before
incrementing to x.N+1.0.0.  When a non-first/0 sequential-incompatible
x.y.N.z wants to do a release candidate x.y.N+1.0, they can simply release
it as x.y.N+1.0 with a warning, and be ready to quickly do another release
if a problem is found with it; in practice this probably isn't any
different than the common skepticism users have of .0 releases anyway, that
they are used to waiting for others to take the chance first etc.
- Generally one should be able to treat every
sequential-incompatible/-compatible release as a release candidate, and if
they aren't that confident, they should be doing a parallel-minor cycle
instead.  Except possibly during the 0.x.y.z period, all sizeable
development should be done in parallel-minor cycles anyway, with all
'sequential' generally staying with minor features or bug-fixes.

* This seems interesting:
http://www.vertabelo.com/blog/notes-from-the-lab/why-sql-is-neither-legacy-nor-low-level-but-simply-awesome

* Useful ta know: https://github.com/daoswald/JSON-Tiny/pull/1

* Bootstrapping aside, it probably actually IS easiest to write all Muldis
D compilers IN Muldis D, for example the code that takes user-defined
Muldis D code (native data structures format) and generates equivalent Perl
or whatever.  The extra verbosity of the implementation, eg the syntax
for Perl ::Value objects, can be written once as factors and then be
duplicated like macros into the actual Perl code where its used.  Likewise
for the ::Value objects / Foundation types and ops themselves.  Ultimately
all processes are just generating/mapping something to something anyway.

* A Muldis D feature to emphasize is how one can do a single query to get
all the related data they want at once rather than many serial queries.
ORMs/etc implemented over this can exploit this, group requests together,
then MD impl gathers the parts efficiently as possible.

* Structurally we can just have these materials:
    - alias
    - constant
    - selection
    - interface
    - function
    - procedure
- Any of {selection,interface,constant} can be used as a type; while
conceivably a function could too, we will require a function to be wrapped
in a selection in order to be used that way.
- Anything that can be used as a type can also be invoked as a function or
as a constant, which will be a predicate function or default value respectively.

* New function trait keywords (ignores implements/is/etc for now):
    returns --> to    - ((True))
    matches <-- from  - ((True))
    accepts requires  - ((True))
    via :             - (args)
- All traits are optional, effective default if missing is given with them.
- In ... cases, {returns,matches,accepts,of} will also take syntactic
shorthands for lists of type names or Structure defs while {via,where} don't.

* A thought:  Fundamentally a function is simply a mapping of values of the
universal type.  All that to/from/requires does is provide metadata on when
the function is expected to be well-behaved or not with certain input, and
strictly speaking they are all optional.  From and Requires are
fundamentally alike, specifying preconditions / constraints on input, but
the one to use is determined by what semantics are desired from the system.
To is a constraint on output.  Normalize is just a factoring convenience.
- From an optimization standpoint, a program known to be correct can have
the 'requires' and 'to' checks disabled, that is assuming we know for sure
they would never be tripped.  As to the 'from', that is only needed in the
context of overloading to select a candidate.
- Actually, lets say Normalize is obsolete, not really that useful.
- As such, a function is effectively a wrapper for this logic:
    if not from-expr then reject_match
    else with normal-expr:
        if not requires-expr then accept_match_fail_input
        else with body-expr:
            if not to-expr then accept_match_fail_output
            else accept_match_success_return body-expr
- But if unifying types into functions, we actually need 2 predicates and
the logic is instead like this:
    if first-pred then second-pred else false
... that is, there is no from/to/requires/normalize logic, no multiple
candidates per se, no exception throws like to/requires etc.

* Consider formally unifying functions and contracts at the native
source code level, such that contracts are just syntactical shorthand.
- Need appropriate representation for an interface contract then.
Or that remains a separate thing, perhaps just call it 'interface' rather
than 'type/contract'?  Or not.  It is still *used* as a function?
- Need appropriate requires-implements then.
- Any Boolean-resulting function ... or any function really ... can have an
associated 'default' trait, which is just a reference to a constant; the
'default' trait is similar to associative/etc traits in how actually used,
or alternately default-foo is always just explicitly invoked when needed eg
to default a variable.
- The in/out/requires/etc traits of a function can just be other functions.
- Consider making the higher level routine call syntax --> just syntactic
shorthand for a Foundation 'call_function' routine combined with a runtime
access to the source code to know the fully qualified name of the routine
to call (analogous to Perl's caller() etc) and/or pass in the source code
itself which might then be a true higher-order function.

* Lets say that one can reference any contract name where a function name
is expected, and in that case it is equivalent to invoking the complete
predicate of the associated type; so eg "(42,) --> \@Integer" means "42 isa
\@Integer" and returns true.
- Similarly, generalizing the constants thing, referencing any contract
where a constant name is expected, is equivalent to fetching the associated
default value, so eg "() --> \@Integer" returns 0 as does Integer().
- Maybe conversely anywhere a type name is expected we may put the name of
a predicate function but that the predicate function must take the payload
value as its real argument, not wrapped in a unary Structure.
- Thus the difference between functions and contracts is essentially none,
or contracts really are just a subset of functions, or such.
- But what about the allowance or not of multiple functions matching the
same floating name, pick one or die?
- Either way we should think about formal function<->contract equivalencies;
eg, a function behaving as a type must have Any as its argument and have
no 'requires' and its body is the predicate.

* See also brand new written http://design.raku.org/S07.html ok.

* Try and keep from reusing operator names for conceptually unrelated uses,
for example, use 'minus' for math only and just 'except' for set diff.

* Make the External type more of a host-bound concept than before, such
that the Plain_Text grammar has no literal/selector syntax specific to it
at all, except for a dummy `external` singleton (like with false and true),
and so trying to map any value is External or has an External component to
a Plain_Text source code string will be a lossy operation.
- This lossyness is similar to how any source code syntax meta not specific
to Plain_Text will be lost when mapping to Plain_Text, and sometimes even
some specific to Plain_Text if it can't be reconciled with the regular code
as the actual semantic regular code takes precedence in a conflict.
- Mapping from Plain_Text will turn that token into whatever is the current
host language's closest concept to a generic undefined value, or whatever
is the External type's "default" value in that host.
- ACTUALLY, Plain_Text doesn't even get a keyword for External; instead we
map to a Foundation routine call:
    (() -->^ FDN__default_External)
- Generally the only place where an External value is lossless is either as
a normal value at runtime (under the host it is native to) or in Muldis D
Hosted Data source code (specific to the host it is native to).
- When one wishes to persist External values in Plain_Text code, they must
map to a non-External value, which at the very least could be a
serialization to a simple Muldis D Blob that only the host knows the
mechanism of.
- As with false/true, there would be a Foundation level constant routine
that gives the "default" value of the External type for its host.

* An expression tree can be represented in data as a list of node name /
node definition pairs, possibly as a tuple.  The list of package entities
could be similar, but that their names have a hierarchy.

* Annotation expressions, that is ::?= stuff, are explicitly meant to be
syntax-agnostic meta-data.
- Example uses include:
    - Important code comments.
    - Guides for generating a user interface / forms related to the code/types.
    - Optimization hints such as how to index collections or expected cardinalities.
    - Hints specific to implementations, such as that something is specifically
    meant to correspond to a table in a SQL implementation.
    - Maybe consolidate certain routine trait info which can help
    with optimization.  That being said, a lot of routine traits CAN'T be
    demoted to annotations because they determine whether certain
    operations are even legal semantically; for example, only commutative
    and associative functions can be used for a deterministic reduction of
    an unordered collection type.
- Plain Text may provide special syntax for some of the uses of annotations
eg routine traits but they would parse to the same?  Other grammars may too.
- Annos are NOT intended for syntax-specific metadata such as is used to
exactly remember the format of code such as literal format or whitespace etc;
rather remembering that is orthogonal and can apply to annotations too as
annotations are considered important code-alikes that appear literally.
- Annotations must be completely foldable compile-time constants so they
can not reference 'source' and should not have any semantic effects such as
side-channel writes unless those are desired to run at "compile time".
- Note, the used term "annotation" has a number of precedents including the
C# DataAnnotations stuff or Java "compile time annotation" stuff or C++
stuff, or other things; they are common in being introspectable at runtime.

* Make collection types support ?/to-boolean directly, rather than requring
?#, but it means the same thing, and the interface type with # composes
boolable; one logically shouldn't have to count a collection to know if it
is nonempty.

* While hosting all the boolean operators in boolish is an interesting idea
and more consistent with say numeric or etc types, in practice that makes
things unnecessarily complicated since no one would really ever want to
actually do all the boolean ops in other types, especially considering
others don't really add any value in that sense, and the built in
conditional ops still require regular Boolean, and not overriding means we
can mix differant boolish and which type they get as a result is
unpredictable, so boolable is all we need.

* A key difference between packages and other entities is that packages
explicitly declare their name inside themselves, they are never anonymous,
while definitions of everything else (materials, expressions, etc) are
always anonymous themselves and only have names that are externally
associated with them.
- A package declaration is the keyword 'package' followed by an unordered
whitespace-separated non-delimited list of keyword-asset pairs same as
'function' or 'type' etc declarations, and like those, individual list item
assets are delimited if necessary.
    - identity
    - uses
    - materials
- Alternately, the 'package' has its whole list curly brace delimited and
its contents separated by semicolons.
- The 'uses' and 'materials' are always curly brace delimited and their contents
separated by semicolons and structurally are loosely like routine bodys etc.
- The 'floating' lives inside 'materials' since its not a public API detail.

* Canonical way for a package to separate public and private materials is
to use the "_" top level namespace for the latter.  So users of package Foo
would just "floating Foo" while inside Foo it would "Floating {Foo,Foo::_}".

* Add Foundation.pod file which has these 4 main sections:
1. List foundation types.  Their 'names' are I<foo>.  They would have
corresponding but different C<foo> names defined in Muldis_D package.
2. List subtypes of the above used to define source code as data, because
the compiler at package registration actually needs to know.  Their 'names'
are I<foo> and are defined in terms of predicates.  They would likely have
corresponding but different C<foo> names defined in Muldis_D package.
- Yes, #1 and #2 I<foo> above are types, just their C<foo> are contracts.
- #2 just eg don't use Tuple, but Structure directly, save complexity.
3. List foundation functions.  Their names are C<foo>.
4. List foundation procedures.  Their names are C<foo>.

* The core grammar can stick to selector node types for just the
fundamental type collections - array,bag,structure - and distinguishing
whether selector syntax specific to set or relation or tuple etc was used,
that is just in the Plain_Text metadata.
- We will however not fold any collection type selections into an 'opaque'
node when there is a lack of arguments etc referenced, so the code still
has a semblence of what the user wrote, and their explicit node names or
comments etc are preserved most appropriately.

* In order to ACTUALLY implement a Plain_Text parser in pure Muldis D, or
other parsers likewise, every literal / selector ACTUALLY has to have a
corresponding foundation function, because the real parser logic is, when
you see this series of integers representing these character code points,
you need to map them to eg a logical boolean or integer or whatever.
- So we need foundation functions to select boolean true false, and
foundation functions of the str2num variety, assuming its better to push
down that common logic than to, say, subtract 32 from the code point integer
then multiply to the radix times the position from the end and then add,
etc, though users should have the latter option available too, and even
then, we have to select the 32 somehow, and repeated succ() from zero is
kind of too far on the micro-boost side of things.
- The parser calls the str2num or whatever and produces an 'opaque' node
which is the source code as data result.
- Actually, no str2num, we just math it with 0/1/-1 primitives along with
the Foundation math ops like add multiply etc, do that at parsing time and
then constant folding would turn things back into effective literals of any number.
- Generally don't have N-ary expression node types maybe but instead an
expr node primitive for making a 1-tuple and a 2-tuple and between those
and functions for for extension etc we can basically produce any literal
values of collection types in the parsers, that can then be constant-folded
to effectively make collection literals of any size collection.
- But implementation would need to optimize certain N-ary things especially
if we coded them with tail-recursion, like string construction.
- The Hosted_Data syntaxes would need something to call in the Foundation
for more direct N-ary mapping for fast data in/out.

* Have Muldis_D::Foundation package be written in the PT foundational
subset, and this primarily defines the full Plain_Text parser.  MD::B has
no other package dependencies, and no other packages declares synonyms to
things in MD::B.  Muldis_D package proper is written in regular Plain_Text
so it looks nicer / is more maintainable.  MD::B may declare contracts for
the same types as other packages independently if it needs them.

* Add expression node kind to represent an opaque value, that is, the value
of the node is its payload itself, this is distinct from literal/selector nodes.
- Use often for values of types with their own Plain_Text 'opaque' literal
syntax where PT Foundation or the catalog itself has no type-specific literal
nodes for them, eg Text,Blob,Fraction,etc rather than using an Array sel node +
Integer lit nodes; the sc meta still remembers Text etc lit were used.
- Also used a lot with 'hosted data' variants when eg passing host-native string values.
- For that matter, any 'opaque' literal types even for Foundation types can
just use this too, {Boolean,Integer,External}, no need for their own node types.
- Save dedicated node types for when there may be child nodes.
- The effect of putting the 'folded' trait on a constant is to give the
implementation carte blanche permission to eagerly evaluate its expression
and then replace that expression with an expression consisting of just an
'opaque' expression node housing its value; while in the general case that
can't be done until package registration time, and all dependencies it uses
are also registered, the constant can still be selectively folded earlier
where there aren't external dependencies or at the very least any sc meta
can be discarded at parse time; in any event, the actual source code as
data value of a folded constant's expression at any given point in time
is implementation-defined.
- ACTUALLY, the folded trait is probably something that needs to be known
about at a fundamental level in the implementation type system, so that the
right thing happens re normalization when the value is actually 'used';
or maybe no more so than ordinary lazy execution optimization, so then
really 'folded' mainly means don't keep sc meta?

* Make String a special case of an Array rather than a fundamental type.
- Similarly the String-specific special syntax is now gone.

* TODO: Demote any special Source_Code syntax for core types that are not foundation types,
eg Blob/Text/Rat/Tuple/Relation etc so they are treated like Source_Code_Meta;
eg, a Relation literal in Plain_Text just turns into Source_Code for a tree
of Structure/Bag/etc and has Source_Code_Meta to specify that the Plain_Text \?%{} etc syntax was used.
- And so, embedded type names even for those in Muldis_D package can be fully-version-qualified.
- The Source_Code_Meta as applicable embeds version info from the language name declaration / Plain_Text itself.
- So at the implementation level, Plain_Text is treated the same as other grammars.
- Just things that foundation routines need to know about, or eg a format of routine args, needs non-versioned values.
- Add Plain_Text_Lite grammar which has no literals but for foundation types,
and possibly doesn't use any Source_Code_Meta either ... Muldis_D could be written in it perhaps
and Plain_Text is a proper superset of it ... it still needs Comments.
- Also don't support non-ASCII scripts.
- Also drop 'fixed' function invo formats and foo(); arg-->func only option.
- Also drop meta ops ! and :=.
- Even Plain_Text_Lite still keeps all the meta like whitespace etc in sc metas,
so core libraries still just as round-trippable.

* Consider support for declaring that certain contracts are exclusive, for
example, a type composing Foo must not compose Bar.  Based on the idea that
an abstract type may be partly defined by what it doesn't do.

* I maybe should emphasize the use of interface types as much as possible
to declare all routines as possible, minimizing those declared over
selection types specifically.
- Maybe this could be an analogy to things declared in .h vs .c ?
- This includes having actual accessor functions per type and not relying on
accessors provided by Structure etc.
- Any such accessors specific to attributes can then be called with the
standard val->attr notation where 'attr' is actually a monadic function.
- Consider turning all the standard built-ins into interface types, eg
Integer etc, then have the implementing enumerated types be things like BigInt or Int32 etc;
we then don't need Integral; Integer takes its place.
- If Point is an interface, Point_Cartesian and Point_Polar could be composers.
- If Relation is made into an interface, the standard composer could be eg
Relation_Name_Value while an alternate could be Relation_Name_Type_Value say,
where the latter incorporates the declared-type-as-part-of-attribute-identity
semantics.  For the latter case etc we may want the concept of a type factory?
The former case is semantically like a special case of the latter where the
declared type is Universal for all attributes.
- There can be an interface type for the general case of
declared-type-as-part-of-value-identity.
- If Fraction is an interface implementers could be normalized or not.
- But then what do the standard literal syntax / selectors yield?
- Lots of DBMS:: or lang:: types/routines could compose the Integer etc interfaces maybe.
- Define interfaces in terms of, what would ALL things of this type have.
- Split into more interfaces with fewer routines if necesscellery.

* Get rid of package segmenting feature?  Live with big or learn to split.
Or make the segmenting logically external to packages, eg make it part of
the repository logic, and a logical package has no such thing.

* Packages are now inherently anonymous same as with materials etc.
- Note: see also https://github.com/perl6/specs/blob/master/S22-package-format.pod
- Packages now instead declare that they 'provide' a pseudo-package interface;
this is declaring that they 'are' that pseudo-package; it is fully-qualified with auth, version;
it also indicates a package-private alias that it refers to itself with.
- Packages may declare that they 'emulate' zero or more pseudo-package interfaces.
- Packages may declare that they 'use' zero or more pseudo-package interfaces;
for each of the latter, they indicate a package-private alias that the
dependency is referred to with.
- 'requisite' is another word for dependency, what one uses

* New files' layout:
    lib/Muldis/D/Overview.pod
        - type system
            - values and types and variables,
            - abstract/interface and concrete/by-constraint types.
            - how to define a type, the parts of the def and meaning
            - requires_implements is what makes a contract
        - namespaces
            - packages, materials, etc
            - what using does
            - what floating does
    lib/Muldis/D/Low_Level.pod
        - define the 8 built-in types descriptively
        - define all the foundation routines
    lib/Muldis/D/Packages/Muldis_D.pod
        - index to Muldis_D/*
        - OR JUST PUT THE WHOLE PACKAGE IN THIS ONE FILE, MAKE IT BIG
        - THE MULDIS D PACKAGE DOESN'T NEED TO BE SO LARGE NOW REALLY,
        TRULY LOW-LEVEL IS THE CORE CORE, Muldis_D CAN BE SPLIT UP
        - IN PARTICULAR, Source_Code CAN BE ANOTHER PACKAGE SURELY...
        EXCEPT THEN THERE'S A CIRCULAR DEPENDENCY, SO LETS NOT DO THAT
        - SO LETS MAKE IT JUST:
            - ALL THE SHIMS FOR LOW LEVEL
            - DECLARE ALL TYPES THAT Plain_Text HAS SPECIAL SYNTAX FOR
            AND ALL VIRTUAL TYPES THEY COMPOSE
            - AND ALL TYPES USED IN THE DECLARATION OF THOSE TYPES
            - AND THE TYPES USED TO DEFINE SOURCE CODE
            - AND THE MINIMAL OPERATORS FOR THOSE, MAINLY THOSE WITH VIRTUALS
        - "ADDITIONAL" OPERATORS FOR CORE TYPES CAN GO IN MOAR PACKAGES
    lib/Muldis/D/Packages/Muldis_D/General.pod
        - define all the types and routines that are declared as immediate
        children of "Muldis_D"
        - visible unqualified when one says "floating Muldis_D;"
        - these are all the general purpose ones that users use most often,
        and for routines, especially if they are interface-defining
        / virtual/abstract types/routines
    lib/Muldis/D/Packages/Muldis_D/Source_Code.pod
        - define all the types and routines that are declared as immediate
        children of or descendents of "Muldis_D::Source_Code"

* Note that http://dlang.org/contracts.html generally considers "contracts"
to be predicate expressions used in various places such as on the input or
result values of a function.

* Create Muldis/D/Types.pod right away and populate it with descriptions of
all the data types defined by the Muldis_D package which are expected to be
exposed to the user either for their data or for defining code.  It will
contain some details currently on Plain_Text.pod, eg this is an integral
numeric or whatever.  The user data or general use types including Relation
will appear first in sections, and the ones intended just for code
including Pair appear after; Structure is kind of a middle ground but will
be with the first group.  Plain_Text.pod can or should then be interpreted
in the context of Types.pod, as the former can assume people know what the
types are and how they are composed in terms of each other, while the former
just adds syntax-specific details.
- To start off with just declare them without the namespace, unqualified;
anything that should be in the file should have a single-level name, more or less,
Plain_Text is an example of such naming.  Or qualify it from the start just in the heading.
- Include an indented is-a hierarchy tree as well as composes trees.

* "The following Unicode character values are considered whitespace in D4:
0x0009 (tab), 0x000a (line feed), 0x000b (vertical tab), 0x000c (form feed),
0x000d (carriage return), 0x0085, 0x2028, and 0x2029." - its manual

* Have some way to support partial indexes on Bag (and in turn on
everything else) and not just full indexes; a partial index is defined in
terms of a predicate function associated with the map function that defines
the index proper, the predicate says whether the key is found by the index
or not; see also how Postgres implements partial indexes.
- Also, as of version 3.8.12, SQLite supports partial indexes too
see https://www.sqlite.org/draft/partialindex.html .

* Note, see also https://github.com/Ancestry/DotQL which Nathan Allan made.

* See http://michael.otacoo.com/postgresql-2/postgres-9-6-feature-highlight-pushdown-improvements-postgres-fdw/
- and http://rhaas.blogspot.ca/2015/11/parallel-sequential-scan-is-committed.html
- and http://tbeitr.blogspot.ca/2015/11/for-better-service-please-take-number.html

* Note, see http://johtopg.blogspot.ca/2015/11/listening-connections-arent-cheap.html
and also https://github.com/johto/allas .

* See http://thebuild.com/presentations/pgconfsv-2015-pci.pdf re
PCI-DSS compliance and Postgres.

* Re Oracle databases, see for example
http://stackoverflow.com/questions/1209039/oracle-gotchas-for-an-experienced-newb
- This returns nothing:
    SELECT  *
    FROM    dual
    WHERE   '' = ''
- "Don't forget to use nvl(column) around any column in a rowset that might
be filled entirely with null values. Otherwise, the column will be missing
from the rowset. That's right, missing entirely!"
- http://www.codeproject.com/Articles/704385/Gotcha-Oracle-Null
- SELECT 'empty string is null' FROM dual WHERE '' IS NULL;
- Catenating foo with null results in foo, not null.
- "NULL;" as a statement is a no-op, needed in otherwise-empty blocks.

* Add MUMPS to the list of (decades old) languages worth looking at.
Focused on the medical field, this is intended/designed for building
database applications, has numerous relevant features built-in.
See https://en.wikipedia.org/wiki/MUMPS for moar.
http://motherboard.vice.com/read/meet-mumps-the-archaic-health-care-programming-language-that-predicted-big-data-2
MUMPS stores data in arrays, and some arrays can be persistant on server
and shared, those arrays use different variable name syntax.

* Postgres has these:
- insert into mytable values (default) returning my_id;
- INSERT INTO foo DEFAULT VALUES;

* Where reasonably possible, make system-defined dyadic operators come in
pairs when they are not commutative so users have more choice on how they
order their arguments; for examples:
    item in container
    container has item
    container subset container
    container superset container
    rel rename atpairs
    atpairs renaming rel
    rel on attrs
    attrs from rel
    rel where func
    func filtering rel
    y divideby x
    x dividing y
- Sometimes symbolic forms naturally come in pairs, other times symbols
only exist in one direction.

* Replacement for literal syntax (arrows can go either direction or mixed):
    \@:(->foo,->bar,->baz)
    renaming
    ?%{(1,2,3),(5,6,8),(2,5,4)}
- That expression effective shorthand for:
    ?%{(foo:1,bar:2,baz:3),...}

* Casts (SOME OBSOLETE):
    % to Tuple from C1 Relation or C1 Tuple_Bag
        - update, now: r->only_member and b->only_member
        - or some symbolic may still be useful
    ?% to Relation from Tuple or Tuple_Bag or C1+ Set or Bag of Tuple
        - update, Tuple version now: ?%{t}
    +% to Tuple_Bag from Tuple or Relation or C1+ Set or Bag of Tuple
        - update, Tuple version now: +%{t}
    ? to Set from Relation or Tuple_Bag
        - update, now: ?|
    + to Bag from Relation or Tuple_Bag
        - update, now: +|
    # to (non-negative) Integer from Set or Bag or Relation or Tuple_Bag
    ?# to Boolean from Array or String or Set or Bag or Relation or Tuple_Bag
        - OBSOLETE, now just ?
    ? to Boolean from Integer or Fraction or Float or anything stringy or homo
- Note that # or ? or ?# would most likely NOT be defined on Blob/Text
as there is no single most reasonable canonical interpretation.
- There is no casting to or from Structure, just longer-form routines/exprs.
- If one wants to compose a Relation or Tuple_Bag from tuple expressions,
use cast eg ?%{t1,t2,t3} or +%\+{t1:7,t2,t2} as there's no literal for that.

* TODO: Think of namespace issues since foundation routine calls, both
function and procedure, are both for built-ins that the parser expects, and
for third-party extensions implemented low-level.
- Also tied to this is how to invoke routines implemented in peer languages
like External is meant to be about, not just host languages.
- Do we want to make foundation routine names multi-level themselves, or use
the single-level management way that C libraries tend to?
- So, eg:
    -->  is for routines defined natively with Muldis D code, live in packages
    -->^ is for routines defined outside Muldis D code
- All -->^ calls are core-defined; to call third-party host or peer
routines, do it by way of a core-defined generic proxy or three.

* See http://rigaux.org/language-study/syntax-across-languages.html .

* Idea of naming math ops after result also used in Logo language, with
those exact names: {sum / difference / product / quotient}.

* See https://en.wikipedia.org/wiki/List_of_numeral_systems .

* Per https://en.wikipedia.org/wiki/Tetration and
https://en.wikipedia.org/wiki/Hyperoperation there are 4 commonly used math
hyper-operators, in succession: addition (repeated successor),
multiplication (repeated addition), exponentiation (repeated
multiplication), and tetration (repeated exponentiation).

* The mnemonics of : are pairing (visually a pair of dots also),
whether pair literal or generally name-asset
pairings either in structs or for naming entities or for assignment.

* http://rajeevrastogi.blogspot.ca/2015/06/presented-paper-in-pgcon-2015-on-native.html

* http://www.frozennorth.org/C509291565/E1939404619/ - The Cult of the NDA

* Apparently APL has right-to-left associativity with no precedence.

* Free up $ to just be for generic operator use.
- Make \$foo mean what $foo used to mean, and \\$foo for \$foo, or we could
do other alternatives, but the main point is $foo is not special syntax.

* Stop using triple-dot "..." as special symbolic Plain_Text syntax to mean
"definition not given here" or such, which could either be a visual TODO
marker that compiles but dies if run, or which is for use by extensions to
mean the implementation must supply a meaning as it is eg low-level.
- Instead have some -->^... or other syntax to mean implementation defines.
- Thus ... is freed up for use as an ordinary symbolic operator, such as
for "x...y" sequence specs etc like Raku has.
- One is still free to user-define a ... operator that means die, though
some alpha name is probably more appropriate.
Example:
    MD.Universal."..." ::= constant fail;
- ACTUALLY, per what its influence Raku does with its yada-yada-yada
http://doc.raku.org/language/operators the ... only has that meaning when
it is the only statement in the routine/type, and so it is an alternative
to a top-level expression/statement rather than a generic expr/stmt node.
- SO WHAT WE REALLY WANT is for -->^foo to be used for all things expected
to be implemented at the implementation level, analogous to a Perl
bootstrap call; the foo may either be system-defined in core or provided by
some implementation-level extension, but syntactically it is all the same.
- Hence the sole use of ... is as a TODO for users, so why not that just be
an ordinary routine, maybe even call it TODO() or such.

* In the Perl parser, at every stage of the pipeline, the input and
output are both node trees consisting usually of nested Perl arrays.
The input of stage 1 is something like ['source_code','...'] and so on.

* Muldis D structs, particularly source code, should be represented in
foo-hosted language variants by their native concepts of structs or generic
objects if possible, or hash maps otherwise.  For example, with Perl
hosts, only Muldis D arrays should be represented by Perl arrays, and
Perl hashes should represent structs in general, with 'ordered' keys just
being strings like "\x0" or "\x1" etc; if hashes displayed in sorted key
order those would sort first presumably.  Or Perl/etc arrays could still be
used sometimes as an alternate terser syntax.

* Even though it takes more space, make the Muldis D code have possibly
several levels of meta that keeps copies of the raw string source, so that
it actually is possible to regenerate the original string source to the
smallest detail, including individual parts of runs of string literal
tokens, and the original escape sequences in strings, etc.
- That is, each stage of the parsing that would otherwise do some kind of
lossy normalization would keep a copy of the pre-normalized tokens.
- To save space/complexity, only save the extra copy if it adds information
such as if the string actually either did have escape sequences or was
split into segments etc.  Otherwise, the non-meta version is identical to
the source string token.
- Besides perfect round-tripping, this will also make it easier to give
meaningful error messages or debugging messages showing the context in
terms of the original source code.
- But its still safe to start this only after the level of Unicode NFD/NFKD
folding; if users REALLY cared about that level of raw, they have the
original Blob/Text as applicable.

* Now ALL string-like contexts in Muldis D are quoted using ['"`] pairs,
including ALL comments.
- Now / and * are treated as ordinary symbolic chars in all outside-string
contexts, and /* or */ are valid as ordinary entity names.
- Now an ordinary backtick-quoted string is a treated-as-whitespace comment
and this is just a simple take contents literally there is no escaping.
- So only '' and "" support escape sequences and run-catenating, `` neither.
- (Note: VBScript uses leading single quote to indicate code comments.)

* Backticks are no longer available (comments took them) to use with generic
prefix/infix/postfix operator call syntax.  Any operator name that is not
syntactically valid as a bareword eg "op arg" or "arg op arg" or "arg->op"
may no longer be called that way and must instead use either the foo() or
()-->foo form.
- All the other disambiguation options eg `op op "arg"` are still available.
- As a side benefit, quoting an op no longer possibly changes its precedence
level, so its much cleaner now, just alpha precedence and symbolic precedence.
- Now there is the new disambiguator '<-' which is the mirror of '->' and
lets you force prefix interpretation rather than infix, eg "op<-op arg" or
"op op<-arg" etc where none means "arg op arg".
- Perhaps for completeness we can support -><- to be explicit / have parity
for that third option, eg `arg->op<-arg`.
- UPDATE 20161004 THE -> <- DISAMBIGUATORS WERE REMOVED AS USING THEM WOULD
TEND TO MAKE CODE MORE CONFUSING IN PRACTICE, SO ONE SHOULD INSTEAD JUST USE
PARENTHESIS AS APPLICABLE WITH PREFIX AND INFIX SYNTAX, OR A NEW ALPHA
KEYWORD pipe REPLACES -> FOR WHEN THAT IS MAINLY USEFUL EG WITH ->nest ETC.

* Now :: is the namespace separator for names within a package, eg
Muldis_D::Integer::NN etc.
- The : is special anyway and the :: mnemonic already assoc with naming things.
- Leading :: means we have an absolute path rather than a floating one.
- Having whitespace around the :: is allowed and doesn't change the meaning.
- Now . is treated as ordinary symbolic char in all outside-string contexts
and . is valid as ordinary entity names, presumably used for infix operators.
- Now :: is no longer a namespace separator within a package name itself,
that is for organizing packages themselves relative to other packages,
and we have to come up with some other scheme to use there.
- Perhaps ;; is a good candidate for separating parts of package names, and
it has the additional benefit of being portable on Mac HFS+ filesystems.
- Languages using :: for namespace separators include C++, Perl.

* Consider namespace-block syntax to provide a syntactic shortcut for declaring
entities, which would prefix that to any entities declared within, for example:
  namespace Muldis_D::Integer
  {
    add ::= function ...
    NN ::= type ...
  }
... which has the same effect as:
  Muldis_D::Integer::add ::= function ...
  Muldis_D::Integer::NN ::= type ...
- See both C++ and C# for prior art that use that exact syntax.

* It turns out C# also like Perl supports trailing commas in lists.
So does Python.

* This is useful http://www.hl7.org/ example http://hl7.org/fhir/2015May/datatypes.html .

* See http://design.raku.org/S22.html about packaging stuff, which is also
relevant to compilation unit long names etc.  Also not to be confused with
distribution names.
- A compilation unit is just something that can be compiled on its own,
typically a file or a string.
- Saying 'use foo' may get foo from the file system or from an installation
repository (not the same thing) or from the network etc; the compunitrepo
in use determines where to look.

* Eliminate 'floating' identifiers as they were previously known such that
they traversed namespaces and tried to match on leaf nodes, which had a
variety of issues and led to other complexities.
- Instead, inspired by C#, the replacement 'floating' identifiers just look
at the 'searching' list and directly catenate the identifiers to all of the
prefixes given and those are the full candidate names to try and dispatch
on where corresponding materials exist.  Corresponds to C# 'using'.
- Note, C++ also has a 'using' keyword which appears similar maybe.

* Fundamental reorganize in-package namespaces to be more like
MD."+".Integer rather than MD.Integer."+" for several reasons.
- One is this works much better with the redef of 'floating' operators.
- Another is this makes the organization a lot more like typical languages
that support multiple instances of the same routine base name in a
namespace and generate a unique name etc on the parameter types; the
overloaded variants now are more clearly bundled visually.
- How this typically works is we would cluster overloaded ops like this:
    MD."+"."" ::= function virtual ...;
    MD."+".Integer ::= function ... implements "" ...;
    MD."+".Fraction ::= function ... implements "" ...;
    MD.NumericOrSomeSuch ::= type abstract;
    MD.Integer ::= type ...;
    MD.Fraction ::= type ...;
- We can still have namespaces to group things under, eg MD.Source_Code or
MD.Integer etc but then those are more for less common or optional things.

* Make Identity_Identifier disjoint from Identifier where the latter is what users
write and the former the system generates/resolves at runtime that is
sensitive to context like what packages are loaded/etc, a Identity_Identifier probably
is just an integer payload like with a filehandle etc, both are Structure.
- As before, identifiers have the absolute/relative/floating variants.

* Fully separate the designs of the fully-qualified language names of
Muldis D syntax/low-level itself and of packages / compilation units.
- Keep the syntax name declarations simpler in principle as in theory this
is less likely to have incompatible versions develop or code wanting to
declare multiple authority support rather than just one; they can declare
just one and the onus is more on forks to figure out how to be compatible.
In theory more variation would happen in the core library than the syntax,
or anyone forking the syntax would be good and use a different Syntax Name.
- Package / compilation unit names can become more complicated and in
particular become data structures definable by routines or possibly able to
evaluate their environments, such as is the case with Raku comp units.
They can then for example declare different vnum ranges per declared auth.
- Make local package short names and their long names logic separate; only
the former is used for referencing in a package and the latter is mainly
just for instructing the package loader / comp unit finder etc.
- While Raku has the more complexity in the language name too, that one
declaration covers both the syntax and core library while mine doesn't.
- Review how segmenting of packages is declared/done.

* http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql/

* Tom Lane starts working for Crunchy Data enterprise Postgres late 2015.
http://www.prnewswire.com/news-releases/crunchy-data-strengthens-open-source-development-capability---tom-lane-joins-the-crunchy-data-team-300168177.html

* DBIx::PreQL looks interesting ... and SQL::Interpol maybe.

* See https://www.snip2code.com/Snippet/454117/Relational-data-languages
or https://github.com/seinokatsuhiro/koshucode-design/tree/master/note/relational-language
or http://c2.com/cgi/wiki?QueryLanguageComparison .

* Apparently here-docs Perl inherited from shells; eg:
psql <<EOF
\x
SELECT * FROM foo;
EOF

* https://www.simple-talk.com/sql/learn-sql-server/sql-server-sequence-basics/
- Also says: A sequence is a list of numbers. A series is a sum of numbers.

* SQLite list comment:
Artifact IDs used by fossil are the SHA-1 hash of the file contents, and
the checkin IDs are the SHA-1 hash of the check-in manifest contents.  They
are *NOT* random but rather, are 100% deterministic -- that is if you run
the sha-1 hash over the same input data you will ALWAYS get the same result.

* Add new abstract types Boolable and Numable which contract that composing
types each have a single canonical way to cast their values as a Boolish or
a Numeric respectively, where said cast operators are the unary ? and #.
- For a numeric type, ? always means is-nonzero, and for a collection type,
? always means has at least one element, for a boolish type, ? just results
in its operand.  For a collection type, # is a count of its elements (a Bag
counts each instance of a quantified value as 1 towards the total), for a
boolish type, # is 1 for true and 0 for false, for a numeric type, # just
results in the # operand.
- Both ? and # are only defined for homogeneous collection types and not
heterogeneous collection types.  For a Relation, they count the tuples not
the attributes.  For Array/Bag/Set/String they count elements.  They are
not defined for a Tuple to eg count attributes, nor for a Structure.
- Whether or not to define ? or # for a Blob or Text is questionable since
there isn't such a clear 1:1, especially for #.
- Note that Raku has removed special cases so while empty string is
False, any nonempty string including '0' is True.  Many other languages are
similar I think.

* It looks like C# has a feature analogous to Raku's "trusts:
internal members are visible to all code in the assembly they are declared in.
(And to other assemblies referenced using the [InternalsVisibleTo] attribute)

* In C#, "extension methods" act like methods defined within a class but
aren't actually defined in or alter that class.  Muldis D analogy is all
in-scope routines for a type whose first parameter is the class object analogy.
Or it seems to me that you can have divide a class's methods into bundles
so some bundles are loaded optionally by the user if they want to use them.

* Derived from / assumed based on http://www.pipelinedb.com/beta (based on
Pg 9.4) how a real-time or streaming DBMS works is that SQL/etc queries are
always active, and continue to send new result rows as new data arrives
that satisfies the query; queries are not constantly re-issued.

* Date:     Wed, 8 Apr 2015 09:58:24 +1000
From:     David Bennett <david@yorkage.com>
The point of this is to avoid having to copy the relation. If I have a relation
of 1 million rows and an update affecting 10 of them, then the update can be
expressed as the removal of (at most) 10 tuples and the insertion of (at most)
10 tuples. Then if the relation fails a constraint, reversing the operation can
be accomplished by the removal of (at most) 10 new tuples and the insertion of
(at most) 10 old tuples.
Full scale DBMS do something similar by creating all the required tuples as
blocks in temporary storage and then just switching around the links. If every
block has a serial identifier then it becomes possible for every user to see a
consistent view at all times, with no locks.
As part of this approach to updates it is straightforward for the implementation
to add a unique identifier (to A and B) so that the deleted tuple and the
inserted tuple can be paired up. This makes them accessible to a transition
constraint using RA operations.

* Demote SC_Identifier and Capsule from low level.
- We then have: Boolean,Integer,Pair,Array,String,Bag,Structure,External.
- SC_Identifier and Capsule are now subtypes of Structure instead.
- A SC_Identifier is just a Structure of the 4 elements where each element
is typically a Structure of String, or an Integer.
- A Capsule is just a Structure whose first positional element is a SC_Identity_Identifier.
- All Source_Code types plus core-defined Source_Code_Meta are now typically
just Structure whose first positional element is a string saying what main
type that value is of.
- Generally speaking, anything related to identity-qualified identifiers has
been booted from the low level.
- Generally speaking, the Plain_Text parser and compiler need to be able to
work entirely in the absense of the Muldis_D package and all of the data types
composing the native form of Muldis D code need to be simplified so each is
identified with a simple single-level string rather than a SC_Identity_Identifier.
- Now typically most types defined by the Muldis_D package are unlikely to
be defined as Capsule but rather just as Structure with a leading positional
element giving its main type.
- So coin a new set of simple strings that are the identifiers for
the non-low-level core types, such as Fraction etc.  The existing core types
that are Capsule are now instead these new kinds, and their definitions are
still essentially unchanged, eg: \%(\Fraction,numerator:5,denominator:8).  That
value we can use as-is in the absense of the Muldis_D package; the package
would still declare though:
    Muldis_D::Fraction ::= type of Tuple where topic ^?= \%(
        0 : type of SC_Attr_Name where topic = \Fraction,
        numerator : $Integer,
        denominator : $Integer,
    );
- We need some generalized shorthand for declaring a singleton type whose
value is drawn from another type, such as the above example.
- The $Foo will still resolve to an SC_Identity_Identifier like before, because
actually testing the $Integer thing formally is still at runtime, though
while bootstrapping and we don't have the Muldis_D package, that check
doesn't actually have a SC_Identity_Identifier to contend with, um stuff.
- Generally any system-defined type that a typical compiler would have
special knowledge of, such as Fraction or Relation etc, would have the new
simpler form of specification.
- The standard hosted-data variants of Muldis D and this new form will then
also very closely resemble each other as much as possible.
- It will generally be all pure Muldis D code that both resolves non-identity
Identifiers to identity ones, and likewise it is pure Muldis D code that tells
the low level register/load-package routines what their identity namespace
actually is, never mind what the package actually declares itself.

* Add abstract type Handle, to be composed by types whose values are each
associated with some possibly mutable state kept 'somewhere' such that the
package defining the composing type may typically know where that is but
the type user might not.  This may be the closest we get in practice to
mutable OO where value identity is per object itself rather than its
payload.  Routines would mainly be procedures rather than functions.
Example API procedures may be analogous to those for explicit memory
management in say C++ or where one explicitly has allocate() and free() etc
calls, plus fetch() and store() etc values there.  The value of the handle
type would have some kind of lookup key into an external data structure and
the value of that key would typically be generated by allocate() etc like
with C's malloc.  Explicit free() would likely be necessary but that for
performance reasons we could probably attach directly-write-only meta-data
to the handle value that lets it auto-release when no one refers to it,
similar to the write-only meta-data that helps say with indexing
Bags/derived etc.  So Handle is a root class in some senses like Object is
in some OO languages with respect to mutable objects.
- Related to this, we might want some global database with an attribute per
loaded fully qualified package and abstracted access to that, where some
packages can keep their squirreled away data?

* Simplify parser by an order of magnitude.  All delimited strings do not
contain a literal occurrance of their delimiter, or to be specific of their
trailing delimiter if different, and all characters between the delimiter
pairs simply represent themselves, including backslashes, iff the string
does not contain a backslash as its very first literal character.  If the
string does contain said leading backslash, then it is stripped from the
input and the subsequent characters are interpreted as including escape
sequences of various kinds.  A trailing single backslash either represents
itself or is an error but won't prevent string from being parsed out of
surroundings.  Optionally/ideally we can forbid some other characters from
appearing literally, particularly all control characters, or at least say
that the canonical Plain_Text form will escape them, eg so they survive
unscathed through various transports, but having them appear literally is
not strictly forbidden.
- Any consecutive delimited strings of the same kind are treated as 1 token
that is 'foo''bar' is equivalent to 'foobar' and "foo""bar" to "foobar";
this feature is not allowed for `foo``bar` which still counts as 2 tokens.
Among said consecutives, the internal leading backslash to indicate
escaping behaviour only applies to one segment, so for example you can have
'foo''\bar''baz' and only the middle part is escaped while the end parts
are treated as is, especially useful if the literal contains something like
a regex or the middle part has a single-quote character in the data.
- In an escaped string, \q stands for a character of whatever the delimiter
is, so '\\q' is a single-quote and "\\q" is a double-quote while 'x\q' or
"x\q" is exactly what they contain.

* See http://en.wikipedia.org/wiki/Dependency_injection and
http://en.wikipedia.org/wiki/Inversion_of_control which describe some
things we already are doing as well as that we could do significantly more
of.  In particular, if for every regular system-defined type intended to be
used for user data, there was an associated abstract type which defines its
entire API, and users always defined their code in terms of the abstract
type, or other abstract types that defined portions of its API that they
cared about, then it would maximize the amount of library code that would
be reusable with different user code and user-defined data types.  That
said certain type definitions may be a notable exception where users may
prefer to name the composing types directly, eg for database relations.

* Remember when translating to/from SQL, "from foo" means "from foo foo"
and that with "from foo bar" that means "for each foo as bar".

* It turns out that Ref_Eng implements the "flyweight pattern" re its design
to share object state where possible, though I'd never heard of "flyweight"
at the time I designed that feature.

* Make a new Muldis D codeless language module for BBEdit replacing the old
one that is completely obsolete.  Use things like "::= function" etc as
search pattern for the generated routines/etc menu.

* Add one or more new node types that make available the compilation/etc
context of code to that code, or other contextual information known at
compile or link time.  The source code of any registered or loaded package
could be returned as a Source_Code etc value and then studied like any data
structure at runtime.  Particularly important also is context-sensitive
info such as, what is the fully-qualified name of the routine asking the
question.  With these bits, we could basically implement all the
complicated stuff such as resolving partially-qualified names to absolute
or identity ones, or looking up or testing dispatch candidates, or reading
traits or other meta-data of types or routines, can be done in Muldis D
itself.  What actually needs to be low level is simpler stuff like "call
this (fully-qualified-named) routine".  This is valid to have in purely
functional code because, during the context of any single higher-level
statement execution, the values returned by such are constant, so its as if
these questions were asked and answered at compile time, as if the answers
were baked literally into the source code of the function asking the
question.  The "current" source code can not change but possibly between
statement boundaries, and it is infrequent.  Logically it is like every
function has been given and is passing an extra read-only argument etc.  An
example use case of this general feature, besides implementing $foo or foo
isa bar or default_of foo etc or "what is the type or MST of this value" is
letting one have multi-dispatch for "constants", eg, one could declare a
virtual constant, or more likely actually a function taking a type name or
example value, and then ask, eg call the function named "one()" for that
type, so eg the Numeric role could call a particular type's selector for
the number 1 in a generic way.  This is largely where one gets a lot of the
introspection power of Muldis D, eg meta-model etc access, eg, given a
value, you can then ask details about its type or what routines are
implemented for it etc.  Also makes the language more self-hosted.  This
stuff may possibly lead the way to being able to invoke functions on their
definitions rather than names, eg more truly higher order functions?
Runtime information such as "tell me about my call stack" is not part of
this so there should be no worries about this breaking pure-functionality
or memoization etc.

* Make abstract types try and implement all operations where logically
possible, shades of Low_Level.mdpt, eg multiply in terms of add etc, and so
the logical semantics are recorded, but then implementing types would then
'implement' these non-virtual abstract type operators in order to be
faster, eg dispatching to -->^integer-multiply etc.  Of course we still
want to minimize the number of -->^ and emphasize each implementation
choosing to override for itself for efficiency, but what I'm talking about
here is for the use cases where it would just be insane to not always
override, like integer-multiply, and so by having the -->^ we declare we
expect all implementations must provide it, or something.  This policy can
be relaxed.

* We want to figure out rules for where particular syntaxes are overloaded
so to help consistency and predictability.
- The "." generally says "accessor" but its a question of how far one can
go with that.  In general it can't have both the Attributive and Collective
applications because Relation satisfies both, and then there's also the
question as to which of those two Tuple should satisfy.  Or alternately
there is some other role that "." is used for that is distinct from those.
- Where we have accessors that access single elements, we also need variants
for questioning the presence of that element or some other aspect.
- Where we have accessors for single elements, we need something for dealing
with multiple elements, though those would generally return another value of
the collection type and may more likely be type specific.
- Would be nice to have some symbolic syntax for has_attrs, has_just_attrs,
and for each versions that both do and don't check the types of those attrs,
these would be used a lot in definitions of attributive types.
- Is \foo just shorthand for \~{foo} or \$foo or something distinct?
Especially given the accessor context we probably want it to be distinct,
eg, maybe that's the new SC_Name or SC_Attr_Name.
- .? for has_attrs, .?= for has_just_attrs; variants take Heading or Tuple
to either not or so check the data type; SC_Name for singular of Heading
to check one attr name; you still need Tuple for the singluar due to needing
name plus type.
- Or ^? plus ^?= for the multiple versions of the above which looks like
projection, though don't forget that ^ has a host for complement or to-set
or to-bag etc so we need to play nice with that.
- We should reserve the dot for things that return single elements and not
slices of the collection, and something else for the slices, it really cuts
on confusion, and then one can use a single item in place of a list when
doing a slice and still get a slice, that's more the dwimmy we want.
So single elem accessors (always by key/index) or does-exists or exists-or-voids (vert+horiz or horiz):
  tup . \atnm
  tup .? \atnm
  tup .! \atnm
  cpsl .> \atnm
  cpsl .>? \atnm
  cpsl .>! \atnm
  ary . index
  ary .? index
  ary .! index  - returns elem val if exists or void
  dict . key
  dict .? key
  dict .! key
  bagsetrel .# member  - returns quantity for bag, else 1 if exists, 0 if no exist
  bagsetrel .? member aka 'has'  - returns bool based on existence
  bagsetrel .! member  - returns member if exists or void
So attributive (always by key/index) slices or tests (vert slice):
  tuprel ^ \atnm or tuprel ^ \~{atnm} aka 'on'
  tuprel ^? \atnm or tuprel ^? \~{atnm} aka has_attr_names
  tuprel ^?= \atnm or tuprel ^?= \~{atnm} aka has_just_attr_names
  tuprel ^? \%{atnm:$Integer} aka has_attrs
  tuprel ^?= \%{atnm:$Integer} aka has_just_attrs
  cpsl ^> \atnm or cpsl ^> \~{atnm}  -- returns as tuple
  cpsl ^>? \atnm or cpsl ^>? \~{atnm} aka has_attr_names
  cpsl ^>?= \atnm or cpsl ^>?= \~{atnm} aka has_just_attr_names
  cpsl ^>? \%{atnm:$Integer} aka has_attrs
  cpsl ^>?= \%{atnm:$Integer} aka has_just_attrs
  tuprel -^ \atnm or tuprel -^ \~{atnm} aka 'but'
  cpsl -^> \atnm or cpsl -^> \~{atnm}  -- returns as tuple
  rel ^+ \atnm  - project to Bag - if someone wants multiple attrs they can wrap() first
  rel ^+? \atnm  - project to Set
For collective (always by key/index) slices or tests (horiz slice):
  ary .+ 4 or ary .+ [4] or ary .+ [4,5,6] or ary .+ {4,5,6} or ary .+ (4..6)
  ary .+? 4 or ary .+? [4] or ary .+? [4,5,6] or ary .+? {4,5,6} or ary .+? (4..6)
  ary .+? $Integer  - do we want this?

* The operator // in Perl and ?? in C# has generic term "null coalescing
operator" https://en.wikipedia.org/wiki/Null_coalescing_operator and most
languages that provide it spell it either ?? or ?: where the latter is
shorthand for x-exists ? x : y.
- As such, ?! or ??!! might be the best-fit analogy for Muldis D
considering x ?? y !! z is already in use like with Raku.
- Note that we have precedents for overloading the mnemonics of ! to refer
to both false/not and void/not-exists.

* TODO: Think about matters of allowing strictness.
MD.Universal.Comparable ::= type abstract;
MD.Universal.Comparable.may_be_compared ::= function virtual
    --> Boolean <-- (Comparable, Comparable)
    is {commutative};
MD.Universal.same_Universal ::= function
    --> Boolean <-- (Universal, Universal)
    is {commutative}
    : \%(topic.0,topic.1) -->^same_Universal;
MD.Universal."=" ::= function --> Boolean <-- (Universal, Universal)
    is {commutative}
    : same_Universal(*topic) ?? true !! may_be_compared(*topic) ?? false !! fail;
Alternately forget about doing this here and just make checks for such to
be a domain of add-on code analyzers.

* It seems C# has a lot in common with what I designed so far of Muldis D
independently of that knowledge.
- Has both 'using' keyword and also local aliases for used 'namespaces':
    using Net = System.Net;
    using DirInfo = System.IO.DirectoryInfo;
- Also namespace disambiguation / like fully qualified names:
    global::System.Console.WriteLine(number);
    # Above works if someone declared a local thing named 'System'
- But C# 'using' seems to do both Muldis D 'using' and Muldis D 'searching'
since Muldis D 'using' doesn't possibly point to sub-namespace.
Or C# 'using' may be more like Muldis D 'searching'
and C# Identity_Identifiers declarations in project are more like Muldis D 'using',
or the 2 'using' may in fact directly correspond as all,
since 'using' not required to call identity/global things in either case.
- In C#, 'using' can appear either inside or outside a 'namespace' declaration.
- Regarding implementation of Muldis D over C#, see C# "dynamic" type.
    dynamic x = new Foo();
    x.DoSomething();  // Will compile and resolved at runtime. An exception will be thrown if invalid.
- In C# the 'set' accessor routine has the implicit parameter named 'value'
which would seem analogous to Muldis D's 'topic' at least there.

* In MS SQL Server:
- The money and smallmoney data types are accurate to a ten-thousandth of
the monetary units that they represent, so 4 decimal places rather than 2.
- The sql_variant type is a union type of sorts that can be used in db
columns among other places, though it is limited in its options.
- Given "select top 5 with ties ... order by foo", what the 'with ties'
does is, when multiple rows might sort equally into the 5th place, all such
rows are returned, even if more than 5 total rows may then be returned.

* I should consider a re-look or merger of Muldis D using/searching concepts
considering that to some extent they overlap functionality.

* Define some shorter-hand for common formats of types restricted from
generic collection types, analogous to eg Array<Text> or Set<Integer> etc
that many other modern languages have.  The shorterhand would take the
place of the "where" in common cases, or supplement it.  We'd want versions
both for restricting simple collections like
Array,Set,Bag,Interval,Quantity but also attributive ones like
Tuple,Relation.  That being said, we can see how short the likes of
has_attrs() or forall() can get, may be good enough.  But we still probably
want a few extra keywords for 'type' for the ones that can be really short,
eg "type of Array member Text" or "type of Tuple attrs (x:Integer,y:Fraction)"
where in the latter case we take the same input as a function's "from" but
disallow positionals for Tuple.  Both member/attrs are sugar for where with
function calls and those functions are defined by abstract types, so a wide
variety of Collective or Attributive types could take advantage of this ...
or maybe this would be a problem if the package doesn't use Muldis_D pkg?

* Define as many routines as possible just against abstract types, so only
a minimal number of routines need to be virtual.  For example, many of the
Numeric ops or Collective ops etc don't need to be defined per composing
type.  Likewise, define most Boolean/Integer/String/etc ops at
Boolish/Integral/Stringy level etc.
- Keep in mind that a routine can still declare that it implements another
routine even when the other routine isn't virtual, and in that case the
former is actively overloading the latter, and calls directly to the latter
will be redirected to the former if the signature is compatible.  That
is, when X implements Y, X will always take precedence over Y for callers
for whom X is directly visible, but when Y is not virtual then X won't be
called, so to avoid action at a distance.
- In that case maybe we want keyword to be 'overrides' instead so it is
more explicit that the named routine already has an implementation.
- Likewise define as many synonyms as possible against just the abstract
versions of things, to minimize duplication, even when there is an
implementing routine, unless synonym only applies to specific composers.
- See also requires_implements for type declarations which helps with
defining a contract and so we have more of a full interface definition.

* Look up Raku Iterable role and other roles to see what we can mimic in
our abstracts/virtuals eg where routines are useful across types.

* Have procedure statement grammar nodes or -->^ for loading or
unloading or reloading etc packages at runtime.  Or call it mounting/etc.
- The loading version take a SC_Package value as input, which other than
any sanity checking it has to do, will cause that SC_Package to appear
verbatim in the system catalog, under the identity (base+auth+vnum) it
declares with 'package', will make it visible to identifier resolution and
routine dispatch code, and will compile it as applicable.  The loading node
will explicitly not load dependencies, and will fail if the package cites
any 'using' dependencies that are not loaded first.  It is invalid to have
circular 'using' dependencies between packages.  The act of actually going
to repositories or parsing to SC_Package values or introspecting the latter
to then go get dependencies, that is the job of higher-level code, and once
bootstrapped is performed in Muldis D code.  The loading code will fail if
a package with the same identity is already loaded.
- The unloading version either takes a SC_Package for consistency, though
it only needs package identity name, or it takes just a Identity_Identifier of said.
- The reloading version is like an unloading followed by a loading, this is
used say when there is runtime manipulation / data definition of a package.
- Perhaps the nodes may be split into separate compile/register and
link/make-available nodes.
- Maybe use 'activate' for the term of when it is linked / made vislble to
run, thinking like the term with chemicals etc or military; similarly or
etc 'reserve' could be the namespace locking or compile/register.
- When registering a package, it can also be marked as immutable, and as
such later attempts to unregister or reregister will be rejected.  This can
be done for practical or security reasons for "built-in" packages or others
that should be treated as read-only rather than updateable.  To change
those the DBMS/application itself would have to be restarted.
- The separate register/mount stage is like a name reservation but the code
doesn't explicitly use memory when it isn't needed (or it still can behind
the scenes but users don't see it so its like its not there).
- For that matter, perhaps also have it defined to an extent in Muldis D
code canonically how to resolve Identifier in general to Identity_Identifier, since
if we have to activate dependencies first, we can see all the candidates we
have to choose from already for resolution, what the invoker can "see".
- Perhaps the data dictionary is implemented at a higher level, as normal
variables, and updates to them would cause a reload/etc behind the scenes.
- So matters of persistence are orthogonal to execution.
- We would do persistence or repositories somewhat in terms of tied
variables or various kinds of I/O facilities whether files or otherwise.
- There may be multiple versions, eg original Source_Code and other that
has been optimized or folded, what was written vs what actually runs, may
be relevant for debugging or other meta-model related querying in functions.
- http://en.wikipedia.org/wiki/Java_Classloader may also be relevant.

* Update Plain_Text.pod concerning how and where the Script Name applies,
update the other parts on the post-20150226 design of packages vs modules
vs repositories etc.
- Split Script Name into 2 declarations, on encoding and on normalization.
- Example declarations at the top of a Module:
    Muldis_D:Plain_Text:"https://muldis.com":"0.300";
    script_encoding:ASCII;
    script_normalize:ASCII;
- Or:
    Muldis_D:Plain_Text:"https://muldis.com":"0.300";
    script_encoding:Unicode:"9.0":"UTF-8";
    script_normalize:Unicode:"9.0":canon;
- Only the script_encoding says how to map a Blob to a Text, and this is
possibly round-trippable losslessly depending on whether an encoding allows
multiple octet sequences for the same character code point.
- Note that valid options for script_normalize are {none,canon,compat}
which correspond to {none,NFD,NFKD} respectively.  There are possibly extra
terms for script_normalize if eg the compat rules changed independently of
Unicode main versions, eg with independent changes of annex versions.  It
is recommended to use canon by default as that most closely corresponds to
what users expect, probably, or compat possibly for security.
- See http://en.wikipedia.org/wiki/UTF-16 for explanations of surrogates etc.

* Formalize layout of package logical contents somewhat.
- A "package" is the primary logical unit for Muldis D code and consists of
a set of type/routine/constant declarations organized into a multi-level
hierarchical namespace.  Every package is versioned, its fully-qualified
name typically including a base name plus an authority and version number,
logical references to a package's contents are fully-qualified with such.
- A "material" is a {type,constant,function,procedure,synonym,etc}.
- A "folder" is a logical namespace that in/directly organizes materials.
- A "package" is a folder that has no parent folder.
- A folder may not directly contain both folders and materials; the
children of an folder must either be all folders or all materials.
- A "binder" is an folder that has at least 1 child material, and exactly 1
of those child materials is the binder's "cover" and must have the
unqualified name "" (empty string).
- A binder is conceptually a proxy for the material that is its cover from
the point of view of external users, and the other materials in that binder
are conceptually components of the cover.  A binder is a pseudo-private
namespace for its parts to reference each other but is automatically
excluded from "search" identifier resolution; "search" identifier
resolution will only ever resolve to a binder's cover.
- All explicitly named public "material" declarations actually are naming a
binder for which that material is its cover.
- All material declarations embedded in other materials
live in the same binder as what they are embedded in, the binder providing
a flat namespace for them.  Any non-cover materials in a binder may have
automatically generated names but could have explicit names.
- The term "schema" is not used to describe any kind of entity in the above
sense and is left available for use as generic database terminology.
- A "constant" with a true "folded" trait is the canonical way to represent
the current value of "the database".  In practice "the database" may be a
"binder" whose cover is a folded constant representing the database as a
whole, and other folded constants are in there too representing its parts
for easier human readability when serialized; in any event, on
serialization the folded constant's format/structure would be generated.
- To be clear, every material declaration such as `::MD::foo ::= function...`
is declaring a binder `::MD::foo` and the explicitly so-named material's
name is `::MD::foo::""` even if the function/etc declares no nested materials.
- Identity_Identifier `0` always means the current material.
- Identity_Identifier `1` always means the binder containing the current material.
- Identity_Identifier `2` always means the folder (possibly a package) immediately
containing the binder containing the current material.
- Explicit declarations like `::MD::foo::""::"" ::= ...` are still only
declaring binders with those names at the most, and never materials.
- One can explicitly declare a binder using syntax loosely similar to a
package or expression, like "binder {...}".

* Distinguish logical compilation units from their repositories.
- A "repository" is somewhere on or accessible to the system where packages
live, such as a "local" or "installed" OS file system, or some "object
store" service, or some version control system say.  When a user or
packages wishes to "use" a package, the available repositories are
consulted when looking for it.
- A "module" is a more physical manifestation of a package which includes
metadata of what version of the Muldis D language/grammar it is declaring
conformance to, as a Muldis D language name.  Usually this is something
like Muldis_D:Plain_Text:"https://muldis.com":"0.300" but depending on
context it might optionally declare the character encoding etc in use,
depending on whether the module is logically already in the form of a
character string versusan octet string for example.
- Muldis D code is often compiled per "module", and one can be completely
parsed and to some extent be compiled in complete isolation from others.
- Raku concepts of CompUnit and CompUnitRepo loosely correspond to the
above package/module and repository respectively.
- A "module" may either be read-only or writeable during the course of
program execution depending on the context.
- A "module" is the level of concept that knows about segments, so for
example the module for Muldis_D is composed of several segments, one per
disk file, and the multiple segments are reflected in the homoiconic/native
form.  But the module as a whole then maps to a package, and at the package
abstraction level the package is in a single piece and is not split up.
- TODO: Expand on what read-only vs writeable means.
- TODO: Use the term "library" somewhere.

* Alternatives we have re Muldis D source code:
- A Blob value whose octets are of some potentially unknown raw format,
this is what is natively read from a generic file system or object store,
if it is composed of character data then repertoire and encoding unknown.
- A Text value where we know we have a string of code points; the repertoire
(eg ASCII or Unicode) is known and the encoding (eg UTF-8) is not relevant
or ceased to be relevant at the language level during the process of
conversion from Blob.  Mapping from the Blob to this Text value involved
the scanning for the declared script name or otherwise determing what the
character encoding was in the Blob, and mapping the string of octets to the
string of character code points.
- A Text value that, in the case of Unicode, has been effectively
normalized to NFD per declaration of canon vs compat, this decision also
driven by the script name declaration.  The script declaration is retained
for maintaining intent should we round-trip to Blob, but Unicode normalize
transforms are lossy, we don't remember the original code points as they
should not be considered logically significant.
- A Source_Code value derived from parsing the latter Text value according
to the language name declaration, where this Source_Code defines a module
or segment thereof etc; at this point the language name declaration is no
longer needed to understand the Source_Code, but it would be retained as
meta-data for the purpose of inserting the same declaration in code
round-tripped to the Text form (subject to changes while in SC form not
invalidating the language name declaration like compatible version numbers).
- A Source_Code value derived from mapping the Module to a Package or etc.
- So the sequence of parsing, where each is conceptually a different pass,
mapping a value to another value, but can be implemented in a combined pass
is: Blob -> Text anormal -> Text normal -> SC_Module, SC_Package.
- A Repository can validly store a Package in any of the above forms
directly, or others.

* Potentially packages could be used to represent symbolic math etc, eg the
numeric values ARE source code specifying how to calculate it etc as applicable.
- A multi-versioned database could be represented on disk as Muldis D
source code where each version is a constant-defining package that refers
to its parent version(s) as 'using' and its value is a constant derived
from those.
- A graph could be similarly represented.
- A non-small database could be represented in multiple files similarly where
different parts are constants which compose by reference each other.

* Allow foo(:.bar) or \%{:.bar} which mean foo(bar:.bar) or \%{bar:.bar}.

* Add or rename-to the keyword/concept semantic_type which exists mainly to
be composed by other types for the purpose giving semantic meaning to a
type, eg this represents money or this represents a duration, and that in
particular these have nothing to do with defining a common interface or
there is no implication that types sharing a semantic_type would have any
operators or interchangeability in common.
- As such, the abstract_type concept which does imply / is intended for
having a common interface, eg Orderable, this is a different thing.
- Both of the above are open union types cited by the types they are
comprised of, and not the other way around.
- In any event, the fact that semantic/abstract types are cited as parents
of other types is important as something that can be introspected, and its
not okay for them to just disappear or merge into a 'where' definition.
In that respect they are more like type traits associated with a specific
type name, and they are orthogonal in some ways to the concept of a type as
a set of values, as here the name is important.
- Logically then, semantic/abstract types actually can not be named in type
constraints per se and rather they only have a program behaviour role in
resolving routine dispatch by extending the candidate list beyond those
normally visible in a scope.  As far as type constraints are concerned,
they are just equivalent to Universal.  Perhaps as far as routine dispatch
is concerned, maybe they can only be used in the signatures of abstract
routines and not concrete ones?
- Perhaps what we want is for this semantic/etc stuff to actually just be
additional traits of a "type ..." definition and not a separate thing.
After all, the "default" keyword of a type has nothing to do with its
definition as a set of values either.
- But types would still have to compose these using separate keywords
since we don't want them to contribute to the set of values, eg don't use
"of" because that would make the composer like Universal.
- ACTUALLY, if these semantic types are considered equivalent to Empty,
that is explicitly consisting of no values themselves, then they can be
named in "type union {...}" with the regular "of" types, so the composing
types then just inherit its "semantic ..." keyword/etc and not any values.
- Maybe lets merge {of,union} into one keyword, which may be spelled either
way for user choice/illustration (as they have the -->|to etc choice),
which takes a set of type names (if missing it defaults to the set of 1
item that is the maximal type) and the curly braces may be omitted if
exactly 1 item is named ... we can make the braces optional for other
commalist-set traits too; with this merger, "where" may be used with union
types, and also the canonical way to specify the empty type is to say "of
{}" (empty set).  As such, "default" is mandatory if "of" lists multiple
non-empty types where empty is defined by their own type defs having an
empty set for "of", recursively plus no "where" ... er, think this through.
On default values, make a set of all the parent types' default values and
then filter it by the current type's constraints; if the resulting set
doesn't have exactly 1 value, the current type must declare a "default".
- Any time a module Y calls X routine, non-abstract X directly visible to Y
are used first if any match the called signature, otherwise abstract X
directly visible to Y are used if any match the called signature, in which
case implementations anywhere on the system may be invoked if declared
somewhere to which the same abstract X is also visible to.  In order for Y
to prevent its call to X being implemented by some unknown remote code, it
must declare its own non-abstract X matching the signature or explicitly
use one such.
- Routines need to have some keyword(s) declaring them as abstract/virtual;
only those may be referred to with 'implements' by some other routine.
- Idea, let routines declare that they 'implement' other non-virtual
routines, as a way of overriding polymorphism, or declaring somewhat how
search-based calls should be resolved when several matching candidates are
found, useful in particular for subtype-specific implementations that are
more efficient, maybe use keyword 'overrides' or such.  However, this is
only respected when the overriding candidate in question is visible
directly to the caller, so such an override definition can't cause action
at a distance.  See also Raku syntax/terms for this.
- TODO: Think about stuff and things.

* Type correspondance between Muldis D and Raku:
Abstract:
    Boolish - Boolean
    Integral - Integral
    Rational - Rational
    ? - Real
    Numeric - Numeric
    Stringy - Stringy
    Textual - ?
    Arrayish - Positional
    ? - List
    ? - Associative
    Setty - Setty
    Baggy - Baggy
    ? - Mixy
    Orderable/Ordered - Ordered
Concrete:
    Boolean - Bool (boolean)
    ? - Bit
    Integer - Int (arbitrary precision integer)
    Fraction - FatRat (arbitrary precision ratio)
    String - Buf|Blob (stringish view of an array of integers)
    Blob - Blob|Buf (undifferentiated mass of ints)
    Text - Str (string of characters)
    Array - Array
    Set - Set/SetHash/QuantHash[Bool] (unord collection of values no dups)
    Bag - Bag/BagHash/QuantHash[UInt] (unord collection of values allows dups)
    ? - Mix/MixHash/QuantHash[Real] (unordered collection of values with weights)
    ? - Enum/Pair (a single key-to-value association)
    ? - EnumMap/Hash (mapping of k-v pairs with no dupl keys)
    Tuple - ?/Stash (a symbol table hash for package, class, lexpad, etc)
    Interval - Range (pair of Ordered endpoints)
    Quantity - ?
    SC_Func_Args - Capture (function arguments)
    SC_Func_Params - Signature (function parameters)
    ? - Instant (point on continuous atomic timeline)
    ? - Duration (diff btwn 2 Instant)
Literals:
    1/2 - <1/2> (general case, or 1/2 depending on context as that is constant folding)
        but <1/2> produces a value that is both a Rat and a Str,
        see http://design.raku.org/S02.html#Allomorphic_value_semantics
    0x'A705E' - :16{A705E}
Note that Muldis D purposefully excludes Complex numbers from core, so
Raku's <5.2+3i> has no core analogy.  Part of the rationale is the
combinatorial explosion of Complex types, eg cartesian vs polar times ratio
vs float etc.  More broadly speaking, anything that depends on irrational
or symbolic math values like pi are excluded from core.
Typedefs:
    Odd ::= type of Int where ?(topic mod 2);
        subset Odd of Int where { $^n % 2 };
Raku Multi-method dispatch goes only on parameters, return type not
considered, which is the same thing we do / how it should be done.

* Provide alternate versions of expressional if-else/??!! and
given-when-default that either do or don't shortcut respectively; the
former exists to provide a way to gate code that is only valid to execute
in some circumstances, while the latter is a logical foundation for
many/all expression operations but that it is valid to execute both
branches; we should have the latter so that these expressions don't act as
an optimization barrier forcing conditional exec when they don't need to.
Note, see also http://design.raku.org/S02.html#Lists eg re eager/hyper/race.
- So what we'll do is make the alpha syntax if-then-else and
given-when-then-default the one that does shortcut, which is consistent
between the expression and statement versions of said as well as common
practice / expectations in the world.
- The ??!! syntax is altered to be non-shortcut version of if-then-else and
it has a separate expr node type from if-then-else; users can optionally
wrap it in a function if they wanted to.
- A new ???{}!!! syntax is a non-shortcut version of given-when and in
parallel to ??!! it is infix.  Example:
    .round_rule ??? {
        $=>To_Zero : ...,
        $=>Up : ...,
        ...,
    } !!! fail
- So generally the language is such that for operator-like things which
have more than 1 argument and aren't called like foo(), everything that
doesn't shortcut is used infix-only, eg "foo and bar" or "x ?? y !! z"
while things that do shortcut have a keyword before the first argument in
addition to having infixes, eg "if x then y else z".

* Add several new expression node types / fundamental operations which are
list map and list reduce etc.  Syntactically they would be like a generic
function call syntax but that they call the other function per element of
the provided collection-typed value rather than exactly once.  Have
distinct ones for Array and for Bag; the former is needed for
non-commutative operators.  These are not low-level routines so they can
properly see the execution environment same as routine call syntax does
such as for resolving what to invoke and also for reading the declared
traits of routines such as is-commutative etc.  See also
http://en.wikipedia.org/wiki/MapReduce including the map-shuffle-reduce
threesome and key-1 plus key-2, I would want to support a similar
algorithm.  When calling map on a Bag source, there would be a single
routine call per distinct Bag element, and that call will be given the
key-quantity pair so can act appropriately; for a non-idempotent routine
preserving or acting on the quantity is important while for an idempotent
routine the quantity can be nullified.  Defining list operations in terms
of these should make it much easier for implementations to parallelize them
including over engines based on map-reduce.  That article should also give
me a clue on a parallel Plain_Text parser definition.  All the standard
relational operators etc should be definable in terms of these.
- http://search.cpan.org/~drrho/Parallel-MapReduce-0.09/lib/Parallel/MapReduce.pm
is also instructive.
- http://en.wikipedia.org/wiki/Map_(higher-order_function)#Language_comparison
- http://en.wikipedia.org/wiki/Fold_(higher-order_function)#Folds_in_various_languages

* See http://en.wikipedia.org/wiki/Language_Integrated_Query (LINQ) also for
inspiration of what operators to have etc.

* Leave -> available to be defined at the function level, hence it can be
overloaded for a wide variety of function types; we have use variants of
--> instead specifically defined in the grammar where we need those.

* The 2 main user-defined stages of the generic map-reduce algorithm
correspond more to our 'classify' and 'summarize' and Muldis D will use
terms more like the latter for referring to parts of the map-reduce
algorithm; the Muldis D terms 'map' and 'reduce' will have simpler meanings
corresponding to those of Perl or many other languages.

* The following is partly obsolete;
- The fundamental Collective ops now look like:
    - is_empty(list)->bool  # like count(list)==0 if most count work saved
    - count(list)->nninteger
    - has(list,universal)->boolean
    - where(list,func)->list like Perl grep is
    - map(list,func)->list like Perl has
    - reduce(list,func)->universal
    - group(list-of-pair)->list-of-pair
    - ungroup(list-of-pair)->list-of-pair
- Non-fundamental Collective ops:
    - in(universal,list)->boolean
    - any(list,func)->boolean  # short for map to bool plus [or]
    - all(list,func)->boolean  # short for map to bool plus [and]
    - none(list,func)->boolean  # short for map to bool plus ![or]
    - one(list,func)->boolean
    - does forall or exists go here / are they other names for all/any?
    - etc
        # To optimize we need a way to annotate functions so we know we can
        # stop once a particular condition is met, for example, with 'or'
        # reduction we stop once we see a true, 'and' when we see a false.
    - generate(state[func,args,list],nninteger)->state
    - ...
- The additional fundamental Array ops:
    - has_ord_pos(list,nninteger)->boolean
    - elem(list,nninteger)->universal
    - replace_elem(list,nninteger,universal)->list
    - slice(list,range)->list
    - replace_slice(list,range,list)->list
    - sort(list,func)->list
    - zip(list-of-list)->list-of-list
    - ord_pos_first_diff_elem(list,list)->nnint
        # For making in_order(list,list) efficient in both cpu, stack use,
        # result index may be just after last element of either input.
        # Alternately we could maybe express by piping simpler ops if we
        # can declare that we keep processing only while elements match.
- Non-fundamental Array ops:
    - subsequence_of(list,list)->boolean
    - supersequence_of(list,list)->boolean
    - ...

* TODO: Add further traits or external declarations that further help with
optimization, such as declaring distributivity, transitivity, negator,
commutator, inverse, monotonicity, truth preserving, false preserving,
symmetry, injective, surjective, bijective, isomorphic, other things.
- More broadly speaking, it is useful for an implementation to be able to
pipe several chained list operations when there is some declaration saying
it is safe to do so, especially say chaining map to reduce so we know the
map can stop when the reduce produces a particular result part way through.
The concept of http://en.wikipedia.org/wiki/Monad_(functional_programming)
may be relevant here.
- Have trait declaring on routine R1 that when we have a repetition of
calls with the same argument/s, we can instead call routine R2 once for the
whole sequence, which will do the same result more directly.  This is
useful when doing a reduction on R1 with a Bag of non-zero quantity
elements, or reduction on an Array with consecutive same elements, etc.
This trait should be useable in principle with any routine, doesn't have to
be commutative or whatever, but it would be invalid to declare for a
routine also declared as idempotent.  Examples are:
    - counting -> add
    - adding -> multiply
    - multiply -> exponent
    - catenation -> replication (Perl's "x" op)

* Per http://en.wikipedia.org/wiki/List_comprehension the main thing we're
missing is a generic generator function or three.  For example:
    - MD.Generator.init(func,seed)->state
    - MD.Generator.next(state,nninteger)->state
... where state is some Capsule type whose attributes are:
    - a CS_Identity_Identifier to the generator function
    - a CS_Func_Args of arguments to give the next exec of that function
    - the last values that the function generated
    - note that besides the generated value, the function returns a new set
    of args to give its next invocation, which lets it maintain its state
    and lets us have effectively a generator of an infinite sequence
... and a simple common example, the function just wraps Integer.succ.
- In some way the Ordinal role is related to this, as anything which
composes Ordinal can generate sequences.  However, the same base type eg
Integer could produce any number of different sequences, for example the
Fibonacci or multiply by -1 etc.
- We need to look up how Raku or other things do lazy lists including the
Iterable role and x...y syntax etc.
- We want an Iterable abstract such that every composer provides functions
like curr_val(state) and next_val(state) etc.

* What Muldis D would have as fundamental operators ...
- read List as either Array or Bag but not both in the same function
- map() takes 1 monadic function plus N inputs and has N outputs.
    - The monadic function is --> Universal <-- Universal
    - map() itself is --> List <-- (Identity_Identifier,List)
- reduce() takes 1 dyadic function plus N inputs of the kind that map takes
or outputs and has 1 output.
    - the dyadic function is --> Universal <-- (Universal,Universal)
    - reduce itself is --> Universal <-- (Identity_Identifier,List)
- sort takes 1 ...
... actually that still needs a lot of work.
- classify takes 1 monadic function of the same kind that map takes or
outputs plus N inputs of the same kind that map takes or outputs and has M
outputs where each output is a class-members pair such that there is 1
distinct 'class' per distinct output of the function and 'members' is a
collection of all the pristine inputs that mapped to this particular
function output, so the count of elements of members across the M outputs
is N.
    - the monadic function is --> Universal <-- Universal
    - classify itself is
    --> (List of Tuple{class:Universal,members:List}) <-- (Identity_Identifier,List)
- summarize takes 1 dyadic function of the same kind that reduce takes plus
M inputs of the kind that classify returns and has M outputs where each
output is a class-summary pair, the summary produced by running reduce on
the group.
    - the dyadic function is --> Universal <-- (Universal,Universal)
    - classify itself is
    --> (List of Tuple{class:Universal,summary:Universal})
    <-- (Identity_Identifier,(List of Tuple{class:Universal,members:List}))
- At the low level, for each of the above 4 functions, there are 2 variants
where one is Array to Array and the other is Bag to Bag.
- For reduce/summarize, all dyadic functions are permitted to be used with
their Array variants, but only dyadic functions which are both associative
and commutative may be used with the Bag variants.

* About low-level 'map' ...
- Very simple, processing each input element yields exactly 1 output
element, and each element is processed entirely in isolation from the
others.
- Where several input elements are the same value, their corresponding
output elements will all be the same value.  The 'map' is pure, and the
system can choose to memoize to arbitrary degrees, either calculating the
output for each distinct input element once, or once per instance, or
somewhere in between, this is an implementation detail that has no logical
effect, aside possibly from debugging.
- There are 2 variants, one being Array-->Array and one Bag-->Bag; use the
first when you have elements in order and their corresponding output
elements need to preserve that order; use the second when your elements are
not conceptually in order and you don't need to preserve that order, but
just the relative number of corresponding values.
- The processor function for 'map' will never be given context information
like index of the input in the source Array or like quantity for the
distinct element in the source Bag, rather the 'topic' will just be the
distinct element itself, and the output is just the direct transform
without any mandatory-for-all-map output format eg specific to a followup
classification or reduction use case.  One reason is to keep the API simpler
and more consistent no matter the source collection type or intended use
case of the output.  Another is so that the processor function's answer
can't possibly change due to factors that should be agnostic to what
sibling elements exist in the collection.  In particular, giving quantity
information to a 'map' processor function per distinct element would
require deduplicating the whole input data set, either unnecessarily or
prematurely, in order to give guaranteed consistent results for map
processors that pay attention to said quantity, and so would either break
our ability to parallelize the operation effectively or force an eager
classifying when otherwise not logically useful.

* Possible 'map' syntax based on general function call ...
    args --> func  # call func once with pristine args
    array_of_args [-->] func  # call func once per array element
    bag_of_args {-->} func  # call func once per distinct bag element, input is element+quantity pair
Or maybe:
    array_of_args -->~ func
    bag_of_args -->+ func
    set_of_args -->? func
Or better:
    ~-->
    +-->
    ?-->
... so the variant is on the same side as the input it varies on.
Then there's the reduce version, needs versions for: not assoc or commut,
assoc only, commut and assoc; commut only doesn't work in this context or
we treat as not assoc or commut.
Maybe have version like chaining associativity in Raku, so its a different
kind of reduce.

* Consider providing 2 implementations of the Muldis D parser in Muldis D,
one functional that parses a Text value to Source_Code, and one procedural
that parses from an input stream instead.  Conceptually the former is what
we really want, but the question is how to work with arbitrarily large
input.  Maybe the answer is for the former to be the canonical parser that
the reference implementation includes and latter can be a third-party-alike
add-on.  Possibly for the best as Ref_Eng keeps the result in memory anyway
so even if the parser works on streams what it feeds to doesn't.

* Adjust or prevent any language grammar features that would make it too
hard to implement a parser functionally as essentially a finite series of
map-reduce-ish operations, eg an early round is tokenize
simply into is-inside-quoted-string from is-outside-quoted-string, so
perhaps things like here-doc quoting might be an issue?

* See "Coding an operator to test a string for numeric" thread on TTM list
for 2015-03-24 for a related discussion.

* NEW PLAN (overrides 'syntax_using' etc) ...
Eliminate Muldis_D::Low_Level package and merge the appropriate parts into
the standard grammar itself.  This brings consistency as not only do the
value literals for Integer etc have their own grammar nodes, now all the
low-level routines do too.
- See also SC_Foundation_Singleton further below for a related concept.
- So now there is no question what behaviour keywords like "not"/"!", whether
meta-op or not, bind to; they always bind to grammar nodes.
- UPDATE - not/! META OP NO LONGER EXISTS; NOW :=foo IS ONLY META-OP.
- Now nothing in Plain_Text or its parse tree / native Muldis D binds to
something declared in a package.
- If there are zero 'using' in a module, then there are also zero ordinary
references to either type names or routine names, except for those declared
in the current module.
- The Muldis_D standard package has no dependencies and is defined entirely
in terms of the grammar nodes; eg Integer plus,minus etc are now nodes.
- A lot of Muldis_D routines are just shims over the nodes, consistently.
- Example nodes:
    - SC_Foundation_Func_Invo - use like "args --> $name" or "name(*args)" for low-level
        - \%(expr,expr) -->^same                  # used by = syntax
        - \%(bool_expr) -->^Boolean_not           # used by not/! keyword
        - \%(int_expr,int_expr) -->^Integer_plus  # used just like this
        - \%(expr,expr) -->^select_Pair
        - \%(pair_expr) -->^Pair_key
        - \%(pair_expr) -->^Pair_asset
        - \%(str_expr,nnint_expr) -->^String_elem # used by x.+[y] syntax
        - \%(ary_expr,nnint_expr) -->^Array_elem  # used by x.[y] syntax
        - \%(tup_expr,atnm_lit) -->^Tuple_attr    # used by foo.bar syntax
        - \%(tup_expr,atnm_lit) -->^Tuple_has_attr  # used by foo.?bar syntax
                # related foo.!bar shorthand "foo.?bar ?? foo.bar !! $=>Void"
        - \%(ref_expr,tup_expr) -->^select_Capsule  # wrapped by x=>y function
        - \%(cap_expr,atnm_lit) -->^Capsule_attr    # used by foo.>bar syntax
        - \%(cap_expr,atnm_lit) -->^Capsule_has_attr  # used by foo.>?bar syntax
    - SC_Foundation_Proc_Invo - as you would expect
        - \*^&[Universal_assign,target_expr,source_expr]  # used by x := y
        - \*^&[Array_elem,ary_expr,nnint_expr,expr]  # used by x.[y] := foo
        - \*^&[Tuple_attr,tup_expr,atnm_lit,expr]    # used by x.y := foo
- Note that some of the above operations may instead get their own nodes.
- ACTUALLY, THE ONLY PLACES SYNTAX LIKE THE ABOVE WOULD BE USED IS WHEN
THAT WOULD ALWAYS BE USED TO BOOTSTRAP ORDINARY ROUTINE OPERATORS; WHEN
THE GRAMMAR ITSELF HAS SPECIAL SYNTAX FOR AN OPERATOR, IT DOES NOT GET ONE
OF THE ABOVE NODE TYPES BUT RATHER ITS OWN.
- Note that we don't need to do this for constants because the constants
that the grammar knows about have their own node types as with previously.
- Nodes look unpleasant/verbose on purpose as people aren't supposed to use
them directly, but rather by way of the Muldis_D package shims.
- All normal low level foundation types are now declared by Muldis_D package
including Universal and Integer etc; for example:
    MD.Universal ::= type where true default false;
    MD.Empty ::= type of Universal where false;
    MD.Integer ::= type of Universal where \%(topic) -->^isa_Integer default 0;
    MD.Integer.NN ::= type of Integer where topic >= 0;
    MD.Integer.P ::= type of Integer.NN where topic > 0 default 1;
- Generally, \%(topic) -->^isa_foo only exist once; other code uses "isa foo".
- The "isa" node looks up any "type" per the same visibility rules as normal
dispatch to "constant" or "function" and invokes the chain of "where"
predicates starting with the parentmost "of" back down to the invoked type,
and results in true iff all the "where" resulted in true.
ACTUALLY "isa" shouldn't do any coercion like that, one should have to say
for example "foo isa $Bar" and not "foo isa Bar".
- Demote isa to a function over \%(expr,ref_expr) -->^isa.
- So "=" (equality test) is now specially recognized by the grammar, same
as each of "!",":=","::=",":","-->" as it just seems appropriate; as a
side-effect using a package declaring a symbolic "=" must now be invoked
quoted to get that one, where bareword = now means the standard one.
- Likewise ":=" is a special grammar node with no -->^ for it.  Ah er?
- Note that zero boolean operators are necessary at the grammar level,
other than the constants false and true; they can all be defined in terms
of ??!! and canonically are in the Boolean package.  The only reason
there's a grammar node for "not" is because its a meta-operator, and this
grammar node just short for "<predicate>??false!!true" (no -->^ for it).
- UPDATE - not/! META OP NO LONGER EXISTS; NOW :=foo IS ONLY META-OP.

* Need to add something to Plain_Text et al like
"syntax_using <alias> ::= <package name>"
which is like "using" but that it has the additional effect of defining
what Plain_Text itself implicitly uses for its various keywords or special
syntax and what they bind to.  Mainly affects what its meta-operators bind
to such as "not" or "reduce" etc, and they bind with "absolute" paths.
- The alias declared with syntax_using is forbidden from appearing in 'searching';
if you want to search the package, you have to declare it with 'using' also.
- Specifying syntax_using is optional, and if omitted, the alias defaults to
the empty string, eg `::""`, and what the package defaults to is
parser-implementation-defined, typically Muldis_D::Plain_Text.
- As such the empty string package lexical alias is reserved.

* Lookup 'abstract' further down this file regarding how identifier resolution
works.  The rest of this paragraph either amends or supplements that.

* Make parser and compiler as simple as possible such that each one has
practically zero knowledge of runtime issues such as the routine dispatch
system.  Each Muldis D package becomes a Perl package, 1:1, and every
Muldis D routine call is mediated by a runtime routine like
call_function(ident,args) so the compiled Muldis D code is isolated from
all that complexity.  Since most MD code goes through Low_Level to eg do
all the math or array stuff or whatever, there's no Perl equivalent
translation to worry about.  All binding between compiled packages is done
strictly at runtime and call_function(ident,args) is the thing savvy to it.
Actually it is resolve() or something that provides a set of 0..N candidates,
each a Identity_Identifier, based just on the names, and call_function() then picks
one based on the argument data type.  Only 'search' can produce more than
one result, all others produce exactly one, which fails if it doesn't exist;
for relative and absolute we can know this at compile time, and search only
if the current package doesn't use any others, and identity only if pointing
to the current package.

* Have 2 main classes of comments, where the first class is treated as
documentation and the second class is treated as whitespace.
- The /*...*/ form is treated as whitespace and is preserved in the parse
tree in exactly the same way as whitespace is, and round-trips in the same
way.  It is not useful to introspect these kinds of comments, and they are
mainly for things like visual dividing lines or commenting-out code.
Given especially the latter use, arbitrary bracketing strings may be used
like with heredocs or quoted-printable, so commenting out code with comments
is easy enough.
- Where we desire the comments to be introspectable, they appear as part of
regular the regular code as Text literals along with some keyword or syntax
or routine name to associate them with particular things, eg ::?= .
- Where ::= is what-binding, ::?= is why-binding.
- Any parse node can be given a comment.
- :::= binds tighter than ::= which is tight.
- You can do:
    x ::?= 'the answer';
    x ::= 42;
  or
    x ::= 42 ::?= 'the answer';
  or
    x ::?= 'the answer' ::= 42;
  which all do the same thing.
- Parens may be needed like with ::= to ensure binding is to the right thing.
- Same method works for whole routines etc, todo how to work with param decls etc.

* Make it idiomatic that if a capsule or tuple type has an attribute that
is inapplicable for some values, that attribute is actually missing for
those values, rather than existing and being populated with for example
some special sentinel value.  Related to this, the terse infix ".?" like
"tup.?atnm" is provided to test the existence of an attribute.  Mainly this
idiom is for cases where there isn't any reasonable "default" value for the
attribute to conceptually use when it isn't explicitly provided.  Case in
point for a type defining a type; "of" and "default" should be missing if
not defined; only "where" can default (to "true"), and even then, should
ideally be missing as well when not explicit.

* Let routines/types/etc have traits like public/private whose effect is
making them visible or invisible to partially-qualified identifier
resolution to identity identifier operations.  If not explicitly stated,
a material explicitly named by a user is public by default, whereas one
whose name is generated or that is embedded in another, the latter is
private by default.  Due to this feature, splitting a package into a
private internals and public wrapper should not be necessary.
That being said, support for declaration of multiple packages within a
single module / .mdpt file should be supported, where a common use of said
may be private/public separation.
Note that the public/private trait would attach to the namespace/name of
the material, not to its definition; eg, it belongs on the left hand side
of the eg "name ::= function...".
* OR WE HAVE THE CONCEPT OF LEXICAL ROUTINES INSTEAD OR ADDITIONALLY;
ALL ROUTINES DECLARED INLINE ARE OF THIS KIND.  THEY CAN STILL BE INVOKED
ANYWHERE IF YOU KNOW THEIR IDENTITY BUT SEARCH WOULD NEVER FIND THEM.
See also http://design.raku.org/S02.html#Scope_declarators eg 'anon'.
* The need for public/private is lessened by more recent developments, eg
the binder/schema split.

* Or we CAN specify a "complete" grammar but that as written you couldn't
actually execute it without help, eg interpreting several barewords in a row,
and so it is still more illustrative; you wouldn't gen a parser from it.
Call the file that defines this illustrative grammer Muldis/D/Plain_Text.pod
and so actually that is a replacement for Plain_Text.pod but it also more
explains the meaning of things rather than referring to Catalog_Types.
Focus example code here probably though Basics etc can have some too.

* In grammar, make it so 2 consecutive quoted string tokens, those enclosed
in single-quotes or double-quotes, will catenate at the parser level and be
treated as a single quoted string token.  This is a way of splitting long
strings over multiple lines without explicit catenation or special
linebreak treatment.  In particular should be useful for splitting
identifiers, as you can't just use ~ for those.  This feature will
emphaticly not be available for backtick-quoted string tokens, both as it
would conflict with things like "x inop preop y" and also no one should be
using such long names with that format; they can use foo() syntax then.
We still want to remember what happened as meta-data though, probably.
Apparently "infix blank operator" actually quite popular among languages.

* 20150126 comment on TTM list:
The @" convention of C# (and others?) is very useful for multiline string
constants, where you want the final string to look like what is written in
the code (including carriage returns etc). Especially where the string
contains code in another language (eg GLSL code embedded in C#).
- Also, different post same day, perhaps "discriminated union type"
might be a term for describing what an abstract type is?

* Make Low_Level just have Integer division func that rounds in one way,
specifically round to zero, and let any other options be implemented at a
higher level in Muldis_D proper.  Or at least the single function taking an
enum of rounding method is not in Low_Level; the closest is Low_Level might
give more than one distinct divide operator if reasonable.

* In new Basics.pod, start out by introducing the concept of packages and
how all code lives in one or another.
Then say you can only invoke any types or routines, including standard ones,
by default if you explicitly use the packages declaring them.
As an exception, some routines or value literals are implicitly made available by
the Muldis D Plain Text syntax itself, specifically those that the grammer
as special syntax for, such as assign() and not() and reduce() etc but that list is small.
First introduce the things you get just because your code is standard Plain Text
and distinguishing what you have to 'use' a standard package for to get.
All invocations due to implicit grammar will use the 'identity' identifiers
so there's no chance of some 'use' changing their behaviour.
Then introduce the most commonly used things that you wuould explicitly use
::Low_Level or Muldis_D proper or ::Unicode etc for, for each one.
An open question is whether using '=>' or '..' etc requires a 'use'.
Declaring :Plain_Text:Unicode just says what literals may exist in the source
eg as identifier or Text values, it doesn't give you the sysdef unicode operators.
Introduce the literal syntax for common data types like numbers etc or
everything that has a special parser-recognized syntax.
Generally speaking Basics.pod should now feature what used to be Plain_Text.pod,
not specifying a complete grammar but by outlining it and giving examples.

* Consider reserving the usage of terms like 'operator' and 'operand' to
being a syntax/grammar feature describing what syntax a routine call takes,
rather than it just being an alias for 'function' etc.  For example, if you
have 3 barewords in a row and either no external context or external
context says that is the whole row, eg delimited by parens, then we say
those barewords are (<operand> <operator> <operand>).  So, we say
backtick-quoting something will force it to be an 'operator' while
doublequote-quoting something will force it to be an 'operand'.  Or maybe
better yet, say 'term' or such rather than 'operand', so eg (<term>
<operator> <term>) and that may be standard practice; so then the operand
isn't a specific thing, it could be a whole multi-part expression itself.
A syntax rule is no 2 terms in a row, but 2 operators may appear in a row
iff the one on the right is a prefix op or the one on the left is a postfix
op, this being disambiguated in general by postfix having a -> marker.

* Note the following is valid Haskell syntax:
    r1 `compose` r2 = (r1 `join` r2) `remove` (r1 `meld` r2)
- the function in backticks is Haskell's standard infix apply

* Make Muldis_D::Low_Level provide multiple selectors for Set values where
some are shorthands/optimized to produce the kind of Set values that are
sets of Tuple eg for implementing Bag or Relation.  The Bag alternate
selectors will for example merge any tuples adding the counts rather than
just eliminating or keeping duplicates; they would also set a property so
that this can be lazy, eg when same() is run and deduplication must occur,
the flag will cause counts to add rather than eliminate.
Or just make Set and Bag disjoint Low_Level types that happen to share
nearly all implementation code, and that distinction is the flag, ok then.

* To keep the first Muldis D implementations much simpler, that is to give
us our 80% solution with 20% of the work, don't try and support any kind of
pseudo-variables.  This means all database updates are direct assignments
to the whole dbvar.  While somewhat awkward for updates, a lot of the power
in Muldis D is demonstratable with what we have left, everything you can
do in SQL SELECTs plus all the typical stuff of general purpose languages,
albeit with some degree of circumlocution, but most non-db variable
assignments are often done directly anyhow and not updating elements,
and that can be done with eg "ary := assign_elem(ary,index,v)" for now.
Or maybe just supporting array elems and tuple elems etc as subject to update
targets will go a long way for usability with little effort.

* NOTE: See latest Raku synopsis 11 and 22 as my concept of Raku long
names and package management is rather out of date, more there I should follow.
- See CompUnitRepo .
- Figure some reasonable code directory pattern to account for storing
multiple versions of the same base name, and also dealing with segments
while also allowing :: in module names to correspond to directory
boundaries.  There could be more than one pattern, see CompUnitRepo.
- Raku has distinct 'need','import','use' where 'use' is simply a shorthand
for the prior 2 in order.  A 'need' is like Perl's "use Foo ();" or
alternatively like a compile-time Perl 'require'; in Raku, 'require'
will also import symbols, at runtime.
- Also, "assuming" keyword is not about "currying" but rather "priming" or
"partial function application"; P6 spec changed in 2011 Oct to more correct
terminology.

* Add new type Quantity which is generic in the same manner that Interval
is generic.  It associates unit names with numbers.
TODO, FLESH THIS OUT PER DETAIL AT: See 2015 Jan 31 - Feb 1 TTM list discussion about it.
But "Subtypes of Capsule" literal syntax list further down this file also says a lot.
- A normalization, eg between cm-cubed and in-cubed, or W = A*V, is simply
defined as a Quantity that is multiplied by one to get the other, or such.

* See http://www.boost.org/doc/libs/1_57_0/doc/html/boost_units/Quick_Start.html .

* Make Muldis_D::Low_Level package API correspond exactly to what would be
hand-implemented in each host language.  API provides and uses just the
disjoint types {Boolean,Integer,Array,String,Set,Tuple,Capsule,Identifier}
plus Universal however Universal is treated as a union type over said and
its internal representation is hidden.  API provides no means to introspect
a Universal in terms of its own possrep, the latter being for now relegated
to a concept rather than something exposed in the language. All selectors
and routines in the API just have parameter or return types that are of
those same 8 types.  The API will include the fundamental operators of each
of the 8 types, at least the minimum needed in terms of which all others
can be defined, eg basic math ops.  The API will include the generic =
operator.  Each host language will have its own hand-written package
providing the same API.  The Muldis_D package will just be written in
Plain_Text and a parser/compiler hand-written in each host language will
translate it into equivalent host language code, same as with user code.
Also, for now, special features that would only be used for defining
Low_Level in Muldis D itself will be left out of the standard
Source_Code/etc types and out of the Plain_Text grammar, including Universal's
own literal.  Thus we avoid unnecessary work done only for theory.
Selecting Universal values not one of the 8 won't be supported.
Any Low_Level package written in Muldis D will just be illustrative.
Leave the name Low_Level rather than say Foundation/etc as its direct use
by users is not recommended; all of its API will have shims in Muldis_D.
The package Muldis_D::Low_Level actually supplied will have ... in the
appropriate places of all type and routine definitions, and nothing will
be synonyms of eg Muldis_D::Low_Level::_ something; the latter won't exist.
As such, the Muldis_D LL will be strictly be interface definitions.
As to the public/private separation of Muldis_D, that will probably remain,
where the public is just synonyms of some of the private.

* To help prove the above point, maybe skip actually trying to define and
compile Muldis_D at the start, and rather write some user-like test
programs that just use Low_Level directly, which users can do anyway.  As
such, Low_Level will need to include procedures like for STDIO as well ...
which, makes sense. Said programs would be more mathlike though maybe as
Text isn't in low level; or minimal Muldis_D could be defined that adds it?

* Maybe skip the parser initially too, and just test natively in Perl too,
as if the host-native Low_Level API were just providing a Perl package for
Perl programs.

* All the concrete Source_Code types should be defined just in terms of
Low_Level types; in particular, none should be defined in terms of
Relation, generally Capsule and Array should do it for collection types
as far as what the canonical Muldis D code is composed of, what Plain_Text
parses into, what the compiler needs to generate Perl/etc.
- Source_Code will not define system catalog relvars, those are elsewhere.

* Add expression node types/etc which let one add hidden meta-data to
values or variables or whatever, particularly for performance hints or
indexing strategies etc.  These are expressly write-only from the point of
view of Muldis D programs, or at least they are within
expressions/functions, so that this can't be used to change the semantics
of functional/pure code, as no logical decisions can be made based on
meta-data read in-situ.  The only places code can read meta-data is in
procedures.  Said hidden meta-data is also often useful for debugging and
the like, not just performance.  Maybe good to also have expr nodes that
can cause side-effects outside the logical flow of the current transaction,
such as message sending to other processes or autonomous transactions.
Still write-only, reading can only be done by procedural code.
The initial point in this bullet is a better alternative to adding traits
to type definitions or whatever about indexing strategies.
Having this may also reduce the need to swap out Set and Dictionary.

* How about make Dictionary a Low_Level type and demote Set to being
Capsule type defined over said.  A Set is a Dict where the keys are the set
elements and the values are always the value False (or alternately always
True); a Bag is a Dict where the values are always positive integers.  This
change brings uniformity to Low_Level types in practice such that all
collection types are essentially a map of unique-key to
possibly-nonunique-value pairs, where the key is either a dense sequential
integer (String,Array) or a non-dense String (Tuple,Capsule) or a non-dense
Universal (Dictionary).

* Actually, Set is going back to the Low_Level and Dict is being demoted,
the latter had problems with semantics, eg what to do if one adds multiple
elements to a Dict at once where there are duplicate keys with distinct
values.  So Set is the Low_Level, Relation is a Capsule over it, and both
Bag plus Dict are Capsule over Relation, or in parallel to it.
That's not to say that Low_Level can't expose useful functions for Set that
are specific to certain Set of Tuple maybe?

* Maybe rename Low_Level to Muldis_D::Foundation/etc to reflect that maybe
it isn't actually a problem after all for people to use it directly, though
those uses should still be limited, and the longer name than plain Muldis_D
suggests this.

* Any non-lexical entity may be referenced or invoked directly by code that
knows what its fully-qualified name is and refers to it explicitly using a
Identity_Identifier literal, even if the invoking code didn't explicitly "use" the
package declaring said entity.  But declaring package "use" must be done
for any non-Identity_Identifier Identifier to resolve.

* Everything the standard Muldis D parser knows about must be expressible
just in terms of what Low_Level provides, meaning all the meta-operators
like ! and [] and >><< etc must be defined by Low_Level.  But types such as
Relation or Text etc don't have to be because the parser can take their
special syntax and just parse them into something like Capsule literals.
- As to the matter of, what does the standard parser know what auth+vnum of
Low_Level etc to use if the code being parsed didn't declare directly or
indirectly, well the parser module itself did "use" Low_Level itself, so
that does answer the question.
- Idea, make the package providing the Muldis D parser just "use" Low_Level
and not "Muldis_D" in general; ditto the Unicode-understanding flavor.

* Demote Relation from lower level to the level of {Float,Text,etc},
replace with generic Set at lower level.  Relation is now a Capsule whose
body attribute is a Set of Tuple.  Maybe add other Relation variants that
are also Capsule and are peer to it.
Example variants:
  - heading isn't stored, just emergent from body, just 1 empty relation value
  - stored heading just attr names (the current way)
  - stored heading includes declared types of attributes
  - things meant to emulate SQL now on more equal footing re implement level
ACTUALLY, just keep Muldis D's current paradigm (#2) as the one true
Relation, and simply refer to #1 as a set of tuples, and keep the likes of
#3 out of the core, that considerably more complicated option can be the
domain of an extension library, especially as it would need its own tuple
etc types maybe?
- Maybe generalize the @ sigil to mean homogeneous collection, and apply it
to both arrays and relations and sets etc, having twigils as necessary.
- Canonically a Relation is over a Set of Tuple, and often done similarly
at implementation level as many languages have Set analogies; in particular
users aren't to think about this in terms of relations being arrays or
whatever.
- Indexing is implemented at the Set level, such that the data types of the
elements can declare routines that guide optimal indexing strategies, in
particular can define multiple indexes per type, mainly useful for indexing
sets of tuples, as indexes can be explicitly candidate keys or not.
An index function will take a value of the type as input and return an
ordered array of values that the Set implementation can then use as keys
in the index, analogous to key declarations in SQL.  Somehow, relation-level
operators can also ask the system to create particular indexes on things
maybe, eg the columns about to be joined, or something.  Or not.
Something like want_index() or whatever.  Indexes are somewhat first class
or the system would be savvy enough with the Set type that they could be
properly done both at the memory or disk level.  But generally the lower
implementations don't have to know about relations specifically, this would
be generalized away, so eg SQL table emulations would be same level or such.
- Set ops would include replace-element or such, some generalization of
replacing or updating a tuple, particularly for implementing a bag, eg
updating the count for an existing thing.

* Note that BS12's UNION is Tropashko's (inner) union.
BS12's UNION was actually what became UNION CORRESPONDING in SQL about a
decade later, meaning union of projections of inputs over their common attrs.

* DO FIRST:
- Split Low_Level into namespaces as appropriate and add the minimal sufficient
operators specific to each of the 11 or so low-level types that a maximal
amount the rest of the 'public' types and operators for eg integers or
relations or whatever, can be defined by the Muldis_D core without their
having to explicitly deal with the underlying List or Natural representations,
and so later Low_Level could, particularly in implementations, be swapped
out just by itself for some other implementations, but the language core
proper would continue to work unchanged.
- Probably represent a lot of internals using Array wrappers.
- Most Natural math ops might be ejected / moved into Integer, maybe.
- Some selection ops might be tricky if they have to do canon list sorting
eg for relation values to meet relation constraints; "put_in_order" etc.
- Make Low_Level code higher level where doing so doesn't add dependencies
on types not declared therein; in particular, add versions of not(), and(),
or() etc to Low_Level so the code can be easier to understand without
excessive runaround from being forced to just use ??!! for purity.

* ACTUALLY:
- New rool - any literals for a type may be used in package declaring said
type or in packages using said, but I can still be selective; eg, Array
lits and Integer and Boolean lits can be used in lowest level package.
- Split Low_Level into 2 modules, otherwise more formally nesting things.
- New lowest level has just ordered or lowest common denominator types:
Universal, Boolean, Natural/Integer, Array, String, Set, Tuple, Capsule, Identifier.
- We eliminate "Structure"; see a TODO point below for identifying priors.
- Above the new lowest level we have stuff that doesn't know anything about preceding
types' internals and are generally defined just in terms of the 8 types
{Boolean,Integer,Array,String,Set,Tuple,Capsule,Identifier}.
- Maybe actually give a name to the complementary subset of Universal that
isn't one of the above 8, such as "SC_Abnormal" say.
- Most everything else is a Capsule subtype.
- New higher low level declares: Text, Blob, Rational, Float, Relation,
Heading, Renaming, Stream, External, and basically everything else that has
its own special literal
syntax or would typically be available system-defined on common prog langs.
- Core or each layer does only interface directly with adjacent layer.
- Maybe rename "Low_Level" to something else, eg Sub_Level, Foundation, etc.
- Consider stratifying everything and turning plain "Muldis_D" into nothing
but synonym declarations.  As such, no internal implementation details
including nested functions are exposed in the namespace.
- Maybe have Muldis_D::Internals::* packages for everything underneath.
- Or we could have a convention that for any package Foo, a separate
package Foo::_/::* optionally may exist that has internals for it, like a
separated public and private namespace, analogous to .h and .c, but that
the public is a subset of the private depending on the latter, rather than
the reverse.  So Muldis_D::_::*.  Mneumonic is both "topic" and "private".
- Where routines have multiple aliases, convention saves them all for the
public interface, and in private there is just one name each;
aliases for parameters is a different matter.
- In any event, all generated names will either start with an underscore or
with a digit, the latter being illegal bareword identifier syntax.

* For a Universal topic:
    if topic has 0 elems then
        topic is Boolean.false
    else if topic has 1 elem then
        if topic[0] has 0 elems then
            topic is Boolean.true
        else topic[0] has >0 elems so
            topic is abnormal
    else if topic has 2 elems then
        if topic[0] has 0 elems then
            if topic[1] has 0 elems then
                topic is the Integer zero
            else if topic[1] has >0 elems that all have 0 elems then
                topic is a positive Integer, the count of topic[1] elems
            else topic is abnormal
        else if topic[0] has 1 elem that has 0 elems then
            if topic[1] has 0 elems then
                topic is abnormal (negative zero is not supported)
            else topic[1] has >0 elems that all have 0 elems then
                topic is a negative Integer, the count of topic[1] elems
            else topic is abnormal
        else if topic[0] has 2 elems that all have 0 elems then
            topic is an Array and topic[1] is its elems
        else if topic[0] has 3 elems that all have 0 elems then
            if every element of topic[1] is a valid Integer then
                topic is a String and topic[1] is its elems
            else topic is abnormal
        else if topic[0] has 4 elems that all have 0 elems then
            if the elements of topic[1] are mutually low-level-sorted then
                topic is a Set and topic[1] is its elems
            else topic is abnormal
        else if topic[0] has 5 elems that all have 0 elems then
            if the elements of topic[1] are mutually low-level-sorted then
                if every element of topic[1] has 2 elems then
                    if the first elem of every elem of topic[1] is a valid String then
                        topic is a Tuple and topic[1] is its attrs, as name-asset pairs
                    else topic is abnormal
                else topic is abnormal
            else topic is abnormal
        else topic is abnormal
    else if topic has 3 elems then
        if topic[0] is a valid Identifier then
            if topic [1] is a valid Tuple then
                if topic[2] has 0 elems then
                    topic is a Capsule and topic[0..1] define its type and attrs
                else topic is abnormal
            else topic is abnormal
        else topic is abnormal
    else if topic has 4 elems then
        if a certain host of conditions on topic[0..3] are met then
            topic is an Identifier whose elements in order are:
                pkg_name_base - 0..N String elems
                pkg_name_ext - 0..N String elems
                rel_starts_n_lev_up - a positive Integer elem
                path_beneath_pkg - 0..N String elems
        else topic is abnormal
    else topic is abnormal

* A Collection is a set of Pair<key,asset>.
    - Array is homogeneous collection of Element: Ordinal+Universal
    - Bag is homogeneous collection of Member: Universal+Cardinal
    - Structure is heterogeneous collection of Field: Nominal+Universal

* Generally, paren delimiters are for structures/tuples/pairs/etc, brackets are
for arrays/strings/etc, braces for bags/sets/etc, relations are hybrid.

* MODIFY OF LOW LEVEL TYPES:
- A Structure field name is now always a String.  A named field has the
regular code-points/string in all elements as usual.  A positional field has
exactly 2 elements, the first being 0 and the second being any integer.  A
positional is equal to a named of 2 characters where the first character is
the ASCII NUL character, that is positional is a proper subset of named
that has an additional special syntax.
- In practice we shouldn't have to worry about name collisions as no
ordinary character based name would start with an ASCII NUL, and if we were
ever mapping structures from some programming language where the 2 were
truly disjoint and could otherwise collide, we can deal with such things as
some other translation over top.
- The positional/integer names are also sparse like the named.  We have the
sparseness to give consistent handling between the 2 kinds of field names,
in particular with regard to substitution or merging of Structure values.
- Any logic concerning infix/prefix/etc operators or associative/etc traits
still expects fields named \+[0,0] and \+[0,1] corresponding to before,
aka "\\c[0,0]" and "\\c[0,1]".

* This table shows the low level types, plus examples of literal syntax:

    Boolean       | just the keywords false and true or \?false or \?true
    Integer       | 42 or 0 or or -3 or \+'-3''50_897' or 0d39 or 0xDEADBEEF or 0o644 or 0b11001001
    Array         | [expr,...] or \~[expr,...] to disambiguate from procedure block
                  |   Also, [expr:multiplicity-expr,...] with or without \~ also works like with Bag, adding N same elements.
    Bag           | {member-expr:multiplicity-expr,...} or \+{member-expr,...} or \+{member-expr:multiplicity-expr,...}
                  |   Eg: {0:0} or {x:4,y:7} or \+{} or \+{x:4,y:7} or \+{x,x,x,x,y:7}
    Tuple         | (expr,...) or \%(expr,...) or (lit:expr,...) or (:lit,...) or (expr,...,lit:expr,...) etc
                  |    where lit is one of: nnint or alpha or "whatever" (what is "whatever" again?).
                  |   Eg: (name:'Joy',age:17)
                  |    Is a set of named attributes where each attribute name is
                  |    a string (isomorphic to Array of Integer).
                  |    - Tuple literal must contain a comma to differentiate
                  |    from generic grouping parenthesis and Capsule, unless it has the \% prefix;
                  |    eg, `(x,)` is a unary tuple while `(x)` is an expression with the value of `x`;
                  |    as an exception, `()` is always the niladic Tuple, and is invalid expr group.
                  |    - Signature is proper subtype of Tuple where the value for
                  |    every attribute is an Identity_Identifier;
                  |    this is used when defining a type in terms of a set
                  |    of typed attributes, or when defining a function
                  |    signature likewise, or when defining a set of Attr_Name;
                  |    a Identity_Identifier is naming the expected attribute contract (type)
                  |    or constant which is treated as a singleton type of its value.
                  |    - Heading is proper subtype of Tuple where
                  |    every attribute value is just false; it has special alt syntaxes
                  |    \@(lit,...) or \@(nnint-lit..nnint-lit).
                  |    The first syntax is list of attribute names, any kind but explicit;
                  |    the second syntax is numeric range for numeric attribute names, eg \@(0..3).
                  |    - Attr_Name is proper subtype of Heading
                  |    with degree of 1; it has special alt syntax of \foo or \@foo.
                  |    - Key_Asset_Pair is proper subtype of Tuple
                  |    with exactly 2 attributes named 'key' and 'asset';
                  |    When working element-wise with a collection, how each element represented.
    Capsule       | (label : attrs) or \:(label : attrs), eg (\Set : (members : \+{...},))
                  |    This is the parent of most types, analogous to an 'object', namespacing things.
                  |    Identity_Identifier is subtype like (\Identity_Identifier : \%(...)).
                  |    There are no Capsule-specific Core package ops on purpose so to not interfere with subtypes.
                  |    The label, conceptually a base type name,
                  |    is generally a Identity_Identifier like \@foo for most subtypes
                  |    but for main core types especially ones that Foundation
                  |    needs to know, it is an Attr_Name like \foo instead.
                  |    Contract_Name is an alias for Identity_Identifier, use for more descriptive parameter/etc types.
                  |    General OO systems can be emulated by optionally having
                  |    a Reference as or as part of a Capsule attrs, in the case
                  |    of "mutable" objects, or no Reference for immutable ones.
    OBSOLETE SYNTAX, FREE UP => FOR NON-CORE / USER TYPES:
                  |    expr=>expr - that is Identity_Identifier=>Tuple eg $Date=>(year:500,month:4,day:24)
                  |    and Singleton is one with nullary tuple eg $To_Zero=>()
                  |    and $=>lit is shorthand for Singleton value eg $=>To_Zero.
                  |    Normally a Capsule's Identity_Identifier is a type name, making
                  |    the Capsule a type, but structurally one is identical
                  |    to a partially applied function if the Identity_Identifier is to a function name.
    OBSOLETE; USE A CAPSULE SUBTYPE INSTEAD:
    AND TODO: MAKE \$foo mean what $foo used to mean, \\$foo what was \$foo.
    AND TODO: USE ~@ rather than $ meaning string of attribute name ... BUT WHAT OF THE PRIOR LINE?
    Identifier    | Several kinds, all mutually disjoint:
                  |    - identity : \$pkg-canon-name.litpath   eg \$Muldis_D:"https://muldis.com":"0".Integer.sum
                  |    - absolute : \$::loc-pkg-alias.litpath  eg \$::MD.Integer.sum
                  |    - relative : \$nn-int-expr.litpath      eg \$2.foo.bar or \$0
                  |    - floating : \$litpath                  eg \$Bag
                  |        UPDATED 4 kinds (TODO, HASH THIS OUT SOME MORE):
                  |        - identity : My_App::Foo_Lib;;auth;;vnum;;X::Y::Z
                  |            or My_App::Foo_Lib;;auth;;vnum
                  |            or ;;My_App::Foo_Lib
                  |        - absolute : ::X::Y::Z or ::X etc
                  |        - relative : 0:: or 2::foo::bar or 1:: or 1::_priv etc
                  |        - floating : Z:: or Y::Z etc
                  |    NOTE that "floating" can NOT search the private namespace
                  |    within a binder, it always stops resolving at a binder
                  |    itself (plus ::""); only "relative" (or ident,abs) can ref private.
                  |    For non-literals rtn call remove \$
                  |    eg "::pkg.foo.bar()" or "0()" or "foo.bar()".
                  |    Note that leading "0" means "self" / same routine,
                  |    and "0.foo()" is not allowed; leading "1" means
                  |    the binder that directly contains current routine
                  |    so a routine with nested routines would behind
                  |    scenes refer to each with "1.nest_rtn_name()"
                  |    but if invoker routine has no nested, then its
                  |    explicitly decl neighbors ref'd with "1.foo()".
                  |    Also special syntax "resolved expr" will
                  |    produce identity from all kinds of Identifier expr eg "resolved \$foo"
                  |    and $lit is shorthand for "resolved \$lit" eg $Bag short for "resolved \$Bag".
                  |    Note that any consecutive double-quoted tokens are treated as 1 token.
                  |    OBSOLETE: A Identity_Identifier is no longer a subtype of Identifier,
                  |    rather it contains one, see further below.
                  |    - Identity_Identifier is proper subtype has all "identity" values.
    Reference     | Note: Reference has no literal syntax in Plain_Text.
                  |    Selecting one uses a special syntax or keyword etc
                  |    with no argument, its internals are an
                  |    implementation-dependent concept of a memory address
                  |    or reference, and selecting one is non-deterministic.
    OBSOLETE; EXTERNAL HAS NO VALUE LITERAL SYNTAX IN Plain_Text.
    External      | \~external~'...' - the "..." are implementation-dependent
                  |    but typically is source code of some kind for
                  |    whatever the host environment is, eg Perl code
                  |    Note that any consecutive single-quoted tokens are treated as 1 token.

* Subtypes of Capsule:

    Bits        | \~?'0xA705E' or \~?'0o644' or \~?'0b00101110100010'
    Blob        | \~+'0xA705E416' or \~+'0b00101110_10001011'
                |    Note that any consecutive single-quoted tokens are treated as 1 token.
    Text        | 'Hello' or \~'Goodbye'
                |    Note that any consecutive single-quoted tokens are treated as 1 token.
    Internal_Comment | Something like \%(\Internal_Comment,'The comment text.')
    Fraction    | 3.14159 or 0.0 or -4.72 or 5/3
    OBSOLETE; SCALED HAS NO VALUE LITERAL SYNTAX IN Plain_Text.
    Scaled      | 4.720s3 or 4.72s3 or 4.720sv1000 or 4.72sv1000 or \+'4720//1000'
                |   or \+'472/100s3' or \+'472/100sv1000' or 4720//1000 (all are same value, last 1 actually expr)
                |   A fixed scale but unlimited precision rational number.
                |   Note the 's' means 'positions Scaled down' not 'significant figures'.
                |   Note the 'sv' means 'Scaled down by Value'.
                |   Note that <int>//X means <int>/SsvX; there is also a regular // operator.
                |   Everything is Integer math but with meta-data of a scale divisor >= 1.
                |   Two Scaled can not compare equal unless they have the same scale divisor.
                |   A generic Scaled is disjoint from Fraction but has the same components
                |   (numerator/denominator), it is still bignum math, but the denominators
                |   of 2 operands must already be equal, and they are not GCD.
                |   This is not about limited storage size, that's to other types.
                |   This is also not about 'significant figures', as outside add/subtract that breaks down.
                |   For all practical purposes a Scaled behaves like an Integer,
                |   its semantics are like Integer, succ() and pred() are defined
                |   for it like for Integer, that is, it is discrete rather than continuous,
                |   whole division etc is defined etc.
                |   We should have an abstract type defined that covers
                |   Integer and Scaled but not Fraction etc, maybe call it Discrete ... actually, Enumerable.
                |   Actually, Scaled is also unlike Integer and more like Quantity,
                |   in that while it is closed under +,-, it isn't under *,/ etc,
                |   at least assuming the Scaled is semantically like a Fraction.
                |   A Scaled in general is just partially ordered, unlike Integer or Fraction;
                |   It is only totally ordered within common scales.
                |   Lets make Scaled just a Quantity like N*(1/1000)^1 or N*(10)^-3;
                |   or Scaled is a Quantity with a single unit of type Integer (scale factor) and power of 1;
                |   but, is that what we want?
                |   CHANGES!  Given that it is likely we'd only really care
                |   about fixed-scale numbers in the context of units, for
                |   example currency or various such, and given the various
                |   possible semantics we might want eg disallow multiply
                |   versus what kinds of results to give, we shall demote
                |   Scaled to just be examples of Quantity in use; each scale factor becomes a unit.
    OBSOLETE; FLOAT HAS NO VALUE LITERAL SYNTAX IN Plain_Text.
    Float       | \+'29*10^30' or 4.5209p19
                |   Note that Perl uses 'p' rather than 'e' so it works with base-16.
                |   Note that 'p' here means 'power' not 'precision'.
                |   Actually, we gonna boot Float from core, since many ways to do it and no best pragmatic generic choice.
    Unit        | Unit(expr) or \%(\Unit,expr)
                |   A tagged value of any type.
                |   Or make this a subtype of Quantity.
    Quantity    | expr+expr or expr-expr or expr*expr or expr/expr or
                |    expr^posint-expr where at least one expr is a
                |    Unit or a Quantity eg 5.7*$Meter/($Second^2)
                |   Quantity has no Plain Text syntax; this is all routine calls.
                |   Subtype Unit is Quantity with single term like 1*Unit^1.
                |   Quantity is like certain special cases of symbolic math, esp if the unit is also a number.
    Interval    | expr..expr  or expr-..-expr etc
                |   Interval has no Plain Text syntax; this is all routine calls.
    Set         | {expr,...} or \?{expr,...} to disambiguate within recipe block or \?{member-expr:multiplicity-expr,...}
                |   Eg: {x,y,z} or \?{x,y,z} or \?{x:4,y:7,z}
    NOTE: Relation et al Tuple collections don't have Plain_Text LITERAL SYNTAX ANYMORE, FOLLOWING ARE ROUTINE CALLS.
    Relation    | ?%{(lit:expr,...),...} or ?%{(expr,...),...} etc
                |   The first syntax has stringy attr names, the second has numeric, mix allowed
                |   Eg: ?%@(name,age) or ?%{ (name:'Joy',age:17), (name:'Envy',age:23) }
                |   D0C0 is ?%@(), D0C1 is ?%{()}; ?%{} is invalid syntax (heading unknown)
    Tuple_Bag   | +%{(lit:expr,...):posint-expr,...} or +%\+{(expr,...),...} etc
                |   Tuple_Bag is to Relation what Bag is to Set; common heading, allows duplicates
                |   Eg: +%\+{ (name:'Joy',age:17), (name:'Envy',age:23), (name:'Joy',age:17) }
                |   or +%{ (name:'Joy',age:17) : 2, (name:'Envy',age:23) : 1 }
                |   D0C0 is +%@(), D0C1 is +%{()}; +%{} is invalid syntax (heading unknown)
    Tuple_Array | D0C0 is ~%@(), D0C1 is ~%[()]; ~%[] is invalid syntax (heading unknown)
                |   Tuple_Array is to Tuple_Bag what Array is to Bag; common heading, allows duplicates, tuples in order
                |   Syntax identical to Tuple_Bag but with square brackets instead of curlies, and ~ rather than +
                |   If the (...):N syntax is used, the N elements are placed consecutively in the array
                |   Eg: ~%[(lit:expr,...):posint-expr,...] or ~%[(expr,...),...] etc
                |   Eg: ~%[ (name:'Joy',age:17), (name:'Envy',age:23), (name:'Joy',age:17) ]
                |   or ~%[ (name:'Joy',age:17) : 2, (name:'Envy',age:23) : 1 ]
    SPECIAL SYNTAX FOR THIS LIKELY ALSO NO PURPOSE, COULD USE SOME PREFIX OP WITH A SET OF BINARY TUPLES
    Dictionary  | \{expr:expr,...} or something

    Renaming          | \@:(lit<-lit,...) or \@:(<-lit,...) or \@:(lit<-,...)
                      |   If one lit is absent, means implicit positional/numeric name.
                      |   Arrow also allowed to point the other way.

* Generally speaking, what any commalist is parsed into is starting with an
empty value of the collection type and insert()/insert_n()-ing each element.

* OBSOLETE AS OF 2016-09-19; ALSO, "args" IS ARG LIST NOW:
Make \<foo a syntactic shorthand for source attribute \foo when source is a
Tuple (EDIT: which is all the time); that is, \<foo is syntactic shorthand for:
    \%(\<*args,\foo) -->^ BS_Tuple_Attr
... which is longhand for \<*args.\foo
- Furthermore, make \<<foo shorthand for \0 attribute named \<foo;
that is, \<<foo is shorthand for \<*args.\0.\foo
mainly useful in list operators like where/map/any/etc when applied to
relations or tuple collections, where they then are attrs of the "current tuple".
- Furthermore, make \>>foo shorthand for \1 attribute named \<foo;
that is, \>>foo is shorthand for \<*args.\1.\foo,
while that doesn't pair with \< plus 1 layer, it does pair with the notion
of \<< drilling into binary op lhs arg, so \>> drills to binary op rhs arg.
- A related feature \<>foo refers to the procedure lexical variable or subject-to-update
parameter "foo" by itself and isn't short for anything per se, that is a
procedure doesn't have a "source" fundamentally.

* Don't have catalog types specific to defining routine parameters.
- Regular contracts / type defs / shorthands do the job.
- But see Signature type.
- Make any updateable vs not details of procedure parameters into a
separate declaration like 'requires' etc are separate.

* Consider using \~+[nnint-expr,...] for something like numerics or Blob
now that String no longer exists to use it.

* Note that $ is free to assign some mnemonic to.

* Math recognizes different kinds of infinities, eg countable or
non-countable, eg based on repeated add or multiply or exponentiate or
whatever double-up-arrow is etc, and each possibly deserves its own
infinity singletons.  But we probably want a generic infinity singleton
that doesn't care, for generic ranges etc ...
Ooh, ooooohhh ... lets make 'Infinite' an abstract type which is then
composed by the likes of those different infinities or +Inf or -Inf etc.
Or otherwise deal with infinities in a similar sense to units, as these are
units eg for use with Quantity in a matter of speaking.  But where is the
boundary with symbolic math?

* Regarding abstract types, virtual functions, 'composes', 'implements':
- Need to evaluate how useful these actually are.
- Something something about association being tied to specific names rather
than implicitly to all synonyms.

* See http://www.javaworld.com/article/2073649/core-java/why-extends-is-evil.html
as to why in principle we want to do more in terms of interfaces/abstracts.

* The key feature of abstract types is that operators defined over them
have different visibility status.
- Normally within a package, identifiers only ever resolve to things that
are explicitly either in that package or explicitly used directly by that
package, and so the internal behaviour of a package can never be changed at
a distance but when the package itself or any of its explicit dependencies
(recursively) are changed.  A polymorphic call will only consider options
within that context to dispatch to, and will ignore / not see any other
overloads of that operator in the program.
- But abstract types provide a lone exception.  If a package uses an
abstract type or an operator defined over said, then an invocation of such
an operator will see / consider overloads over the whole program.  Or to be
specific, if the normal more restricted resolution ends up picking a
virtual operator, then other operators over the program that claim to
'implements' said virtual will be considered; but if a non-virtual and
non-implements version is found the normal way which satisfies the argument
types, it will be chosen instead.
- To be more clear, the entities are subject to two-class rules; in effect,
2 namespace searches will be done; if a candidate visible under normal
rules is found which is not a virtual and satisfies the argument, it will
be chosen regardless of if a virtual also exists that satisfies; a virtual
will only be chosen, on a second effective pass, if no non-virtual matches.

* When a 'search' identifier-chain resolution is performed, the algorithm
is like this (ignoring checks for type or abstract vs not):
0. Set BASE to be the actual invoking entity, which is almost always a
routine but possibly might be something else; we will pretend it is
possible for an invoker to be a schema; go to step 1.
1. If the unqualified name of BASE matches the initial identifier-chain
element and the rest of the chain matches corresponding children of BASE,
we found a match, then stop, else go to step 2.
2. If BASE is a schema then search within its children in a breadth-first
manner, first considering every immediate / first-degree child of BASE per
step 1, then every second-degree child, onwards until either a match is
found (and we stop) or all descendants are checked, and then go to step 3.
Within each level, the candidates should be visited in a truly random /
non-repeatable order (so visiting order within siblings can't be relied on
for semantics between successive invocations, depending on said is a bug;
more generally, having multiple matches on the same type within found
candidates is possibly or often a bug itself).
3. If BASE has no parent then we can't keep searching; no match was found,
so stop, else go to step 4.
4. Set BASE to become the immediate parent of the current BASE, and then go
to step 2, but that we ignore / don't revisit the same child that
previously was BASE.
Amendments:
- The entirety of those 5 steps applies only if the invoker lives within a
search path, in which case the root of its search path is treated as being
the BASE with no parent per step 3.
- For search paths that don't include the invoker, step 0 instead defaults
BASE to being the root of the search path, and then only the descendent
search per step 2 applies.
- As a general rule of thumb, all 'private'/embedded entities should never
be invoked using 'search' syntax, but rather use relative (often this is
automatically generated anyway) so attempts to invoke some embedded thing
don't accidentally call something farther away due to search paths order.

* Study how MoarVM is implemented, including how it manages memory, as
informative on how Muldis D implementations can do it.

* https://6guts.wordpress.com/2016/04/15/heap-heap-hooray/
- Moar uses generational collection, based on objects being allocated in 2
pools, and assuming that most objects are very short lived.
- Moar uses memory write barrier.
- Moar uses a boxed integer cache.
- Moar can save and restore heap snapshots, useful for debugging.
- File locks and unlocks should be placed on the same code-path rather
than be far apart in the code.
- "And, for all of you out there using Rakudo’s Lock class: first, don’t,
and second, if you must use it, always prefer the $lock.protect({ ... })
form over manual lock/unlock method calls."

* https://6guts.wordpress.com/2016/04/21/framing-the-problem/
- A call frame is also known as an invocation record.

* https://6guts.wordpress.com/2016/04/30/refactoring-and-torture/

* https://6guts.wordpress.com/2016/07/23/assorted-fixes/
It turns out that a number of implementation details I figured out myself
to do for implementing memory sharing or performance help for immutable
Muldis D structure types were independently done by developers of MoarVM.
- "MoarVM is somewhat lazy about a number of string operations. If you ask
it to concatenate two simple strings, it will produce a string consisting
of a strand list, with two strands pointing to the two strings. Similarly,
a substring operation will produce a string with one strand and an offset
into the original, and a repetition (using the x operator) will just
produce a string with one strand pointing to the original string and having
a repetition count. Note that it doesn’t currently go so far as allowing
trees of strand strings, but it’s enough to prevent a bunch of copying – or
at least delay it until a bunch of it can be done together and more cheaply."

* The Muldis D compiler/runtime/etc written in Muldis D (eg, particularly
to make cross-compiling easier or reduce manual code per language) should
generally be procedural in nature.  It should use one or more nested array
variables to represent the main memory data structures, basically mimicking
what would be done in Perl with its Memory objects and such.
- HERE'S A KEY POINT: To avoid recursive implementation issues, there would
actually be 2 instances of Muldis D running, where the outer one is
implementing a virtual machine in which the inner one is running, and
values do not directly cross between them at all.  That is to say, a "value"
that lives inside the VM is represented by a DIFFERENT "value" (or variable)
outside the VM.  The wrapper on the outside contains extra stuff such as
indexes or whatever that are not semantically part of the inner "value" but
are used in implementing it.  The outside instance of Muldis D is what would
typically get substituted for by Perl or whatever.
- Outer instance would use a minimum of types, in particular no relations.
- Outer analogous to miniperl or NQP or something.
- Outer natively just ASCII source or something.
- Outer possibly slimmed syntax, eg, PT_STD_MINI.
- Things and stuff.

* Create sibling project called "Muldis D Lite" (MDL) which is essentially
to Muldis D (MD) what Not Quite Perl (NQP) is to Raku.
- MDL is officially disjoint from MD but also corresponds directly, such
that all syntax in MDL is also valid MD syntax, with the same effective
semantics, but that formally each is declared into separate package and
syntax namespaces.
- For example, using `Muldis_D_Lite:Plain_Text:"https://muldis.com":0` rather
than leading with `Muldis_D`.  The different first element implies
different language and not just different syntax for the same language.
- MDL is intended to be useful by itself but it is also as minimalist as
reasonably possible while still meeting that goal.  The main differences
from MD are those things which are a lot tricker or more verbose to
implement and mainly provide convenient niceties that programmers expect to
have or that may aid making larger programs.
- The intent is to build MDL first, its reference implementation being
bootstrapped in Perl and otherwise self-defined and able to compile itself,
to Perl and NQP and various other things.  Then the reference
implementation of the full MD would be built next and be bootstrapped in
MDL and otherwise be self-defined and able to compile itself, though
initially compiling itself may just compile to MDL.  To be clear, one would
write a Muldis D VM/compiler/etc in MDL as if they were writing it in say
Perl, so for example what the Muldis D inside the machine sees as value X
would not be the same value X to the code outside/implementing the VM, but
rather some value Y which represents X but has extra meta-data such as
indexes or whatever, so one can have multiple host values Y which on the
host side are not the same value but to the language in the VM they are.
- Formally, MDL is treated as a foreign language by MD, so invoking MDL
modules in MD modules would be done like invoking Perl or a host language;
at the same time, MDL can be made native in the same way as SQL or other
languages, albeit trivially.
- Definitions of some corresponding features would differ, eg on traits
for Integer or whatever, and particularly the catalog types would differ.
- Parts:
    - Muldis_D_Lite
    - Muldis_D_Lite::Primitives
        - Logically declares {Boolean,Integer,Array,Structure} over
        Universal, so rest of MDL code can just assume these things exist
        as primitives and would make no attempt to divine internals.
    - Muldis_D_Lite::Memory
    - Muldis_D_Lite::Types
        - Declares likes of String, Tuple, Capsule, etc and a bunch of
        catalog types as needed to represent MDL code.
        - Or maybe Tuple not included, and perhaps we use Structure instead
        of Capsule.
    - Muldis_D_Lite::Storage
    - Muldis_D_Lite::Compiler
    - Muldis_D_Lite::Parser
- Features included:
    - 7-bit ASCII
    - non-abstract types
    - non-virtual functions
    - hierarchical namespace
    - absolute and relative identifiers
    - polymorphism in the sense that operators may take Universal/etc
    - positional collections / types / arguments
    - invoking routines with `args --> foo`
    - non-streaming parser / regular-size code files / small db dumps
    - non-embedded / side-by-side defs of eg routines and types
    - higher-order routines (by name-passing only or that is "-->name")
    - Universal, Boolean, Integer, Array, Structure
    - String, Heading, Renaming, Tuple, Identifier, Capsule, Stream
    - Blob, Text
    - type-specific sorting and ordering, for Integer, minimal other types
    - minimal operators for included types
    - minimal selector syntax for included types
    - generic file I/O as byte streams or memory byte mapping
    - in memory databases
    - single concurrent users of in memory databases
    - trivial expressions / arrays on left hand of assignment
- Features NOT included:
    - non-ASCII
    - abstract types
    - virtual functions
    - 'search' identifiers
    - polymorphism in the sense of operator overloading
    - meta-operators
    - named collections
    - named arguments
    - infix/etc notation eg `arg foo arg` or `foo arg` or `foo(arg)`
    - streaming parser / parsing very large files / typical/large db dumps
    - embedded defs of eg routines and types
    - generic sorting and ordering and intervals
    - most / trans-minimal operators for MDL types
    - most traits such as associative or whatever
    - most alternative selector syntax for included types
    - persisting databases / tied variables
    - multiple concurrent users of in memory databases / memory merging
    - non-trivial assignment left-hand expressions / views in general
- Features could go either way:
    - named collections / types / arguments
    - Relation
    - Set, Bag
    - type-specific intervals for eg Integer, minimal other types
    - some traits, maybe

* ACTUALLY, Muldis D Lite is not necessarily a pure subset of Muldis D; for
core/main language operations it probably is, but in some cases there might
expressly be a break.  []  A more common example is the hierarchy or
existence of the catalog types in general, eg, the same string syntax may
not parse into the same catalog types.

* Make first Muldis D Lite implementation in Pure Perl.  If I have to write
any of Muldis D Lite in MDL itself, the language is too big.
- Any MDL impl in MDL would be a separate second implementation later,
which would really just be a cross-compiler.
- In contrast, Muldis D proper though implemented in MDL fundamentally
would also have a full self-definition in Muldis D.
- At each abstraction level, a level should really conceptually just be
about translating code from one layer to another.  Conceptually, a MDL
implementation is really just a compiler for MDL to some other language
(eg Perl) and also conceptually the VM it runs in is also provided by said
target language (eg Perl).  Likewise, a Muldis D implementation is really
just a compiler for Muldis D to either MDL or Perl or whatever.  This might
be muddied though by implementing one MD version in another MD version.

* See this: http://www.unicode.org/reports/tr39/#Restriction_Level_Detection .

* If possible add functions corresponding to same_base_type() and
same_low_level_type() that are unary and just return some value that
uniquely identifies the type, a generalization of returning the name of the
type; perhaps it would be the default value of the type;
for example, which_base_type(42) returns 0 and which_base_type('hello')
returns '' etc; which_low_level_type() is the same but that for all
Capsule it just returns the default Capsule value.
- Idea is that for misc Universal types, every one of which is treated as
its own low level singleton type, we just return the same value.
- However, this idea falls apart if there are multiple independently
defined Capsule types that have the same Capsule_type, so which to choose
in order to determine the default value?
- So maybe then what we actually want to do is just hardcode in the new
which function its own idea of a consistent default value that probably
may not even be a valid default value in the normal sense, and so:
  - For each Capsule, just return the value Capsule_type(topic)=>\%{}.
    - Above was for which_base_type(); for which_low_..., default Capsule.
  - For all normal other Universal, return their default value.
  - For all SC_Abnormal just return the argument given to which().
- Maybe also have a which_types() that returns a list of Identity_Identifier for all
known types that the given value is a member of, if practical;
running this might be slow, though implementations can try maintaining a
type graph that may help this and other things.

* BEFORE DOING ANY OTHER LARGE THING, CREATE A GIT BRANCH THAT I JUST DO A
PILE OF CHANGES IN TOGETHER, AND THEN SEPARATELY I CAN MANUALLY MERGE BITS
OF THAT TO TRUNK WHEN READY TO DO A SERIES OF MORE ORDERLY RELEAESES.
- I SUPPOSE PROBABLY BEST TO DO THE BRANCH ON GITHUB'S END?  RESEARCH!
- I KNOW I WANT TO SEE THE BRANCH AND TRUNK SAME TIME BOTH THERE AND HERE.

* It must be mandatory for each grouping () paren pair in an expression to
have exactly one sub-expression which is not explicitly named with ::= because that
is the root sub-expression of the grouping () ... or alternately we change
the definition of ["" ::=] to simply indicate which sub-expression in a
grouping () is its root; but then we actually should have a keyword,
and maybe "returns" (nee "<--") could be that keyword ... the keyword would be optional
and the simple absense of leading ::= from one sub-expr marks it as main;
but if all sub-exprs are named then the keyword is mandatory.

* Eliminate uses of |,&,- to mean union/or,intersection/and,minus with
respect to type defs or {schema object, param/attr} alias declarations.
- NOTE, NEWER VERSIONS OF SOME OF THE FOLLOWING POINTS ARE FURTHER BELOW.
- We should discourage aliasing even if supporting it, and the best way is
to not provide special/terse syntax for doing so that isn't generalizable.
- For schema object aliasing, people can just do this:
    sum ::= add ::= "+" ::= function ...;
... assuming all of said aliases are okay to have identical signatures/etc;
actually, better just make them say
    bar ::= function ...;
    foo ::= synonym of bar;
- In practice, explicit union/intersect/minus types probably aren't that
common except for say enum declarations, so just support those with the
likes of:
    Int_or_Text ::= Universal where (.0 isa $Integer or .0 isa $Text);
- Provide appropriate generic expression syntax / functions such that one
can use them effectively for declaring routine signatures/etc without those
needing special syntax.
- As such, the general form of the declaration of function foo would be
in terms of 5 mutually isolated (having their own expression node
namespaces) generic expression trees, any of which or parts thereof may
optionally be factored out into (the bodies of) other functions;
within each tree, "_" is a keyword / the default name of the (locally)
singleton expression node representing the topical input value of the
expression aka its sole parameter; these are those 5 trees:
    1. A e(Boolean<--Universal) that declares the (usually Tuple) dispatch
    type of foo's input parameter; that is, a call to "foo(x)" will cause
    this particular foo to be dispatched to if True<--e(x) assuming also
    this particular foo is the first so-matching one found during dispatch.
    2. A e(Universal<--Universal) that normalizes foo's input parameter
    from whatever conceptual multiple parameter aliases may exist to one.
    3. A e(Boolean<--Universal) that declares any further constraints on
    the input parameter, eg that denominator may not be zero, when we want
    to explicitly fail certain function calls rather than the dispatcher
    simply falling back to no-such-routine when you call say divide(4,0);
    a result of False means fail and True means accept.
    4. A e(Universal<--Universal) that declares the main body of the
    function, that which does the real work the function exists for.
    5. A e(Boolean<--Universal) that declares foo's result type; for a
    function this is the other part of foo's main signature besides #1.
- A general procedure declaration would follow a similar pattern, except
that there wouldn't be a #5 so to speak and inverse expressions may be
necessary (ideally generated) for #2 etc considering the common practice
of subject-to-update parameters; procedures need more thought.
- Generic system-defined functions such as Tuple_follows_template() can
provide conveniently short syntax for signature declarations, including
whether each attr is mandatory or optional, what aliases an attr may
have, whether the attr list is exhaustive or a subset, whether attrs have
positional-alike/integer names, etc.
- Generic system-defined functions for testing multiple "isa" on the same
value can be given, eg "v is_any_of {$Int,$Text}", which also works for
declaring enumerated types, or "v is_all_of {$Odd,$HundredPlus}" etc.
- Remove ":" from function declaration as the line between function head
and body is more fuzzy, but have some terse way of marking each part of the
function.  Also make bounding parens optional for the 5 expression trees so
eg you don't have to say "(5)" when "5" would do; the bounding parens would
just be needed when it isn't otherwise clear where the expression ends and
the next part of the function declaration or next schema object declaration
begins.  These should all work equally well when a function declaration is
embedded within another value expression, such as for the meat of generic
relational restrictions.  Perhaps something like this:
    foo ::= function --> (_ isa $Integer) <-- (_ follows ...) : (#_);
    foo ::= function to (_ isa $Integer) from (_ follows ...) via (#_);
... noting that the 5 parts may come in any order and are all optional.
Clearly this will take a lot of thought.
- See Raku design for guidance, it already does a lot of that stuff.
- Have function shorthand eg "tup?.attr" that returns a VoidLike value
rather than dying if the tuple doesn't have an attribute of that name.
- Maybe go further and make "?" a parser-recognized meta-operator, similar
to how "!" is; so while "bar !x foo" means "not(bar x foo)",
"bar ?. foo" means "has_attr_name(bar,$foo) ?? bar.foo !! :$Void" or some such.
- UPDATE - not/! META OP NO LONGER EXISTS; NOW :=foo IS ONLY META-OP.
- So then defaulting an optional parameter can be like "_?.mynum // 3".
- Have shorthand to choose, among multiple possible attribute names, to
determine under which name, if any, the attribute exists, take its value;
it would also assert no more than one of the attr names exists at once.
- We can also say that technically the only actual required part of a
function definition is #4 the main body expression.

* Add keyword "type" that is like "function" but just takes a body def expr
where that body is a predicate, and implicitly is Universal-->Boolean.
- Actually, it would take a binding def too in the general form, like this:
    type of Foo where ...
    function from Foo to Boolean via ...
- So "of" means "from" and "where" means "via".
- The "type" also takes "default" expr, optional only if inherited.

* Have keyword "constant" that is like function but just takes a body def
expr and explicitly has no input.  As all "function" must have exactly 1
argument, they can not be used to define constants or singletons properly,
so we have "constant" to fill in a critical gap; it looks like this:
    constant expr
    constant 3
    constant foo::(6)
    constant ...
- Practially speaking, the concept of embedding constants in an expression
tree would only ever make a named expression node and not also a schema
object, unlike say when embedding a function or type.
- The body expr of a singleton may still be empty/... though if it is
expected the implementation will fill it in as a special case.
- The implementation will likely fold any constant declarations at compile.
- A type 'default' is a constant and a function 'identity' is too.

* Invocation differences (`...` must include some non-whitespace):
    foo::() - calls a 'constant' only
        foo :: () - likewise
        ACTUALLY, FOLLOWING 3 OBSOLETE AS () IS A TUPLE LITERAL NOW,
        SO THESE THEN TURN INTO 'FIXED' UNARY FUNCTION CALLS.
        foo() - likewise
        foo () - likewise
        foo( ) - likewise
    foo::(...) - calls a 'function' only
        foo :: (...) - likewise
    foo - either an expr node name or a pre/post/infix style 'function'

* Note that foo(...) in an expression is no longer special and means the same as `foo (...)`.

* Lets say :: is mandatory for procedure calls also, maybe?

* Change how routines work so that the actual sole input argument may be a
Universal rather than having to be a Tuple.  Also create a new core type
that represents the actual argument which has both an Array and a Tuple
component, which is the canonical way to represent effective argument lists
that have either or both ordered and named arguments.  So we then eliminate
the kludge where ordered parameters are represented by Tuple attributes
named "0" and "1" etc, which is partly bad due to being base-10-centric.
Along with that new type is new special syntax for selecting a value of it,
where that syntax resembles the existing complexities of an argument list.
Then alter the general syntax of a routine call to be more like that of a
Capsule value selector ("=>"), in that it takes the appearance of an infix
operator, eg "<-" (like "." does), like this:
  rtn <- args
eg:
  $div <- :( x, y )
where that particular example also has the shorthand:
  div(x,y)
or:
  x div y
Within a routine, the sole parameter is named "_" and where the new special
type is used, the value composes both ArrayLike and TupleLike, so you can
access the actual parameters like this:
  _.[0]
or
  _.foo
... or as ".[0]" or ".foo" respectively.
There would also be normal operators for merging a Tuple+Array into one of
the new types, or extracting the whole Tuple or Array component.
Obviously the default value of the new type is an empty array+empty tuple.
The "rtn" input to "<-" could also be an anonymous function declaration,
which in its general form is just a body-defining expression; but in that
case, for legibility, we may want to offer "->" as well so one can say:
  args -> rtn
Actually, we may want the "args -> rtn" form to be the "normal" general
form, assuming it is mainly used with anonymous routines, and that use with
named routines is uncommon, and then simply not support "<-" at all.
We will likely replace "v->op(...)" with some other syntax then or drop it.
We will want to use Raku for guidance on how this type/etc works.
Or use "args --> rtn" and leave "v->op(...)"?  But what is more common?
In Raku ":(...)" creates a Signature while "\(...)" makes a Capture.
* SC_Func_Args will compose both the TupleLike and ArrayLike interfaces,
that is, one can say foo.bar or foo.[1] on SDFA which just goes to attr elems.

* Formally make all functions be unary Universal-->Universal and deal with
it re consequences on relationship to other languages or whatever.
- This gives Muldis D more theoretical purity and means that special types
for representing arguments or parameter decls don't have to be low-level.
- But if all functions are unary, how to represent nullary avoiding
recursion eg of the definition of its placeholder argument?
Maybe one way is to say a function may be invoked without an explicit
argument and in that case it is the empty LL_List that the function gets.
- All Low_Level.foo will explicitly not use the new input argument type
that combines Array+Tuple etc, only non-Low_Level foo would.
Instead they would explicitly use LL_List of particular arity for any
functions that conceptually take multiple arguments, and just the argument
directly when just one.  As such, all invocations any LL_foo would by
necessity use the general form of function calling eg args-->func
which would be another way to discourage direct use of LL_foo
in general code.
- All non-Low_Level.foo explicitly will use the special args type.
- The new args type definitely will be a Capsule, a SC_foo to be specific.
- All invocation syntax like "x op y" or "op x" or "op(x,y)" or "op(...)"
will always mean use the new args type, and so they'll only be for non-LL.
- All predicate functions intended to be used in the position of defining a
type must be bare Universal-->Boolean; that is, the type of the "topic" is
NOT a SC_Func_Args in the general case.
- Similarly, all predicate functions used in defining routine headings,
that is dispatch type, input normalizer, input constraint, result type,
for these type type of "topic" is also NOT a SC_Func_Args in the general.
- Consider for general predicates as well eg for relational "where".
- Consider making it "normal" for routines to often take Array or Tuple
rather than SC_Func_Args ... or don't ... but the fact that SC_Func_Args
can "do" both Array and Tuple could mean such practice could be transparent.
- So, Universal."=" will have to be changed to be a wrapper of LL_same
rather than a synonym of it, since the former will use a SC_Func_Args while
the latter will not.  Some other similar splits would happen as approp.

* Make backslash "\" only mean escaping within delimited strings; outside
said it has no mnemonic and can be used as generally as any symbolic chars.
However some special syntax starts with a \, eg relation literals, which
will override in the case of ambiguity.

* Also, make it so function decl type is essentially just a tuple of
traits, where body and params decl etc are just traits same as
is-commutative or default or whatever.  Every trait is a key-value pair
with the key being the trait name and the value either being a boolean or
some other type, typically an expression node set.

* Let "isa" have prefix shorthand so "isa $foo" means "topic isa $foo".

* Ruby has 'assert' and 'refute' keywords, also 'alias'.

* First task - we need to build a list of expression and stmt node types.
- SC_Topic - represents the real function input argument
- SC_Resolved - represents the "resolved" keyword; turns Ident to Ref
- SC_Inlined - represents the Identity_Identifier name of an entity that was
    declared inline with the expr, typically a function (or type, which is
    a function); this name is generated unless function def preceded with bind
- SC_Routine_Name -
- SC_ISA - represents "isa" keyword - OBSOLETE
- SC_Default_Of - represents "default_of" keyword - OBSOLETE
- SC_Meta_Op_Not - represents "!foo" meta-op - OBSOLETE
- SC_Meta_Op_Reduce - represents "[foo]" meta-op - OBSOLETE
- SC_Meta_Op_Assign - represents ":=foo" meta-op
- SC_Lit_Universal - represents "\*[...]" literal syntax for Universal
    values; also has "\*:[...]" variant that asserts exactly 2 elements
- SC_Foundation_Singleton - represents "\*^1[...]" special syntax for
    foundation singletons, each of which may exist exactly once in the
    codebase, and since they're all used once each in Low_Level.mdpt, means
    they are otherwise non-invokable by users; possible singletons are:
    - \*^1[maximal_type] - what Universal/LL_List is defined in terms of
    - \*^1[constant_empty] - what constant LL_empty is defined in terms of
    - \*^1[function_prepend,expr,expr] - function LL_prepend
    - \*^1[function_is_empty,expr]     - function LL_is_empty
    - \*^1[function_first,expr]        - function LL_first
    - \*^1[function_nonfirst,expr]     - function LL_nonfirst
    Note that only the first term in the list defines each singleton.
    Due to these, exactly zero of all system-defined constants or functions
    have "..." bodies, other than to mean TODO.
- ...

* NEW FOR 20160628...
- Any entity allowed to contain referrals to possibly-non-lexical things
can introspect its own source code by way of a number of special
context-specific constants etc.
- The expr node \<*source yields a Source_Context Tuple with info on the
host entity as it is known at declaration/interp/compilation time; it has
the following attrs, many of which are optional:
    MINIMUM SUBSET TO GET EVERYTHING:
    - expr_name - an Attr_Name
        - The expression node name E previously mentioned, if there is one.
    - stmt_name - an Attr_Name
        - The statement node name S for the prior, if there is one.
    - trait_name - an Attr_Name
        - Which trait the trait_code T is from.
    - nested_material_code - a Material (eg Function)
        - The entire anonymous innermost material containing the \<*source
        iff it is a lambda; whereas, if \<*source is contained directly in
        the only/outermost material, then there is no nested_material_code.
        - No \@foo can refer to this because they must always be expandable
        to names, which this doesn't have.
    - material_name - an Attr_Name
        - The unqualified externally-visible name of M,
        that would be its "private" name inside its containing binder.
        - This is what \@0 refers to.
    - binder_path - an "absolute" Identifier, isomorphic to an Array of Attr_Name
        - The fully-qualified "public" name for B within its package.
        - This is what \@1 refers to.
        - The first element of the path is the package-local alias of the
        package containing the \<*source, per the definition of an "absolute"
        identifier, until that changes.
    - package_code - a Package
        - The entire package containing the \<*source.  See its "identity"
        attribute to identity-qualify any contained entities.
    REDUNDANCY THAT COULD BE DONE IN FUNCTIONS:
    - expr_code - an Expr_Node
        - The entire anonymous expression node tree E the \<*source is a leaf
        of, starting at its most distant ancestor.  Said ancestor typically
        is the asset for some attr name of a Tuple that defines all the
        named expression nodes in a routine/etc, or otherwise it may be an
        argument for a procedure call.
    - stmt_code - a Stmt_Node
        - The entire anonymous statement tree S the \<*source is in etc.
    - trait_code - ?
        - The set of named expression nodes and/or statement nodes
        comprising the innermost material trait T containing the \<*source;
        it could be any of these things: constant "value", function
        "requires" or "intends" or "via", selection "where" or "default",
        procedure "requires" or "intends" or "performs"; this could
        potentially be another trait such as "from" perhaps.
    - material_code - a Material (eg Function)
        - The entire only or outermost material M containing the \<*source.
    - binder_code - a Binder (or Folder?)
        - The entire binder B containing the \<*source.
    - material_path - an "absolute" Identifier
        - The fully-qualified "private" name for M within its package
        (combines binder_path and material_name).
- The expr node \<*inner produces a Identity_Identifier to the innermost material
containing it; it is analagous to \0::() but that its referent will be a
Material rather than an Identifier if \<*inner is inside a lambda.  This
can instead be done longhand using \<*source if one wants to.
- A Identity_Identifier value has these attributes:
    - referent - either an Identifier or a Material or a Package
        - The thing that should be invoked/called.
        - If this is a Package, then its "entry" is what we invoke.
        - If this is a Material, it must be a Function or Procedure or
        Constant or Selection; it can't be an Interface or an Alias.
    - referrer - a Tuple with attributes drawn from a Source_Context
        - The thing that requested the invocation/call to occur; use this
        to put a context on "referent" so eg paths can be resolved.
        - Defaults to \<*source in many situations.
        - ACTUALLY, these are the attrs referrer should have, all the time:
            - package_identity
            - binder_path
            - material_name
    - args - a Tuple
        - Arguments to pass to referent when invoking it, if applicable.
- A Function_Call is sort of a Identity_Identifier alias, it uses args.  Also,
Function_Call_* are further subtypes of that.
- A Contract_Name is sort of a Identity_Identifier alias, it doesn't use args.

* NEW FOR 20160809...
- Unified Plain_Text syntax for invoking Foundation functions as with
package functions, eg the same --> and \--> and f::() and \f::() etc.
- Likewise, Function_Call needs to be able to represent both.
- So Foundation things can have names that are data, and calls can be
partially applied, etc.
- A Foundation item name is represented just by an Attr_Name, that is,
like a single-level Identifier or so.
- We will formally give Foundation types names, maybe, or alternately we
rename the unary isa funcs to eg FDN__Integer, and maybe we'll overload
them to accept 0 arguments too, ... or not, handling defaults would change
the result type and do we want that?
- Idea, f1::^() means Foundation where f1::() doesn't.

* NEW FOR 20160527...
- Unified syntax in general expression context for invoking or referencing
functions and constants and types, all of which are invoked as if they were
functions, which in an abstract sense they are.  An invoked name will match
a package entity regardless of its kind, and then invocation will succeed
or fail depending on whether the argument list matches or not.  A constant
matches a nullary arg list only.  A type matches both a nullary arg list
(returns default value) and a unary positional arg list (test of membership).
A function matches any arg list at all potentially.
- To yield result of evaluate now any of these when the "function" name is hard-coded and so
are the arguments, its like this; there must be both "::" within the name
somewhere, and parens immediately following the name (opt ws):
    f1::() - const or default or func
    f2::(x) - membership or func
    f3::(x,y) - only func
- A limited form of the call-it-now for either 1 or 2 positional args only:
    f2 x
    f2 <- x
    x -> f2
    x f3 y
    x -> f3 <- y
- To yield result that is a specification of a function call, specifically
a value of the Function_Call type (a Identity_Identifier-Tuple pair), which includes
the name of a "function" and a (possibly either empty or total) subset of
the arguments it expects, just prefix a backslash:
    \f4::()
    \f5::(x)
    \f6::(x,y)
- The above is the preferred form of the "function" argument to
where/map/all/reduce/etc, though for such things the partially applied
arguments, the x and y above, should be named args instead since we want to
reserve "0" and maybe "1" for the "current element" provided by "where"/etc;
or otherwise one has to do the remaining positionals like this:
    \a::(1:x,2:y)
- To yield result that is spec of func call where function name or args are
supplied indirectly / via named exprs etc, this form:
    fc \<-- args
    args \--> fc
- The "fc" here is a Function_Call value and "args" is a Tuple value;
strictly speaking, what you are doing here is optionally applying further
arguments to the function call.  While this seems to setup a recursive
definition, how a normal user breaks out is to use the \f::() syntax the
first time to yield the initial "fc", whatever the context it is defined.
Further application is just extending the tuple in the "fc" with the "args" one;
replacing is not allowed, so if any attr names duplicate, it is an error.
- At the lower level for code implementing the Plain_Text parser, here is
longhand for selecting the initial fc:
    \Function_Call wrap (func: \@funcname::, args: ())
- The \@foo syntax selects a Identity_Identifier not wrapped in a Function_Call.
- Contract_Name is an alias for Identity_Identifier to help parameter self-documenting.
- Subtypes by constraint of Function_Call:
    Function_Call_But_0 - any Function_Call lacking a "0" argument
    Function_Call_But_0_1 - any Function_Call lacking both "0" and "1" args
- To yield result of evaluate now where function name or args are supplied
indirectly / via named exprs etc, this form:
    fc <-- args
    args --> fc
- The "fc" here is a Function_Call value and "args" is a Tuple value; as
such this is also how you take a specification of a function call value and actually
perform the call, optionally with extra arguments (use null Tuple if no more).
- Calls to Foundation functions/constants (types are not distinct there)
are still dealing with Tuple arg-lists to hard-coded names, like this:
    fdnfuncname ^<-- args
    args -->^ fdnfuncname
... but the ^ is subject to be replaced by something else.
- Embedded function definitions now result in a Function_Call rather than in
a Identity_Identifier; eg this results in a Function_Call:
    (function : (args.\0 + (args.\1)))
    (function : (args --> "+"::()))

* Function call syntax:
    args --> rtn
    rtn <-- args
    \%(x,y) --> \@"+"
        # some example Haskell looks similar (and Raku uses -> in 'for')
        \x y -> concat ["(",x,"+",y,")"]
    "+"(x,y)
    "+" <-- \%(x,y)
    x->"+"(y)  - THIS ONE IS OBSOLETE; x->y NOW A MONADIC POSTFIX MARKER
        # New use is, for example "arg ->op" for example "list ->group".
    x + y
    div(x,y,round_meth:\@=>To_Zero)
    \%(x,y) --> (function to Integer from (Integer,Integer) via ...)
    \%(x,y) --> (function --> Integer <-- (Integer,Integer) : ...)
    42 --> \@some_func_taking_Integer_directly_without_Tuple_wrapper
- Note that "\@0" can appear anywhere "rtn" does, and means call-myself
as long as it isn't quoted.

* Note that "rtn()" with nothing in the parens now explicitly calls a
"constant" and not a "function"; you only get a "function" call if
something is in the parens.
- If you want the above treated like a function call you instead say like:
    \@foo <-- \%()
    \%() --> \@foo
    OBSOLETE since functions now must have non-nullary Tuple topics.
- How you call a constant in both forms:
    foo()
    (() --> \@foo)
    (\@foo <-- ())

* Foundation function call syntax (only option):
    arg_expr -->^ opnamelitbarealphastr
    opnamelitbarealphastr ^<-- arg_expr
eg:
    3 -->^ Integer_opposite
    Integer_opposite ^<-- 3
- This is in contrast with a regular generic function call:
    arg_expr --> op_expr
- Mnemonic of ^ / up-pointer for raise/bootstrap remains.
- The parser can tell them apart as easily as it does naming_expr vs aliasing_expr.
- As usual, whitespace around the -->^ is optional.
- Foundation constant call syntax (only option):
    (() -->^ opnamelit)
    (opnamelit ^<-- ())
eg:
    (() -->^ FDN__default_External)
    (FDN__default_External ^<-- ())

* In terms of regular function calls, a foundation function call is one that
always has a -->/from/signature of Universal, that is it is like a
multi-dispatch that binds to all types, and then it has a 'requires' which
asserts the actual allowed inputs, such as must be 2 ints.

* Support symbolic constant calls with not "foo"() syntax.  A bare symbolic
is considered a constant (niladic function) call under many of the
conditions when it appears where a bareword token that would be interpreted
as an expression or variable name would appear.  Examples demonstrated here
with ∅ being the symbolic constant.
- Sole item in brackets:
    (∅)
    [∅]
    {∅}
- Whole item in commalist:
    x, y, ∅, z
    foo : ∅, baz: quux
- On tail-end of a routine precedence arrow (arrows point towards all functions):
    foo<-∅
    ∅->foo
    ++ <- ∅
    ∅ -> ++
- Right hand side of another symbolic with no precedence arrows:
    ++ ∅
    foo + ∅
- Right hand of a binding or assignment is no because confuses with
function call / hyper-op probably.
- Things that would benefit include: ⊤, ⊥, -∞, +∞, ∅.

* Inline function declare syntax:
    expr_node_name_containing_Ref_of_func_name ::= function ...
    function ...
So both of these result in expr node whose value is like one said \@foo
but the former the user decides what it is and otherwise its generated.
Note that all expressions of inner function have their own namespace and
are completely independent of the expression containing the function decl.

* NOTE THAT INLINE DECLARATION OF A FUNCTION/ETC THE NAME OF SAID WILL DUAL
AS A LOCAL EXPR NODE NAME AND ALSO A SCHEMA OBJECT NAME, IN THE PRIVATE
SCHEMA CREATED FROM THE PARENT FUNCTION BY VIRTUAL OF AN INLINE EXISTING,
TRUE WHETHER GIVEN EXPLICIT NAME WITH ::= OR NOT.

* Function declare syntax general form (same as inline actually?):
    0. keyword "function" followed by 0 or 1 of each of next in any order:
    1. keyword "virtual" is optional, not followed by anything of its own
        - if used, the keywords "via"/":" must be omitted
    1.5 keyword "commutes" is optional, followed by a function name
        - if used, replaces all keywords except {function,implements,overrides}
        - is for dyadic positional functions that just commute others, eg <= vs => or in vs has
        - these 2 declarations are equivalent:
            foo ::= function commutes bar;
            foo ::= function --> Any <-- (Any,Any) : (args.\1 bar args.\0);
        - a function may not name itself with "commutes" and must instead
        say "is commutative" when that applies
        - it is expected that any functions in a "commutes" pair are not
        associative or idempotent etc
        - Note that Haskell has "flip" programming pattern that does this.
    1.6 keyword "negates" is optional, followed by a function name
        - if used, replaces all keywords except {function,implements,overrides,is,identity}
        - is for functions that just negate others, eg != vs = or < vs >=
        - these 2 declarations are equivalent:
            foo ::= function negates bar;
            foo ::= function --> Boolean <-- Any : (args --> \@bar ?? False !! True);
        - it is assumed that function pairs in "negates" relationship
        may be different re associativity etc so each declares its own "is"
    2. keyword "returns" (nee "to" or "-->") is optional, followed by the name of a type
        - 'to' and 'from' support all of and exactly the same options
    3. keyword "matches" (nee "from" or "<--" or "signature") is optional, followed by the name of a type
        - the definition of the type may appear in place of the name using
        the syntax "selection ..."
        - a simplified form of a SC_Func_Params literal may appear in place
        of the name using the syntax "(...)" where the "..." is a commalist
        of 0..N tokens each of which is either the name/def of a type or a
        lit:type pair; they declare positional/named params respectively;
        these are basically equivalent:
            function <-- (Integer,foo:Integer) : ...;
            function <-- type of SC_Func_Args
                where (args.\0 matching \%($Integer,foo:$Integer)) : ...;
        - a brace-delimited list of 0..N type names / type defs /
        simplified literals, each like previously described; this is useful
        for defining a list of alternatives, eg {(foo), (foo, foo)} means
        the function takes both unary and binary arguments, eg for "-" op
        - the above list options are recursive; anywhere a type name may
        appear either of the alternative forms (...) or {...} may appear,
        allowing for example inline abbreviated Tuple argument definition
        or alternatives within a Tuple argument
        - NEW RESTRICTION: "from"/"<--" now must have a Tuple
        as its actual argument, so we don't have to deal with less common
        edge cases in the system; "to"/etc is still unrestricted
        - if omitted, the unnamed system maximal type is implicitly used
        - EXCEPTION for "from"/"<--": if omitted, the FDN__Tuple type is implicitly used
    4. keyword "implements" is optional,
        followed by set of 0..N associated function names
    5. keyword "overrides" is optional,
        followed by set of 0..N associated function names
        - while referant of implements must be virtual, overrides must not
        be virtual; they are essentially the same but we're being explicit
        as to what's going on and users can avoid easy mistakes
    6. OBSOLETE: keyword "normalize" followed by expr
        - defaults to source if omitted
    7. keyword "accepts" (nee "requires") is optional, followed by predicate expr
        - defaults to true if omitted
        - tests the argument over and above it being the type named in "from"
    7.1 keyword "intends" is optional, followed by predicate expr
        - defaults to true if omitted
        - tests the result over and above it being the type named in "to"
    8. OBSOLETE: keyword "is" is optional, followed by set of trait name keywords in
        braces: "associative", "commutative", "idempotent"
        and other terms like bijective etc may be added
        - when any of {a,c,i} are used, 'from' must be a Tuple type with 2
        positional arguments {0,1} of the same type with no other options allowed
        and of the same type as 'to'
        - use in the context of a reduction over a collection type requires
        at least "associative" to be set, and if the collection type is
        unordered, then also "commutative" must be set
        - OR RATHER, "associative" is needed in order for the operation
        to be parallelizable; no "associative" means, at least naively,
        it can only be serial, and it must also be an ordered collection type
    8.1 keyword "is_associative" is optional, see above
    8.2 keyword "is_commutative" is optional, see above
    8.3 keyword "is_idempotent" is optional, see above
    9. keyword "identity" is optional (when associative), has associated expr
    9.5 keyword "repeater" is optional, followed by a function name
        - may only be used when associative true and idempotent false
        - the named function must have a 'from' which is a Tuple type with 2
        positional arguments {0,1} where 0 is the same type as both of those
        of the current function and the type of 1 must be (a supertype of) Integer_NN
            - if the named function is virtual, then an applicable
            implementing function must exist with said integer argument
        - this tells an optimizer that it may call the other function instead
        of the current one were its {0,1} arguments to be the same value,
        such as when doing a reduction over a collection with repeating members
        - for example 'add' may indicate 'multiply' or the latter 'exponentiate'
        or catenate may indicate replicate etc
    10. keyword "map" (nee "via" or ":") is optional, followed by expr
        - defaults to source if omitted
        - this is the main body of the function F1; it takes the function's
        actual argument (as source) which conforms to "from"+"requires" and
        produces the function's result which conforms to "to"+"intends"
    11. OBSOLETE: keyword "inverse" or "-:" is optional, followed by expr
        - has no default / doesn't exist if omitted
        - this is the main body of the canonical inverse function F2 of the
        main function F1 but existing specifically to support pseudo-variables
        - an invocation of F1 may only be used within an expression denoting
        a pseudo-variable (left-hand side of an assignment, or argument to
        a subject-to-update parameter) if F2 is defined for it
        - a few system-defined operators such as "." have "inverse" defined
        for them, but the majority would need to be wrapped in user-defined
        functions in order to have an appropriate inverse behaviour for the
        use cases at hand, eg this is how "database views" would often work
        - the "source" of F2 is a tuple with these attributes:
            - arg_old - the pre-assignment value of the actual argument of the F1 call
            - pvar_old - the pre-assignment value of the pseudo-variable that is the F1 call
            - pvar_new - the post-assignment value of the pseudo-variable that is the F1 call
        - the "result" of is "arg_new", the intended post-assignment value of the actual argument of the F1 call
        - it is to be decided whether some syntax would be provided for invoking
        F2 directly as if it were a regular function, eg such as by implementing
        "inverse" by producing a nested function
            - its not like there's a way to directly invoke the expression
            in "default" or "identity" etc either, unless those become nested constants?
        - meanwhile, for invoking a given F2 for foo simply, run this statement:
            vars {old_source : FooType};
            foo::(old_source) := new_result;
        ... and then the variable old_source will have the result of running F2
        - when "inverse" defined, there is implicitly another constraint that
        runs after F2 executes and produces "arg_new" (and "from"+"requires"
        constraints have passed); it will run F1 again
        with arg_new and check its result matches pvar_new; if they don't
        match an exception will be thrown;
        in other words we rigorously enforce "The Assignment Principle"
        - inverse should work for any surjective (or bijective) function
        - the fact we have this constraint means we might be able to have
        some looser inverse functions which may not necessarily always
        work, eg letting instances that work through and blocking those that don't,
        as might happen with certain insert/remove/etc through joins
    ...
- Note that "symmetric" is obsolete here; it meant what "commutative" does.
- Every part has a default value that is used if omitted
- Many of the numbered items above correspond to Raku sub traits
- INVERSE IS OBSOLETE, THE REST NOT:
Note that declarations of a/c/i/identity/repeater etc could potentially
be tested automatically when a routine is called, eg run it twice with args
swapped and check the result, however if any of this is provided it likely
would be an optional feature to enable like a warning; however, given the
complexity of doing "inverse" right, it would stand to be reasonable that
this run-forwards constraint is just something that is always done, and in
particular this is more likely something that would normally be an actual
regular database constraint like type checks or key checks etc and that
making it possible to turn this check off would be bad.
- OBSOLETE: It may be interesting for any system-defined stuff conceivably used like
a view would have a system-defined inverse defined.  Example is UTF-8
encode/decode, or Plain_Text parse/generate, or different information schema,
or emulators of other DBMSs, etc; some may only work with/without normalization constraints.

* Let's explicitly say that any assignment to a variable, whether regular
or pseudo-, of the same value it already has, is a no-op in every regard;
it will NOT cause any inverse functions to fire, or any triggers to fire,
etc.  This should be a significant optimization.  Also prevents an edge
case where one might try to assign a pseudo-var to itself to "normalize"
things at the source end.
- This being said, applicable triggers should explicitly be named
"when updating x" rather than "when assigning to x", or have both if we want
to easily log the fact an assign was attempted even if it no-oped after.

* One option, update virtual routines to be more explicit, say, on when
composers MUST have certain traits, but also be able to declare that some
composers MIGHT have certain traits.  Basically encode some things that
otherwise are just mentioned in documentation for the virtuals.  For
example, that member_plus must be associative but may be commutative.
- Another option is to make virtual routines declare less or very little,
and simply have them being a shared name, and then each of their options
is tried to check for "signature" matches, rather than also first checking
whether the signature of the virtual matches.  This may be a less popular option.

* Constant / named value declare syntax general form:
    0. keyword "constant"
    1. keyword "folded" is optional, not followed by anything of its own
    2. keyword "composes" is optional, rules as on a "selection"
    3. keyword "value" is optional, followed by expr
        - defaults to (\@0) if omitted
- Note that unlike with functions, "virtual"/"implements" don't make sense
for constants because there's no way for them to be polymorphic dispatching
on a nonexistant argument, so all references to constants need to be
sufficiently name-qualified that the name is unique in the caller context.
- Iff "folded" is absent, the original value expression source code
is guaranteed to be preserved and introspectable in the same manner as
types and routines etc as if it were a niladic function.
- Iff "folded" is present, there is no such guarantee, and the
expression will officially be folded into a single value, so only the value
itself remains and how it was derived is not, aside from it being
maintained as a named constant that can be referenced.  Muldis D source
code produced from this value would be generated by the system, and is not
guaranteed to match the formatting of user-written source code.
- A folded constant is now the canonical way to represent the current value
of "the database" at any given point in time.
- TODO: Consider some other mechanism to declare that a set of routines or
constants for a virtual type should be composed by implementing types.

* Type declare syntax general form:
    0. keyword "selection" or "interface" followed by 0 or 1 of each of next in any order,
        but with some restrictions:
        - if "interface" used, the 3 keywords {of,where,default} must be omitted
        - until another type composes an interface type, that interface type
        is equivalent to the empty type and any attempt to have a variable
        of the interface type will fail at runtime in the same way
    2. OBSOLETE: keyword "singleton" is optional, not followed by anything of its own
        - if used, replaces all 3 keywords {of,where,default}
        - is for singletons that have their own Capsule type
        - these 2 declarations are equivalent:
            foo.bar.baz ::= type singleton;
            foo.bar.baz ::= type where !\%(topic) -->^isa_Capsule ?? false
                !! \%(topic) -->^Capsule_type = $1 and \%(topic) -->^Capsule_attrs = \%{}
                default $=>1;
    3. keyword "of" or "union" is optional, followed by set of 0..N
        type names in braces or exactly 1 not in braces
        - if omitted, the unnamed system maximal type is implicitly used,
        and so the new type consists of all values in the type system but
        where otherwise restricted by its explicit "where"
        - if exactly zero type names, the current type has zero values
    4. keyword "where" is optional, followed by predicate expr
        - if omitted, system treats it as if it were unconditionally true
        - while one could declare an empty type with a "where" of false,
        the canonical way to do it is with an empty set for "of"
    5. keyword "default" is optional, followed by expr
        - it is invalid for any used "default" to specify a value that "where" rejects
        - if omitted, system follows "of" chain until it finds a "default"
        and then uses that value; to be specific if the set of effective
        "default" for all immediate "of" types has exactly 1 member that
        is a member of the current type, then that is used, otherwise the
        current type must declare its own "default", except if the current
        type has no values at all, such as with the system minimal/empty type
    6. keyword "composes" is optional, is followed by set of 0..N
        names of abstract types
        - if omitted, semantics are equivalent to if an explicit empty set
        - when type A composes type B, the behaviour of the system when
        doing "isa" tests is as if B was defined with "B ::= type of {A,...}"
        but that the definition is done backwards
        - no need to rename "composes" to something else, meaning is approp
        - for each type name the keyword and_provides_its_default may
        appear after it, and if present, the composing type's default value
        is also used as the default value of that abstract type; only one
        composing type per common abstract type may declare this within
        some particular scope to be defined of each place where the
        abstract type is otherwise referenced, or it is an error;
        TODO, figure out that scope, but its probably per package
    7. keyword "requires_implements" is optional,
        it may only be used if keyword "interface" is used,
        followed by set of 0..N names of virtual routines whose signatures
        use the current type, and live in same package as the current type
        - if present, then for every type X which "composes" current one,
        the package declaring X must also declare routines which "implement"
        the named virtual routines and signatures have X in the place of
        the current abstract type
        - this is a means for interface types to declare a contract

* Enumeration (unordered) declare syntax general form:
    0. keyword "enumeration"
    1. keyword "of" or "union", followed by set of 1..N unqualified
        identifiers in braces
    2. keyword "default", followed by exactly 1 unqualified identifier
- An "enumeration" is not a material kind itself, rather it is an idiomatic
alternative syntax for declaring a set of 1..N "constant value (\@0)" plus
a "selection" that is just a union of those constants.
- When declaring a "Foo ::= enumeration" outside an explicit "binder" definition,
every one of the "constant" and "selection" will each get their own "binder"
housing just that material, and "Foo" is a folder containing those binders;
this is done so one can use "floating Foo" and have it behave as expected.
- In contrast, when declaring a "Foo ::= enumeration" inside an explicit
"binder", then each "constant" and the "selection" will simply become
materials private to that binder.

* New node syntaxes (OBSOLETE AS OF 2016-09-19):
    Ref function (and type) arguments iff source is a Tuple:
        - \<foo or \<0 means \<*args.\foo or \<*args.\0
            - but we could make these their own node type since why the hell not?
            - or merge that with node types for unpacking and packing tuples
            we need anyway due to chicken and egg though all we really need is pack
        - (:<foo,) in a tuple literal means (foo:\<foo,)
    Ref read-only procedure arguments:
        - \<foo or \<0 reference them directly, there is no source
        - (:<foo,) in a tuple literal means (foo:\<foo,)
    Ref subject-to-update procedure arguments AND locally declared lexical vars:
        - \&foo or \&0 reference them directly, there is no source
        - (:&foo,) in a tuple literal means (foo:\&foo,)
    Ref (lexical) named expressions in functions (and types) and procedures:
        - foo or "0" reference them directly
        - (:foo,) in a tuple literal means (foo:foo,)

TODO: SPLIT 'SIGNATURE' INTO 2 PARTS, ONE THAT IS A LIST OF 0..N SEPARATE
READ-ONLY IN PARAMETERS, THE OTHER THAT IS A LIST
OF 0..N SEPARATE VARIABLE IN+OUT PARAMETERS WHICH IS EACH ITS OWN BASE LEXICAL
ITEM INSIDE THE ROUTINE AND EACH OF WHICH MUST BE PASSED A SEPARATE VARIABLE
(NOT EXPRESSION) BY THE CALLER; SIMILARLY, 'VARS' NEEDS TO BE REIMAGINED AS
A SET OF 0..N SEPARATE VARIABLES AND NOT A SINGLE USUALLY TUPLE-TYPED ONE;
ALSO REVISE 'UPDATES' ETC AS APPROPRIATE.
* Procedure declare syntax general form (same as inline actually?):
    0. keyword "procedure" followed by 0 or 1 of each of next in any order:
    1. keyword "virtual" is optional, not followed by anything of its own
        - if used, the keywords "performs"/":" must be omitted
    2. keyword "matches" (nee "<-->" or "signature") is optional, followed by the name of a type
        - 'signature' supports all of and exactly the same options
        as 'signature' of a function
        - the prior point also means that if "signature" is omitted then
        callers may pass any value at all as its sole actual argument, so
        explicitly declaring a constant signature such as the empty Tuple
        will effectively declare there to be no conceptual arguments
        - this is implicitly a variable in the general case readable and
        writeable by both the invoked procedure and the invoking procedure;
        that is, what the called procedure sees is a pseudo-variable defined
        over some pseudo-/variable that the caller sees
    3. keyword "updates" is optional, followed by set of 0..N
        attr names in parens or just the keyword "source" not in parens
        - if omitted, the meaning is the same as if it had zero names
        - "updates" declares that a particular subset of the conceptual
        arguments, or the whole actual argument, or none, may be updated
        by the procedure; a statement invoking this procedure must pass
        pseudo-/variables where they are marked updateable, and it may pass
        non-variables otherwise
    4. keyword "implements" is optional,
        followed by set of 0..N associated procedure names
    5. keyword "overrides" is optional,
        followed by set of 0..N associated procedure names
        - see also "overrides" comment for function
    6. keyword "accepts" (nee "requires") is optional, followed by predicate expr
        - defaults to true if omitted
        - test of source beyond its signature type before procedure executes
    7. keyword "intends" is optional, followed by predicate expr
        - defaults to true if omitted
        - test of source beyond its signature type after procedure executes
        - normally would be omitted if "updates" is omitted
    8. keyword "vars" is optional,
        followed by set of 0..N variable declarations in braces,
        where each variable declaration has the form "varname : vartype",
        where varname is an attr name and vartype is a type name that
        can be written with all and any of the same options as "signature"
        - each declared variable can be referenced within "performs" with
        the same syntax as referencing a named expression, as an attr name,
        but unlike the latter it can be the target of assignments, etc
    9. keyword "performs" (nee ":") is optional, followed by stmt
        - defaults to no-op if omitted
    ...
- Every part has a default value that is used if omitted

* See https://doc.raku.org/type/Routine about Raku routine traits that
may be interesting to use.
- These could become annotations: DEPRECATED, hidden-from-backtrace.

* Schema objects can be easily indexed for fast(er) dispatch.  To be
specific, we can pre-make a map from each abbreviation (particularly the
fully-unqualified) object names to a list of the fully-qualified names that
said short one is short for, so an actual scan isn't needed per dispatch.
We rebuild/alter the map whenever a data definition action occurs.
Similarly, there is also a map for each fully-qualified schema object name
and the data types of its arguments, though that one might be lazily built.

* Perl 5.20+ supports a long-desired analogy to tuple projection.
The new C<%hash{...}> and C<%array[...]> syntax returns a list of key/value
(or index/value) pairs.  See L<perldata/"Key/Value Hash Slices">.
- While older @hash{...} can be used as lvalues, new feature can't, seems.

* Update ordering type per change to Raku (instead of [Inc|Dec]rease):
https://github.com/perl6/specs/commit/5e2c4205
"These operators compare their operands using numeric, string, or C<eqv>
semantics respectively, and if the left operand is smaller, the same,
or larger than the right operator, return respectively C<Order::Less>,
C<Order::Same>, or C<Order::More> (which numerify to -1, 0, or +1,
the customary values in most C-derived languages)."

* From http://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science)
it would seem Muldis D qualifies as "covariant" everywhere, such is the
nature of subtyping by constraint.
Or perhaps more accurately, every Muldis D function is covariant, as this
is more a property of routines than of types.
See https://msdn.microsoft.com/en-us/library/dd799517(v=vs.110).aspx also.
- Covariance
Enables you to use a more specific type than originally specified.
- Contravariance
Enables you to use a more generic (less derived) type than originally specified.
- Invariance
Means that you can use only the type originally specified; so an invariant generic type parameter is neither covariant nor contravariant.

* Have a look at the "active object" design pattern.

* Useful info at http://www.mathsisfun.com/ for examples:
- http://www.mathsisfun.com/sets/function.html
- http://www.mathsisfun.com/sets/injective-surjective-bijective.html
- http://www.mathsisfun.com/sets/function-inverse.html

* http://arstechnica.com/science/2014/05/scientific-computings-future-can-any-coding-language-top-a-1950s-behemoth/

* "idiomatic" - "Peculiar to or characteristic of a given language."

* See http://terrancalendar.com for something interesting.

* Useful:
http://www.itworld.com/slideshow/163234/head-scratchers-10-confounding-programming-language-features-434442
Example: Java will automatically convert primitives types to objects
(autoboxing), such as int to an Integer object. It will also, by default,
cache Integer objects for values from -128 and 127. This can lead to
unexpected behaviour when using == to compare autoboxed Integers with the
same value (TRUE from -128 and 127; FALSE otherwise).
Also number zero is TRUE in Ruby.
Also C/C++ have trigraphs like SQL does, say for people who don't have
curly braces on their keyboards.  They are processed before anything else,
as if they were part of the character encoding or such.

* From http://www.reddit.com/r/PostgreSQL/comments/26u0wk/why_is_postgresql_becoming_more_popular_than_mysql/
Some features missing in Oracle [that PostgreSQL has]:
- Range types
- Exclusion constraints
- True serializable
- PL/R
- Indexed KNN
- Transactional DDL
- Transaction level control of synchronous replication
- UNIX domain sockets with user based auth
- psql (the by far best SQL client I have used)

* Here's an article comparing how MVCC works in different DBMSs,
mainly focusing on PostgreSQL, Oracle, and SQL Server:
http://amitkapila16.blogspot.ca/2015/03/different-approaches-for-mvcc-used-in.html

* Actually, it seems transactional DDL support is much more widespread, and
hence MySQL is the main thing lacking it in modern days (InnoDB might have
supported it but the MySQL interface forces implicit commit on DDL),
although various DBMSs still have limitations.  See
https://wiki.postgresql.org/wiki/Transactional_DDL_in_PostgreSQL:_A_Competitive_Analysis
For PostgreSQL itself, you can't alter an ENUM in a transaction, but you
can basically do anything else (other than modify tablespaces).
http://www.postgresql.org/docs/9.4/static/sql-altertype.html
As of Oracle Database 11g Release 2, Oracle supports Edition-Based
Redefinition, which basically gives the feature, and otherwise performing
DDL is an implicit commit to prior operations, like MySQL does.
Also supporting DDL in transactions: SQL Server (in read-committed
isolation, more limmited in higher isolation levels), Sybase Adaptive
Server, DB2 UDB, Informix, Firebird (Interbase).

* http://blog.2ndquadrant.com/progress-online-upgrade/ gives good
explanation on doing zero/etc-downtime DBMS upgrades, how the new logical
replication of Pg 9.4 helps, and how pg_upgrade works (dumps and reloads
just db schema, and copies over disk files for data as-is or something).

* http://blog.2ndquadrant.com/bdr-postgresql-present-future/ is about how
bi-directional asynchronous multi-master replication support for PostgreSQL
has been converted from a branch/extension over time into core piecemeal,
including {Background workers (9.3), Event Triggers (9.3), Replication
slots (9.4), Logical decoding (9.4), REPLICA IDENTITY (9.4)} and the merge
may be done by 9.6 perhaps.  See BDR project.  And also
http://blog.2ndquadrant.com/when-are-we-going-to-contribute-bdr-to-postgresql/
is an updated blog post like the above.

* http://blog.heapanalytics.com/postgresqls-powerful-new-join-type-lateral/
shows how Pg 9.3's LATERAL join is useful in practice, letting you do in
declarational SQL what you may have needed procedural code for before.

* http://michael.otacoo.com/postgresql-2/postgres-9-5-feature-highlight-pg-dump-snapshots/
seems to give a way for multiple independent connections etc to see the
same database state, where one sets up named snapshot others can refer to.

* http://blog.endpoint.com/2014/11/mysql-to-postgresql-migration-tips.html

* Postgres 9.3+ event triggers probably very useful for DBMS wrappers to
efficiently know when a schema has changed or not, and when their memoized
schema information are still valid or not.
http://www.postgresql.org/docs/9.3/static/event-triggers.html
Also as an extra way to help prevent unwanted schema changes.
http://www.postgresql.org/docs/9.3/static/sql-createeventtrigger.html

* Consider signing up with https://www.gittip.com or other such venues as a
way to collect miscellaneous revenue to support Muldis D / etc development.
Just start this after I make my first deliverable and so supporters have
something already in exchange.  Would treat as business income by way of
Muldis Data Systems.  Not sure if I can identify Canadians from the givers
but if I can I'd treat part of theirs as GST collected.
See also communities such as https://www.gittip.com/for/perl/ .
See http://www.dagolden.com/index.php/2325/why-i-finally-joined-gittip-and-why-you-should-too/
and other blog posts it links to.

* How to make a generic join-like operator that is like SQL's and not like
normal natural join.  SQL's is fundanentally a cartesion product and each
input relation has its own namespace in the output; in Muldis D we will
have to specify said namespace as data, it isn't just gleaned from the
expression or variable names in source code, we do this with a tuple.
  :%{artists:db.artists} xjoin :%{albums:db.albums}
Each tuple of the result has 2 tuple-typed attrs named {artists,albums}
and the values are the tuples of the source relations.
One can then use where() to define a generic join condition like SQL eg:
  artists_tup xjoin albums_tup where :(.0.albums.artist_id = .0.artists.id)
Or another signature of xjoin itself could take more than 2 args where a
third etc provides join conditions etc; but then xjoin(a,b,c) call needed.
To support this in an N-ary fashion, xjoin has 2 signatures (both marked
commutative) which are tup*rel and rel*rel restricted such that a tup must
be unary with a relation-type attr and any rel may be N-ary where every
attribute is tuple-typed, and thus everything should just work.
The output of xjoin is the same type as said rel input type.
In the standard scenario xjoin requires that the primary attr names of all
inputs are mutually distinct since the result is a disjoint union of them.
So this should work:
  [xjoin] {a, b, c, d}
or
  a xjoin b xjoin c xjoin d
This is all basically an alternate way of doing:
  (a wrap all as a) join (b wrap all as b) etc

* It should be worth pointing out explicitly that when I write SQL I
automatically keep wanting to put the WHERE clause earlier in an expression
than SQL allows it to go, eg:
  update x where y set z;
or:
  select ... from x where y join a using (b) ...
... and so on.  And in Muldis D such intuitive actions are actually valid.
So an example of how Muldis D makes common tasks easier than SQL,
especially ad-hoc queries.

* Apparently Raku doesn't have union types after all, eg Int|Str where a
type name could go.  Closest is "subset IntStr of Any where Int|Str".  See
http://www.nntp.perl.org/group/perl.perl6.compiler/2014/03/msg9483.html .
So | is always a junction.

* Note the following change in Perl 5.18.0; I should do the same in the STD
parser, that is treat all 11 of the same characters as white space such
that literal occurrences of said are treated as equivalent to a SPACE etc.
"When a regular expression pattern is compiled with /x, Perl treats 6
characters as white space to ignore, such as SPACE and TAB. However,
Unicode recommends 11 characters be treated thusly. We will conform with
this in a future Perl version. In the meantime, use of any of the missing
characters will raise a deprecation warning, unless turned off.
The five characters are: U+0085 NEXT LINE U+200E LEFT-TO-RIGHT MARK
U+200F RIGHT-TO-LEFT MARK U+2028 LINE SEPARATOR U+2029 PARAGRAPH SEPARATOR"

* Create another Git repository and release series or three that just
contain Muldis D code, these being analogous to Larry Wall's STD.pm / Raku
grammar written in Raku, and the Raku Prelude and standard libraries
written in Raku, and the Raku test suite written in Raku.
- Contained therein is all of the "official" muldis.com Muldis D packages,
just as Plain_Text text files, and the Muldis D compilers written in Plain_Text,
and the Muldis D official test suite written in Plain_Text.
- This release series is *not* expected to be released as an executable
product by itself; rather it is more of a data product whose files are then
copied into other projects that actually have the foundation or framing or
whatever Perl or whatever code so that the other projects are executables;
they include the as-data files to save themselves work and test themselves.
- Said other Perl/etc projects would synchronize themselves periodically
with the Plain_Text-only project for releases, but the latter has no Perl/etc
in it.
- Think of the Plain_Text-only product like the data files that the Unicode
consortium releases, and the latter being like Perl etc which bundles said
Unicode data files as effectively parts of the Perl/etc executables.
- Maybe the modules, compilers, and test suite will all live in one
repository, but those parts may be released as separate product distros.
- The repository can have lib/, bin/, and t/ dirs to group the above into.
- The above Plain_Text-only alone probably would *not* be distributed on CPAN.
- Even http://www.unicode.org/Public/UNIDATA/ has both spec-data+test fils.
- Maybe name this repository "Muldis-D-Standard"?
- This new repository doesn't have to be held back for any work in existing
repositories such as "Muldis-D" or whatever, and in fact would probably be
the development process leader for a little while.

* Create another Git repository and release series "Muldis-D-Ref-Perl" or
such that would have the Perl based reference implementation, and it is
bundled into this that "Muldis-D-Standard" would be distributed on CPAN,
and it is this that would actually run.  The "Muldis-D-Ref-Perl" would
include both a module for embedding in Perl programs ala DBI which succeeds
"Muldis-Rosetta" and also a script for compiling standalone Muldis D
programs.  Any or all of the Perl in this distro may be generated from
Plain_Text code in Muldis-D-Standard, or at least the script and shared parts
would be, but the public parts of the module might not be.
The bundled DBD-alike module may have the name "Muldis::D::Ref_Eng".
- Yet another repository, say "Muldis-D-Perl", can specify HD_Perl_STD
as "Muldis-D" soon wouldn't, and also specify a common Perl API for both
"Muldis::D::Ref_Eng" and other DBD analogies sharing the API to follow.

* PRIORITY LIST:
1.  Split off Perl stuff to separate distros/etc, chg language name fmts.
2.  Canonicalize low-level tp-sys so that simple "=" just works everywhere.
3.  Redefine all routines to have exactly 1 parameter, tuple-typed.
    - Have minimal other changes, including minimal syntax changes.
    - Syntax changes mainly re pass/def tup literally or not ala Raku.
4.  ...

* Add "is deprecated" flags to materials so that definers can say in
advance that something is going away in future versions before it actually
does, and users can be warned.  I think Raku or Rakudo has it for
subroutines and such.

* Add references to or adoptions of ISO/IEC 11404:2007(E) "Information
technology -- General-Purpose Datatypes (GPD)" which could be very useful.

* Add mention of http://rubydoc.info/github/blambeau/alf/master/Alf .

* See http://scale-out-blog.blogspot.ca/2014/02/why-arent-all-data-immutable.html .

* See http://docs.python.org/2/library/datatypes.html .  Also, Python uses
the type name Counter for what Muldis D calls Bag it seems.
Also a lot of Python and Raku type names are the same, suggesting influence.
Also a Python Dict key can be any immutable type {string,number,tuple,etc}.

* Apparently names like_this are called "snake case".
https://en.wikipedia.org/wiki/Snake_case .

* Apparently this is valid in Perl (trailing ::), and can be disambiguating:
    Package::->method();

* About MySQL: For a multiple-row insert, LAST_INSERT_ID() and
mysql_insert_id() actually return the AUTO_INCREMENT key from the first of
the inserted rows. This enables multiple-row inserts to be reproduced
correctly on other servers in a replication setup.

* Perl 5.19.x/20.x add native sub signatures.

* See https://github.com/wbraswell/rperl/ for something that might be
relevant to translating Perl code to Muldis D code.
Or I can support RPerl as a compilation target; faster subset of Perl.
See also http://perl11.org and stuff.

* See newish DBMS FoundationDB that's about widely-scalable ACID supporting
both relational and non-relational.  A prime DBMS candidate to target, may
possibly be easier due to its design to support than Postgres, or maybe
not; it is also closed source.  Supports Linux (main) + Mac and Win (dev).
See also https://foundationdb.com/blog/call-me-maybe-foundationdb-vs-jepsen
for some stuff on how it works, and also the separate party Jepsen.

* See also Google's Spanner and F1 which are much the same, but internal.

* I probably want to use LMDB http://symas.com/mdb/ for my second
implementation, or alternately support it as an option in the first/ref.
There is lots of precedents for doing this too, eg SQLite4, multi "nosql".

* Quoth Richard Hipp:
Creating an index is essentially the same thing as sorting the whole table
- sorting in index order.  So you are going to have to sort the whole table
three times, once for each index.  The time to do the sort dominates.  The
time needed to scan the original table in order to pull out the elements is
usually negligible compared to the sorting time.

* Use same node type to represent bounding parens as used for synonyms.
Eg `foo ::= bar ::= 3` the foo is a synonym/aliasing node.
But aliasing nodes can have generated node names same as other anon exprs
so eg `1 + ((2+3))` the outer paren is an aliasing/syn node.
Parens themselves are general metadata that can go on any kind of node,
eg, is this surrounded by explicit parens in the text source or not.
Whether explicit node names are quoted or not is also metadata.

* Postgres 9.3 has ddl event triggers so eg you can block accidental
drop table and such if you want with a trigger.

* The Oracle foreign data wrapper for Postgres is declared stable now;
see http://oracle-fdw.projects.pgfoundry.org .

* Postgres 9.4 supports this apparently:
coalesce(min(val) filter (where val > 0), 0)
http://www.depesz.com/2013/07/23/waiting-for-9-4-implement-the-filter-clause-for-aggregate-function-calls/

* Note http://rhaas.blogspot.ca/2014/03/vacuum-full-doesnt-mean-vacuum-but.html
that VACUUM FULL is actually quite different than VACUUM, and in particular,
plain VACUUM is needed to accomplish certain things; generally plain VACUUM
is what you want during regular maintenance, unless eg your relation is
mostly empty and you want to return the space to the operating system;
VACUUM FULL does not deal with the id wraparound as VACUUM does.
Pg 9.4 changes some of this, so FULL deals with id wraparound too, somewhat.
Also, in Pg prior to 9.0, FULL worked differently.

* Postgres 9.5 adds row-level security:
http://michael.otacoo.com/postgresql-2/postgres-9-5-feature-highlight-row-level-security/

* See http://blog.endpoint.com/2014/02/dbdpg-utf-8-perl-postgresql.html and
why we really should use DBD::Pg 3.0+ if we have non-ascii chars, it is much
smarter than 2.x, actually paying attention to client encoding.

* See http://blog.endpoint.com/2014/02/mysql-ascii-null-and-data-migration.html .

* See http://www.sqlite.org/lang_savepoint.html ; apparently SQL savepoints
are already a generalization of "transaction" that can be nested and don't
have to require a parent "transaction" to be active; the first "savepoint"
could take that role itself; this is a lot like my design plans actually.

* Things even SQLite can do (as can Postgres) but MySQL can not do:
- subject data definition to transactions
- savepoints
- WITH clause, including recursive
- EXCEPT clause

* Apparently Windows 8 extends the use of shared pages to allow any pages
that just happen to be identical to be shared. It periodically sweeps
through all the memory in the system, identifies pages that have the same
contents, makes them shared, and frees up some physical memory. If one
process should then try to modify the newly shared page, Windows uses COW.
http://arstechnica.com/information-technology/2012/10/better-on-the-inside-under-the-hood-of-windows-8/3/
So my plan to do similar in Muldis D impls (when = is invoked) does not
seem that unprecedented, though in my case its both memory+performance.

* Apparently Windows applications can have an "application manifest" which
Windows looks at to determine the behaviour of some APIs it provides, eg
Vista behaviour vs Windows 7.  See also
http://arstechnica.com/information-technology/2014/11/why-windows-10-isnt-version-6-any-more-and-why-it-will-probably-work/ .

* Apparently C# has a "where" construct useable like this (it is lazy):
    var q1 = q0.Where(MoreThanTwenty);
... note that MoreThanTwenty is the name of a predicate function.
Also it has a "select" that works like "map":
    q = xs.Select(x=>1/x);

* See http://queue.acm.org/detail.cfm?id=2611829 - The Curse of the
Excluded Middle - "Mostly functional" programming does not work.

* See also "Haxe" language/platform.

* See also Test::Database the likes of which I can also support.
Its HISTORY.  http://www.nntp.perl.org/group/perl.qa/2008/10/msg11645.html
Quoting Michael Schwern:
"There's plenty of modules which need a database, and they all have to be
configured differently and they're always a PITA when you first install and
each and every time they upgrade."

* See http://www.pgbarman.org about Pg disaster recovery tools and
http://thebuild.com/blog/2014/10/24/be-very-afraid-backup-and-disaster-planning-at-pgconf-eu/ .

* PostgreSQL's model about transaction ids, and that it has special {1..2}
reserved values may be useful in my design.
http://www.depesz.com/2013/12/06/what-does-fix-vacuums-tests-to-see-whether-it-can-update-relfrozenxid-really-mean/

* This is probably useful, concerning timestamps:
http://www.depesz.com/2014/04/04/how-to-deal-with-timestamps/

* http://www.craigkerstiens.com/2014/10/01/a-simple-guide-for-db-migrations/
What happens when you have a not null constraint on a table is it will
re-write the entire table. Under the cover Postgres is really just an
append only log. So when you update or delete data it’s really just writing
new data. This means when you add a column with a new value it has to write
a new record. If you do this requiring columns to not be null then you’re
re-writing your entire table.

* NEXT PRIORITY ...
Rename Muldis::D::Outdated::Dialect::Plain_Text to Muldis::D::Outdated::Plain_Text.
Rename Muldis::D::Outdated::Dialect::HDMD_[Raku|Perl]_STD to remove the ::Dialect.
Split off the Perl STDs from Muldis::D so they're no longer core
but they're still official, and distribute the HDMD_Perl_STD in the same
distro as the Perl Rosetta Engine API documentation.
(SEE ALSO THE TODO FOR MULDIS::ROSETTA FOR CONTEXT.)
Lets call that documentation distro Muldis::D::Perl.
Create it like how we created Muldis::D::Manual ... empty then transfer.
The meat module lib/Muldis/D/Outdated/HDMD_Perl_STD.pod is renamed as such so it
can also have its version reset to 0 and then the new distro can
advance versions on its own without needing to match or incorporate the
Muldis D versions which would become onerous.  But the language declaration
name would not change, except to add more vnums, eg to have:
[ 'Muldis_D', 'https://muldis.com', '0.149', ['HDMD_Perl_STD', '0'] ]
Add new directory mod lib/Muldis/D/Perl.pod like Manual.pod.
All of this likewise for the Raku versions.
THIS GIVES US MORE FLEXIBILITY TO NOT HAVE TO UPDATE THE PERL-STD SPECS ON
THE SAME SCHEDULE AS THE TEXT-STD/REST.

* SAME TIME AS ABOVE ...
Actually, take this opportunity to overhaul the concept of qualified
language names and such.  Promote "dialects" from being subservient to
language into being full languages of their own that are derived from
Muldis D.  That is, say that what was "Plain_Text" is now simply *the*
concrete syntax of the official "Muldis D" language, and that what was the
Perl dialects are now 2 full languages whose definitions are derived from
the standard text-based one, and have their own versioning.
- So, here are new examples of fully-qualified language declaration names:
    Muldis_D:Plain_Text:Unicode(9.0,UTF-8,canon):"https://muldis.com":0.149
    ['Muldis_D','HD_Perl_STD','https://muldis.com','0']
    ['Muldis_D','HD_Raku_STD','https://muldis.com','0']
- Actually, we are best to split things up so that Muldis D code typically
declares 2 main versions separately, which are the concrete syntax version
and the system catalog / core libraries version.  Also, the Muldis D spec
itself would start using different version numbers for different parts,
so the concrete syntax and system catalog or core libraries would be
differently versioned and would only increment when those parts change.
- The Muldis D core spec could use at least 3-4 version series:
    - The version of the documentation distribution: 0.149 is next.
    - The version of the Core (name subject to change) library, which
    determines the data types and routines intended for users to use.
    - The version of the system catalog schema, that is the version of the
    "real" Muldis D concrete syntax / data structures as seen by code.
    - The version of the Plain_Text concrete string syntax that users write in,
    which a parser needs to know but the system catalog doesn't.
    - All of the above would start at version 0 except the first item.
    - The second item Core is what is expected to change the most often,
    while the catalog or string syntax are expected to change less often.
    - The second/Core and third/Catalog might be stay merged into one item
    as they are kind of inter-dependent, but we may review minimizing Core.
- Example of Muldis D code declaration of language:
    Muldis_D:Plain_Text:Unicode({1..6.1},UTF-8,canon):"https://muldis.com":{0..42};
    pkg ::= package FooLib:"http://foo.com":0;
    MD ::= using Muldis_D:"https://muldis.com":0;
    Spatial ::= using Muldis_D::Spatial:"https://muldis.com":0;
    Pg ::= using DBMS::Postgres ... emulations of all Pg types/routines/sys-cat/etc
        ... this is defined as a wrapper over Muldis_D/etc, but in a MD
        impl over Postgres, these would likely be implemented natively
        and then Muldis_D::* is implemented as a wrapper over these
    dfoo ::= using DBMS::foo ... emulate other DBMS
    Perl ::= using lang::Perl ... emulate Perl
    lfoo ::= using lang::foo ... emulate foo
    vxyz ::= using VendorXYZ::FooLib:"http://vendorxyz.com":0;
    Lulz ::= using MyAppBar::Lulz:"http://mybusiness.com":0;
    searching [pkg,MD];
    ... then our own module/depot/value/etc content follows ...
- Note, "using" is the C# keyword for this.  Note, "System" is C# core lib.
- Note, "java.lang" is core Java package; it is precedent for "Muldis_D::*".
- Note, a Java "package" is like MD's package/module concept, in that it
can define multiple classes/interfaces etc as per my multiple types/etc.
- Having DBMS::* and lang::* etc makes it easier to build compatibility layers
as eg a SQL parser just has to parse into DBMS::foo stuff and that's it.
    - A DBMS::* package also defines alternatives to Tuple/Relation types
    to represent SQL ROW/TABLE/etc types; these most certainly are all
    Scalar types from Muldis D's perspective and loosely are Tuple/Relation
    wrappers but that extra meta-data is included such as a concept of
    attribute order that is built-in to each value, or of attributes being
    allowed to have non-distinct/anon names, or of having certain 3VL etc.
    - Probably make the "real" names as numbers, like SQL does, and the
    text names are then aliases.
    - Our letter case of package entity names matches the folding rules for
    that DBMS with unquoted identifiers; in Pg they're lower, others upper.
- We use "::" as separators in package/module/extension names because those
are more public namespaces, whereas "." is used for internal namespaces
that subdivide each individual package/etc.  By doing this, if package Foo
exists with internal namespace Bar and routine baz, there won't be any
ambiguity if Foo::Bar comes to exist with a baz() in its root namespace;
they are invoked as Foo.Bar.baz versus Foo::Bar.baz so a clear distinction.
- The package name of the Muldis D language core is simply "Muldis_D"
(keeping it simple/terse); so no reason to explicitly say "Core"/etc.
- Strictly speaking, all other packages don't need to have any particular
hierarchy or be in any particular package namespace, though it would be
good to come up with something early on.
- Note that say a code file that is simply a direct translation of, say,
SQL, may not "using" Muldis_D::* at all, but just the DBMS::* in question.
- In fact, we should emphasize a feature of Muldis D where it is suitable
as a very good alternate syntax for SQL, easier for ORMs to build, etc.
It can exactly repr semantics of each SQL dialect while being cleaner syn.

* Version numbers of anything by authority https://muldis.com are formatted
as strings of 1..N nonnegative integers, where each string element is
separated by a period, and strings sort pairwise one element at a time from
start to end, where each element sorts numerically and shorter strings sort
before longer ones whose leading substring they are equal to.
- By default, version numbers are just a single string element and add more
when they feel they need to; most of the time, just 2 explicit elements are
used, and the initial version of any project tends to be the single zero.

* Make the fully qualified language names declarable by code to be more
flexible than the names declared by the language spec itself, so that the
ones in code can specify multiple language versions that they conform to,
as if the code is declaring that it only uses the parts of the language
spec that are unchanged or that intersect between all the specified
versions.  For example, let one say:
    Muldis_D:Plain_Text:"https://muldis.com":{0.112..0.125,0.127..0.136}
... and then any implementation which takes one of those versions will
parse the code according to any of those same that it supports.  The idea
is to make code more easily compatible with a wider range of interpreters,
such as newer ones designed for version 25 who don't explicitly know how
to emulate version 23 or know what its differences are, and are just
trusting the code to be valid for version 25 even if declaring 23; by the
code declaring a range, it is declaring it is willing to take its chances.
- When an implementation provides or emulates multiple of the language
versions that the code declares itself to be valid for, the implementation
is free to treat the code as if it was just any one of those versions, and
which version is chosen is formally undefined; the implementation can just
pick any version that it chooses and the code compiles/runs or not as such.
If the code fails to compile/run as any random version it claims to be
compatible with, the implementation shall *not* try again as another
version in the list as a fallback, probably.
- A multiplicity of stated authorities is also possible.
- A number of consequences still have to be thought out here.
- Specifying a closed range is the code saying it *knows* it is compatible
with all those versions, specifying an open range says take chances or is
not recommended and maybe won't be supported.
- Or, rather than having open ranges, we could stick to just closed but
have both whitelist and blacklist ranges, where we explicitly specify what
versions we *know* either are or are not compatible, and so then any
versions not explicitly named in one of those 2 lists is considered unknown
as to compatibility.
- Similarly, an implementation could be introspectable for the same 2 lists
white and black that it supports for implementing.
- It would be possibly implementation-defined or externally
user-configurable for what happens if the implementation only supports
language versions outside the whitelist including versions outside the
blacklist.  A stricter setting would say reject, permissive may say accept.
- For permissive, its possible that if vnums are fully orderable, then a
nonmatching implemented vnum directly between or just next to versions in
the whitelist could be considered to work, likewise for blacklist
considered not, and between one in each list considered not.  Eg, if the
program code declares {!1..5,6..10} and the implementation just provides
version 12, then it could be considered to work by default.
- In the interest of being nullary, we could permit code not specifying any
version number at all, or authority, same as Raku, meaning that the code
does not explicitly consider that it would work or not work with any
specific language/module/etc version at all.  So a strict implementation
would reject all such code out of hand and a permissive one would accept
all such.
- On the other hand, in the interest of preventing bad habits from
starting, where people just leave out the auth/vnum specifiers and then
people seeing that code not realizing that being specific is even possible,
we could say that it is always mandatory to specify at least a
single authority+vnum that the code is known/declared to work with, so that
any implementation has at least some idea to work with as to the likely age
of the code or what implicit expectations it might have.  The writer of the
code had presumably been testing it with *something* before releasing it.
And even neophytes who copy-paste whole code would be getting either
correct behaviour or a rejection by an implementation from version diffs.
And savvy people that go around the internet can more easily spot bad or
outdated code examples in tutorials or script archives because they'll just
have an old dependency version specifier and not mention newer versions.

* Consider use of the term "curator"/ed/ion/etc with respect to authorities.

* Lets say that language name declarations are mandatory in code files but
are optional in ad-hoc code read from a shell or fed from a host language,
because in the non-file cases the shell context or host language could
previously/separately set the context for what language is expected.  In
the case of a shell, lets just say that a language name declaration is
written just like a shell statement or command; we know its that kind of
command if it looks like a language declaration, as no normal code would.

* See also Jesse Vincent's "Perl 5.16 and beyond" talk, where Perl core
policy or Jesse's proposals for such are discussed that when you say "use
v5.xy" then no matter what the actual (newer) executing Perl version is,
you have declared that you expect it to behave with the same semantics as
the same version 5.xy that you named in your code.

* See also
http://tech.valgog.com/2012/01/schema-based-versioning-and-deployment.html .

* Note, http://strangelyconsistent.org/blog/dash-n-and-dash-p-part-three
clarifies what "setting" means in Raku, and there can be more than one of
them at a time that are layered; it does not mean "language core"; Raku
has the separate CORE concept, so staying with Core seems approp for MD.

* Remove TTM concept/language of "possrep" from Muldis D spec, at least in
the way it is presently used.  Instead, each scalar base type is simply
defined in terms of exactly 1 list of components/attributes, which doesn't
have a name (or corresponds to the empty string).  Also, the idea of
"possible representation" actually applies to the whole language, including
nonscalar types, because the DBMS can still choose any physical
representation it wants; its not like scalars are special there to reserve
the term "possrep".  Additional "possrep" are just syntactic sugar for
wrapper routines or such or pseudovariables.

* Quoth Anthony Clayden: "Specialisation by constraint is the defining
characteristic of the TTM model".

* Don't use the term "scalar" to refer to things that are neither relations
nor tuples as TTM does; conversely don't use a term to refer to things that
are either relations or tuples but not something else; just say "tuples or
relations" when that is meant, which wouldn't be terribly often, and
similarly "values/types that are neither tuples nor relations" when meant.

* Change Array/Set/Bag/Dict/Interval/Maybe/etc so that they are no longer
relation subtypes but rather are disjoint.  Generally eliminate any
system-defined tuple or relation subtypes, leaving all such to users.
This should help eliminate a number of gotchas from Muldis D that could
trip users up, such as users sometimes having to know the names of the
special relation attributes despite those being hidden in practice.
Also we would not confuse semantics; eg, tuples and relations
have predicates but sets/arrays/etc do not.
Emphasize the fact that Arrays are quite different from all the those other
types, in that they have a dense keyspace starting at a fixed point, where
the others are all sparse; eg, never make Array a subtype of Dict or such.
Similarly, consider defining Array directly in terms of List rather than
say in terms of a "scalar" with a Relation component like with Dict/etc,
because List can be used natively; but leave unordered types as over rels.
So Array's like, but disjoint from, String; list of anything versus of int.
The various details need more thought.

* Have the 10 Relation subtypes
"Relation.{K0,K0C0,K0C1,D0,D0C0,D0C1,D1,D1K0,D1C0,D1C1}" where "D" means
"degree", "C" means "cardinality", "K" means "degree of minimal key".
Having D0 implies K0 but K0 does not imply D0; also, K0 bi-implies C0,C1.
And thus we have names for all the relations that can be used unambiguously
without any need to know their attribute names for special purposes.
The D0s are the boolean or identity values for join.
The K0s (and D0s) are the "maybe" types with C0->"nothing", C1->"just";
but there is no single "nothing" value anymore, rather an infinity;
for that matter, redefine the type "Maybe" as an alias for "K0"; despite
the heritage, we may want to ditch the "nothing"/"just" names actually.
The D1s are convenient in that one may reference their attribute
unambiguously without knowing its name.

* All the selector syntax specific to Set/Array/Bag/etc would produce
non-relation values, as if it did, the attr names would be implicit.
But Set-taking functions should also take any D1 relation as an alternate.

* In an attribute list, the first of these is a shorthand for the others,
or specifically for the second, but the third has the same semantics:
    maybe foo : Bar;
    foo : maybe_type { foo : Bar };
    foo : relation_type { foo : Bar; primary_key {} };
... but while the first can only do D1K0, the latter can do K0 in general:
    foo : maybe_type { x : Bar; y : Baz };
    foo : relation_type { x : Bar; y : Baz ; primary_key {} };
So the new maybe is a more workable/flexible way of handling nullability.
Actually, the "maybe foo : Bar" reads interestingly because it might
suggest to some readers that having the "foo" at all is optional;
however the latter thought is not one that we want to encourage per se.
Remember, "maybe" and "set"/etc are now completely disjoint.

* Operators like "//" that deal with maybes need to be closed, where both
arguments must be maybes and the result is a maybe.  This means that the
final/default value in a //-chain must be a just, eg:
  x // y // :@{{3}}
... where each arg can be any zoo relation, and :@{{3}} means :@{{0:3}}.
And so the //-chain is then followed by attr extraction when that's wanted,
eg: "attr(x // y // :@{{3}})" for D1K0s or "(%(x // y // :@{{3}})).foo" or
something for K0s in general where we must name the attr.
Note that a // defined this way can't have an identity value since multiple
values would qualify for the semantics, same as with bitwise-or.
Note that overloading //, eg to work with some "special" value to mean
null, can only accept non-relation args in practice so not to overlap.

* Make a depot/etc have 3 main payload sections (each of them being a
"Database") rather than 2; where we had "catalog" and "data" before, now
add "constants"; names of the 3 subject to change.
- The purpose of "constants" is to be the most effective way to represent
the bodies of non-trivial niladic functions, or the values of non-trivial
expression trees with no free variables.
- The "constants" are semantically part of the code, and changing them is
considered a data-definition rather than a data-manipulation; however they
are stored directly as the values they represent, same as things in "data",
rather than as trees of expression nodes in the catalog.
- Unlike things in "data", you can refer to "constants" directly in the
bodies of function or type definitions.
- Each constant typically lives in its own RVA of the constants database,
or several may be in the same if they have the same structure or type.
- An example usage is that the Unicode character database could be
accessable to Muldis D within a "constants" database of some appropriate
module.  Another example is parser rules defined as relations.  Another
example usage is for the user text of programs, or exception/error
messages, especially multi-lingual ones.
- Where one wishes to comment individual items in a collection constant,
they do so by adding extra RVA attributes for the comment text.
- You do not normally use constants to define enumerated data types that
define new values, such as Boolean etc, which are unions of singleton
types; but you can use constants to define enumerated types that are subset
types of others.  Constants are analogous to values/functions, not types.
- Constants live in the same shared schema-namespaces that are shared by
types/routines/etc and relvars, and all must have distinct names therein.
It is as if the "constants" and "data" databases are superimposed, so no
RVAs in one may have the same names as those in others, but schema/TVAs may
exist in common.
- One always uses function syntax to refer to "constants" RVA/TVA within
routines, as if each one was in fact declared as a niladic function.
- The only way to refer to "constants" as variables in routines, for
dynamic data-definition, is in procedure pseudo-parameters, same as when
referring to the catalog database or "data".
- We would probably have special/shorthand syntax for declaring constants
in code like we have for declaring types or routines, etc; for example:
    FOO ::= constant (...);
    dbname.schemaname.BAR ::= constant (...);
- Similarly, whatever syntax we use for nesting a higher-order function
definition or a type definition in a routine, we use something similar for
nesting a named constant declaration in a routine, in order that it be
treated as a "constants" item rather than an expression tree that it would
otherwise be.
- You can't use "constants" for values that are not deeply-homogeneous,
like something kept in a "Database" must be; for those you have to use
regular expression trees.  But it is expected that in practice any values
that aren't deeply-homogeneous would be transient or generated at least
partly from free variables, so this limitation shouldn't be a problem;
or you should likely be able to refactor the values down into
deeply-homogeneous inputs to a simple regular value expression or niladic
function which then gives the actual desired vaulue.

* CHANGE AFFECTING THE ABOVE ...
Change the "Database" type so its attributes may be "Relation" in general
and don't have to be "DHRelation"; that is, the type of a database relvar
attribute may now be anything up to "Universal".  Or alternately, perhaps
go for some kind of middle ground where we say that every relvar tuple
attribute value must be a Scalar|Tuple|Relation and not some other List.
- Or alternately, the language supports "Universal", but users can request
varying levels of strictness for their databases, such that it should be
very easy for them to declare they only want to allow DHRelations; then
again, the existing facilities already let them ... the declared db type.
- This should simplify Muldis D in some respects, because we would be
removing a language limitation that is more arbitrary than essential.
- Now this change might seem to complicate certain database design or
implementation issues, in that the general case of a relvar TVA/RVA would
no longer be refactorable into a single other relvar per TVA/RVA, if
different relvar tuples might have tup/rel attrs of different headings;
each possible one would then have to become a separate relvar, or otherwise
such refactor just wouldn't be possible.
- However, in practice we already face this problem by our TTM-blessed
support for scalar union types in relvar attributes, generalized to
"Scalar" typed relvar attributes, should we, say, wish to unpack such
scalar values into relvars, or implement as such behind the scenes in some
engine that just likes to have simple homogeneous scalar relvar attributes.
And so, allowing it for TVAs/RVAs just makes things more consistent.
- I also don't see any logical issues with this support, in general.
- This change also means that a "constants" db *doesn't* have to be DH.
- In theory we could also take a next step and have Muldis D support
"databases" that aren't relational at all, as an option, but rather let the
"database" value be of any type at all.
- Or to keep things more manageable, we still require that a "database" is
some kind of "Tuple", but then it is no further restricted than that.
That is, we require "Tuple" so it can be overloaded as a namespace for
types and routines etc, including the restriction on when type/routine
namespaces and attributes may be equal or not, if the current language
grammar even requires them to align, but we will assume that it does.
- So then, the system-defined type "Database" simply goes away, as "the
database" is just a "Tuple", but we can make it very easy for a user to
define a "Relational_Database"/etc subtype with whatever level of
restrictiveness they desire.
- And then, one can also more directly represent a SQL database in Muldis D
(or some other non-relational db/storage du jour), not just by using the
appropriate packages that define DBMS-specific types/routines, but by also
declaring that "the database" is of some DBMS-specific/allowed type, such
as, that all database tuple attrs are "Table" typed rather than "Relation".

* Make Muldis D self-hosted.  Have it support parsing Plain_Text code using a
read-only operator into trees of scalars that are the new native Muldis D
code, which the older system catalog is just views on.  Make it support
writing the full Muldis D compiler in Muldis D, that can eg generate Perl
code, and then the Muldis D compiler can compile itself.  That is, the
reference implementation would be compiled to Perl, and that Perl code is
capable of translating equivalent Muldis D code into that same Perl code.

* Ooh! Oooooh! The trees of scalars form of Muldis D code might just then
give us the option of more comprehensive higher order functions, where you
actually pass the function definition as the argument, rather than its
name, as you do now, though the latter would still be perhaps easier to
implement on more foundations. We'll see if that's necessary or works.
That would certainly be more of the true anonymous function nature.

* Note that the typical action of Muldis D implementations could be called
"transcompiler" or "source-to-source compiler" because they translate
between high-level languages rather than directly to machine code.

* The new system catalog would have several main namespaces where each
comprises a different view of the same package code.
- The primary view, maybe call it "trees", would present each complete
package/module/extension as a single scalar value that is a concrete syntax
tree package node, whose attributes contain the rest of the tree defining
the whole package.  These trees are composed of the nodes that are now the
native Muldis D code format, which Plain_Text very closely approximates, and
the HD_Perl*_STD slightly less closely approximate.
Preserves details such as: whether child expressions were written nested
under parents without explicit node names vs nested with explicit names vs
off-side with explicit names, and
all code comments that are explicitly bound to names or parts of code like
named declared entities, and the visual sequence of code elements in the
string source, and whether operators are invoked prefix or infix, etc.
Does not preserve details like the actual whitespace in the code, or the
actual string escape sequences, or whether identifiers were quoted or not,
or the exact format of the numeric literals, and so on.
Does not explicitly hold gen names for nodes that were anon to programmer.
- Another view, maybe call it "relations", can be loosely what the
current/old system catalog is, having all of the same information as
"trees" but all broken out into a flat namespace, like the current/old but
that all the RVAs/etc are basically ungrouped or split off into separate
relvars.  Or maybe it would be partially flattened and partially not,
whatever makes for the easiest use by people, or there could be multiple
sub-views showing varying degrees of being flattened.
For various reasons it may be wise for "relations" and "trees" to
be information-equivalent, so each is fully updateable with changes
propagating to the other deterministically and losslessly.
- As "relations" has explicit names for all expr/etc nodes while "trees"
only does when the programmer explicitly provided them, names must be
generated when deriving "relations" from "trees" and stripped the other
way; the names must be generated deterministically, and a constraint must
exist on "relations" so explicitly setting them differently is disallowed
while the "generated name" attr is also true on that node; expect that said
gen name is probably something like catenating the names of the ancestor
nodes in the expr/etc trees starting with the nearest non-generated one;
the generated names could take the form of a chain of integers, one integer
per ancestor level, where the integer is the ordinal position of each node
beneath its parent, starting at zero.
- Another view, which is not information-equivalent to the others, would
have further derived information, say more resembling the SQL system
catalog, say showing in one place what all the actual attributes or known
keys constraints of a relvar are, regardless of whether they are defined
using special syntax or as value expressions.
This view would be writeable, but as it is a dependent on the others,
changes to it would propagate in certain ways, such as how changes to a
lowercase attribute would propagate to a mixed-case determinant one.
- The above views, or at least the 2 information-equivalent primary ones,
would be provided by the language core; the last one might be an extension.
- To clarify, views are allowed between modules, so eg one can define
another kind of system catalog, but only between modules that are loaded as
extensions rather than depots, I think.

* An EXTENSION module, eg Muldis_D::Plain_Text, is what provides the routines
to translate Plain_Text code strings into Muldis D's native dialect, the trees
of scalar nodes that the "trees" catalog view uses, and said extension also
provides the reverse translator; working with Plain_Text code strings is *not*
part of the Muldis_D core module, and such would only be needed in a
Muldis D program implementing a REPL or a compiler or one that uses Plain_Text
as its disk storage format.
- This extension would also define another set of syntax node types which
holds a lot more concrete syntax details than the native node trees, such
as the actual whitespace used between things or the actual format of
identifiers (bareword vs quoted) or any extra/explicit nesting parenthesis
and so on, such that the exact same Plain_Text
code string could be reproduced from it; mainly these would just be used
internally to the extension, except say when someone wanted to extend it.
- The Plain_Text->native translator function would probably have 3 main other
functions that it invokes serially, where one is a tokenizer that just
converts the Plain_Text string into a flat array of tokens, where each token
may be represented by some token-type-specific scalar value rather than
being a Text value, so eg each end of a delimiter pair and whitespace hunk
would be a separate token value here, and the second function takes this
token array and converts it to a tree, eg combining delimiter pairs into a
single value with children from 2 without; the third converts this full
concrete tree to the quasi-concrete tree of the native Muldis_D.
- It is expected that some level of Muldis_D::Plain_Text is what deals with
unescaping string literals or compat-normalizing Unicode etc or otherwise
following the wishes of concrete syntax directives.
- Similarly, the extension would be what determines whether a string is
valid Plain_Text, including that it provides a "is this Plain_Text" bool func.
- Similarly, Muldis_D::Plain_Text would provide config options for the
translater to Plain_Text from native, say on what num format opts to prefer
or what length to wrap lines etc.
- To help specify a pure functional parser that is both resistant to having
too-deep levels of recursion (say, a process defined on head+rest), we
should utilize some of Muldis D's more unique features, mainly the
relational operators, and define the parser largely in terms of say
relational join and such, so it is more conceptually parallel than
recursive (or iterative).
- Have a function that derives an Array-of-Code-Point (or similar relation)
from a String, then use it on the Plain_Text source string as the first step,
and then say do tokenizing in terms of joining that relation against a
static relation having say 1 tuple per distinct character in the text
repertoire, mapping for example traits like is-whitespace or is-symbol or
is-alphanum or is-string-delimiter or is-escape-char (\), so say the result
of the join is the indexes into the original string of where tokens are.
Filtering out say escaped chars from unescaped ones is easy; find all
occurrences of the char and then set-minus those with a "\" just before.
It is useful to do edge detection early by joining a folded (to char kind)
string to itself offset by one ordinal position and seeing where the pairwise
characters are not of the same character kind.
The static relation of characters may be best stored in code in a compacted
range-defined form, and if expanding is needed, do it when needed;
then we may only need to store a few dozen tuples rather than thousands.
- This same design would scale transparently to different repertoires,
eg ASCII vs Unicode; just the static tables of characters are larger.
- The very first step in parsing Plain_Text:Unicode(...) is to bring the source
Text to a normal form, either "canon" or "compat" (with semantics of NFD or
NFKD in the spec) and then the Plain_Text parser proper then works with that
Unicode normal form as its input.
- We would have a whitelist of code points that may appear outside of a
quoted string in the source, and all others are illegal outside, for
security purposes (lookalikes) if nothing else; it is applied post-normal.
This makes it similar to pigeonhole charcters into symbol vs alpha for
example, where that distinction mainly just matters for barewords.
- We have to figure out how best t work w Unicode combin chars for parsing.
- Note, according to http://en.wikipedia.org/wiki/Unicode_symbols, Unicode
has a main distinction between "scripts" and "symbols".

* Thanks to syntax/package separation, we have a nice semi-orthogonal
situation where Plain_Text syntax can be used to write Muldis D or SQL,
and SQL syntax can likewise be used to write both, in theory.
- That is, the package determines the "semantics", the grammar the syntax.

* It remains to be specified how to support code expecting different
versions of Muldis D or other modules, such as where to say no and where to
emulate and at what level to emulate.  Raku's solution may be a guide.
We might consider multiple packages as a solution where different packages
provide different versions of the Muldis D API, and they can be used at
once, similar to having packages provide the APIs of SQL / other languages,
so that the parsers eg for Plain_Text don't have to do it themselves for
modules but rather just for concrete syntax vers.  Still open to debate.
- The best way to do this emulation cleanly, for package/module versions,
is that in the general case we consider every name+auth+vnum permutation
to be a distinct module, and that permutation is the actual name of the
module, so code expecting/using different module versions is using
different names to refer to its entities.
- It is mandatory for a package/etc to, for each of its explicit module
dependencies, to also declare a package-local alias for that dependency,
by which name it must always refer to the package or its entities.
- The effect that this has is like that of the global params of recipe
routines; there is no longer a global namespace of used packages in the
DBMS, but that rather each package's code only ever directly sees within
the same package, and any used packages' contents are aliased to namespaces
within the using package; the language core pkg is treated the same way.
- An implementation may provide packages for different versions of the same
base package at once.  When a package's "using" statement indicates that a
range of versions are suitable, the DBMS implementation would just pick a
single one of those for which it actually has that dependency package
version and associate the user's local package alias to that specific
version longer name.  Since a "using" specifying multiple versions means
any of those should have the same semantics for features used, the DBMS is
also free to remap at runtime which of several satisfying versions it has
to the alias, so that say if multiple dependent modules specify overlapping
but unlike dependency versions, the DBMS can choose to use the same version
it has for both in the intersection of those ranges, without breakage.
A package's code should not generally need to know which specific
dependency version it actually got or has, though there might be rationale.
- The package-specified alias specifier might conceivably be more generic
than just a Name; it might be a path in the using package's namespace into
which to "mount" the other package's namespace.
- Depending on how the aliasing works, conceivably if package Y explicitly
uses X and Z explicitly uses just Y, Z can also access X's namespace within
Y's namespace that it sees.  ACTUALLY, NO, that's a bad idea; Z should not
see any part of Z by way of Y but for Y's explicit synonyms of things of X.
- Regardless, when dealing with higher-order functions and such, those
functions are a closure of sorts wherein all references to package entities
in the function code are resolved relative to the packages where the
functions are declared, not relative to where they are actually invoked.
- We also need to revisit matters of depot mounting and such, and see if
the aliases we declare there and for packages can be generalized together.
- Perhaps from each package's perspective, the entity namespace is this ...
    /local/*  - current package's namespaces/entities
    /used/<alias>/*  - each used package's (or core's) namespaces/entities
... those being the absolute paths.
- Or perhaps better yet, if each package is also required to declare an
internal alias to refer to itself with, the above can be flat, just:
    /<alias>/*  - current or each used or core pkg's namespaces/entities
- Separately, each package needs to declare an explicit ordered list of
search paths that it will use internally by its own code to resolve
unqualified entity references in its own code.  Typically the first 2 items
in the list will be the package's own self and then the core language
package.  The list can be either empty or include all the used packages;
searching will only look in this list, and one can't use search/unqualified
syntax to reference anything outside this list, just relative or absolute.
- Example:
    Muldis_D:Plain_Text:ASCII:"https://muldis.com":0;
    package pkg ::= FooLib:"http://foo.com":0;
    using MD ::= Muldis_D:"https://muldis.com":0;
    using Pg ::= DBMS::Postgres ...
    using Spatial ::= Muldis_D::Spatial:"https://muldis.com":0;
    searching [pkg,MD];
... uses alias "pkg" for current package, "MD" for core, and so on.
- Making the explicit search path list mandatory in order to get
unqualified referencing is important because then neophytes can't just
copy-paste parts of code from random online tutorials or script archives
without being explicit about resolving any references therein, which can
trip up people where the same code may resolve to different operator calls
depending on what modules are used.
- Perhaps having just the language core in the search path list and not
even the current package is best in some circumstances.
- Note http://www.postgresql.org/docs/current/static/ddl-schemas.html#DDL-SCHEMAS-PATH
which has a "search_path" setting that works similar to this.
- But the aforementioned is for schemas; therefore Muldis D may be best to
offer a more finely-grained option, where Muldis D's search path takes a
list of name-chains rather than names.  This has an advantage of letting
one select just parts of a package to search, or the order within a package
to search, etc.  Each name-chain in a search-path list can be either
absolute or relative, the latter being resolved relative to each individual
reference; or better yet, make each search-path item absolute only and that
the language separately has the option to search relative to the
referencing entity first.  This needs more thought.
- For implementation, we are probably best to preindex all the packages,
where all possible unqualified or semiqualified namechains are mapped to a
list of all fully qualified chains they match the endings of, so no actual
"search" is necessary; we just have to evaluate on arguments later.

* We also need some distinct syntax for defining that a package definition
is split into multiple pieces for storage, which for source code typically
means one disk file per piece, but they are logically still one package.

* Consider making the "lib" or "cat" or "data" portions of namechains
optional in contexts where they would always be the same, especially "lib",
so for example one can just say "nlx.myfunc" rather than "nlx.lib.myfunc";
this would be loosely similar to the elimination of "lex".

* For that matter, consider restructuring the namespace tree in Basics.pod
so the lib|cat|data|etc are the namechain first element, with fed|nlx|etc
the second element; and sys doesn't have lib|cat|data already, or in fact
that would all be grouped under "lib" arguably.

* For that matter, lets just eliminate the special namespace prefixes
entirely, even as an option, because typically certain contexts only allow
certain kinds of things anyway, such as just lib or data, and so one should
never have to or be able to write chain elements that mean anything other
than their literal selves.
- Rewrite ENTITY NAMES in Basics.pod to get rid of the formally structured
shared namespace as it currently is, and instead have that pod divided into
context-based sections, saying for example "in this lexical context a
namechain means this" and "in this nonlexical context it means this".
- Where one needs to explicitly use an absolute path vs an explicit
relative path vs a searching relative path, we can provide a new data type
which wraps the basic name chain of old to provide this.
For examples (new arrayish structure <-- old way of stating):
  ['abs',['mydb','myschema','myfoo']] <-- fed.{lib|data}.mydb.myschema.myfoo
  ['rel',2,['myschema','myfoo'] <-- nlx.par.par.{lib|data}.myschema.myfoo
  ['rel',0,['mybar']] <-- nlx.{lib|data}.mybar
  ['shp',['myschema','myfoo'] <-- shp.{lib|data}.myschema.myfoo
The above could possibly be written like this:
  .$mydb.myschema.myfoo or abs$mydb.myschema.myfoo
  2$myschema.myfoo or rel$2$myschema.myfoo
  0$mybar or rel$0$mybar
  myschema.myfoo or shp$myschema.myfoo
I'm still missing an example when one wants to say "sys", maybe, though
both "abs" and "shp" kind of apply there but kind of not.
Note that in the above it is mandatory for there to be no whitespace
around the "$" ... the formatting is like with "#" in numeric literals.
- Now while there are {$:,%:,@:} prefixes for {S,T,R} value literals, and
the colon-less {%,@} mean cast between tuple and relation,
the colon-less $ doesn't make sense for a similar purpose,
so use $ for name literals, that is, "$foo" means "Name:foo",
so one can write projection like "r keep {$foo,$bar}" not "r{foo,bar}",
and then for example "$<>foo" means "NameChain:foo" (and "$<>" the emp nc).
- Also add the ability to use namechains in declarations too, which is
essentially a shorthand; for example, this:
  foo.bar ::= function ...
... is logically equivalent to:
  foo ::= schema {
    bar ::= function ...
  }
Now technically this new way only lets one declare a nonempty schema but
to make an empty one one can still always say:
  foo ::= schema;
- The scalar type name stored inside a ScalarWP is always an 'abs' one,
or some separate canonical ns that takes into account type name aliasing.
- The scalar type name of a scalar value actually stored in a depot
can/should be exactly what the user wrote as per source code; it only is
resolved to some global namespace when compiled from the system catalog;
this is visible to the user of course in the low-level type system when
looking at scalar values, but not in the system catalog / source code.

* To keep things simple, the core Muldis D language does *not* for the most
part have any Unicode knowledge or knowledge of any character repertoires
beyond what common legacy 8-bit encodings handle; extensions are where
Unicode/etc goes because it is sooo complicated and large.
- The Muldis D core still fundamentally considers String/Name/Text to just
be a sequence of "big" integers, and so still supports Unicode etc, but
what the core lacks is any knowledge say about what Unicode characters are
letters vs symbols vs whitespace etc or base vs combining or normal forms
or canon vs compatibility or case folding rules and collations so on.
- The Muldis D core has the "Text" type which just has the integer string
possrep, and it has proper subtypes for ASCII,Latin1,etc (maybe EBCDIC?)
where each adds a possrep.  The Muldis D core knows a small amount of
Unicode, but just the subsets of its repertoire and character codes/names
for characters in Latin1/etc, such that the base number string possrep
still is valid Unicode character numbers for those characters.
- An extension, say Muldis_D::Unicode, adds the complexity of knowledge for
the rest of Unicode, including the NFD and NFKD Text subtypes/possreps etc.
- The Muldis_D::Plain_Text string code parser also comes in 2 or more versions,
where said base version only can work with source with th same simple/8-bit
knowledge as the Muldis D core, and you need another version say
Muldis_D::Plain_Text::Unicode to handle source using the wider repertoire,
at the very least so it can parse code properly where the parsing semantics
depends on the notion of what characters are letters or symbols or ws
and on canonical vs compatibility notions of equivalence.
- The Muldis_D declaration at the top of a code file also indicates what
character repertoire or encoding or etc the file is written in (which may
even include the "auth" part of the name), so it appears almost-first,
say like this:
    Muldis_D:Plain_Text:ASCII:"https://muldis.com":0;
    Muldis_D:Plain_Text:Unicode({1..6.1},UTF-8,canon):"https://muldis.com":{0..42};
    Muldis_D:Plain_Text:Unicode(6.0,UTF-8,compat):"https://muldis.com":0;
- There may be further variants for diff versions of Unicode standard.
- Muldis_D::Unicode also adds the synonyms for core routines/etc that have
Unicode character names, while the core just has the Latin1 char names.
- There is still the question about avoiding combinatorial explosions about
lots of other modules that want Unicode op names but that should also
degrade gracefully for implementations lacking them.  Probably one easy way
out is that these can just declare these with declared delimited entity
names using numeric escape sequences rather than literal Unicode chars.
- We should be using subtype polymorphism to our advantage.  The Muldis_D
core package should declare a mixin type "SourceCode" or several such like
"PackageSourceCode" or "ValueSourceCode" etc and then declare a virtual
parser function whose input type is "SourceCode" and whose result type is a
node of the native Muldis D language.  Then Muldis_D::Plain_Text would declare
a subtype of Text that composes the mixins say "PT_STD_SourceCode_ASCII" as
well as a function implementing said virtual parser which takes that ASCII
type as input, while Muldis_D::Plain_Text::Unicode does a corresponding thing
but for a different subtype of Text as appropriate.  These Text subtypes
would have their defining constraint as, Text value that starts with the
string "Muldis_D:Plain_Text:ASCII:" and such.  And so the logic for what parser
to select in a generic case just comes down to regular multi-dispatch based
on analysis of the Text values for characteristics specific to a language,
and it is easy to auto-extend the system with support for new languages,
simply by somewhere "using" a package that decl an impl of the virt func;
but a main importance is that the Unicode vs not thing is no longer a
complicating factor for users.

* Here is the main program of a possible Muldis D cross-compiler to Perl
written in Muldis D; it reads Plain_Text Muldis D code from STDIN and writes
equivalent Perl code to STDOUT.  One of the first goals of implementing
Muldis D should be to empower this being able to compile itself.
Presumably all of its dependencies would start out as shims that are
written directly in Perl, which are gradually converted.  Of course, the
following would initially also have to be written directly in Perl too, to
bootstrap. This is a static compiler, which by default would succeed in
producing valid Perl code just as long as the input Muldis D code is
syntactically correct (and defines a Muldis D package), in which case its
output would compile successfully in Perl.  Generally all errors which
would not be caught in that process are of the nature of invoking or
referencing some entity that doesn't exist or is of a mismatched type etc,
and then by default those would produce errors when running the Perl code.

    Muldis_D:Plain_Text:ASCII:"https://muldis.com":0;

    package perl_from_mdpt ::= perl_from_mdpt:"http://example.com":0
    {
        using Muldis_D         ::= Muldis_D:"https://muldis.com":0;
        using Muldis_D::Plain_Text ::= Muldis_D::Plain_Text:"https://muldis.com":0;
        using Muldis_D::Perl  ::= Muldis_D::Perl:"https://muldis.com":0;

        searching [Muldis_D,Muldis_D::Plain_Text,Muldis_D::Perl,perl_from_mdpt];

        bootstrap ::= stimulus_response_rule
            when loaded invoke main;

    /*************************************************************************/

    main ::= procedure () :
    [
        /* Read the Plain_Text source code from STDIN until end-of-file.
           Today we'll just assume that it defines a package. */
        var input_source : Text;
        read_Text_file( &input_source );

        /* Validate the input source code and exit if bad Plain_Text syntax or
           it doesn't define a package. */
        if not (input_source isa $PT_STD_Package_Source_Code_ASCII) then
        [
            write_STDERR_Text_line( 'Sorry, that source code has a syntax error.' );
            leave;
        ];

        /* Parse the input source code into a Package-defining AST value.
           This AST is the "native source code" of Muldis D. */
        var package_AST : Package;
        package_AST := Package_from_Source_Code( input_source );

        /* Compile the Package-defining AST into Perl source code which
           when run has the same semantics. */
        var output_Perl : Text;
        output_Perl := Perl_from_Package( package_AST );

        /* Write that Perl to STDOUT. */
        write_Text_file( output_Perl );
    ];

    /*************************************************************************/

    };
    /* package perl_from_mdpt */

* To keep things simpler, don't have a distinct "value" parser node but
rather just use "expr" instead; when we only want a "value" we can then
either do further constraints on the parse tree from using "expr" or do
constant folding or both as applicable.

* A (Unicode-savvy) parser should treat Greek/etc letters in the same way
as Latin letters, as to where they may appear as barewords and how they are
interpreted, so eg "απΣθ" is parsed the same as "abcd", so these are all
var or foo() and not prefix/infix when barewords.
This should work well with how Greek is commonly used in maths.

* I should really be exploiting the variety of mature parser generators out
there to do the hard work for me, certainly in development, but also to
help any Muldis D implementations.
See http://en.wikipedia.org/wiki/Comparison_of_parser_generators for lists.
Dave Voorhis of TTM forum tersely recommends ANTLR (http://www.antlr.org/).
David Barrett-Lennard of TTM forum recommends Coco/R.
Quoth DBL: "Coco/R validates an LL(1) EBNF quite well, but uses a non-ISO
syntax (which seems to resemble Wirth syntax notation). Section 3.6 of the
Coco/R user manual discusses the errors it is able to identity."
The comparison url suggests I should try ANTLR first, as it seems to
generate parsers in more of the languages I'd be interested in, esp Perl.
Including: C, C++, C#, Java, JavaScript, Objective-C, Perl, Python, Ruby.
Or the ANTLR homepage actually says Perl support is at early prototype.
Also ANTLR is BSD, runs on the JVM, takes EBNF as input, generates lexers,
and it has an IDE, and its algorithm is LL(*).
See http://www.antlr.org/grammar/list for various already written language
grammars, including ISO SQL 2003, other dialects;
I should probably use the PHP grammar example to go by for mine.
http://www.antlr.org/wiki/display/ANTLR3/Quick+Starter+on+Parser+Grammars+-+No+Past+Experience+Required\
has the tutorial for the flavor of EBNF that ANTLR uses.

* Note http://blog.endpoint.com/2011/12/sanitizing-supposed-utf-8-data.html
the Perl modules like IsUTF8, Encoding::FixLatin, Search::Tools::UTF8,
Encode::Detect, Unicode::Tussle.

* TODO: Declare explicit subtypes of String in core:
String.U1,String.U7,String.U8,String.U32,String.U64
so String values can be explicitly marked as such and one can know quickly
whether the String is known to be just bits, ASCII chars, octets, etc
and so treat it more efficiently rather than defaulting to "big" semantics.
The U8 and U32 versions correspond to Perl's strings with isutf8 off/on.

* Change all the routines that take "Set of Name"/etc as arguments so they
instead take empty/no-tuple relations as arguments instead, such that the
headings of these relations convey the same set of Name.
- They're both relations anyway, but using headings for this is more
efficient, and make more semantic sense for such as projection or ungroup,
such as to say, "make the result look like this".
- Also make the last part of a relation literal optional for empty
relations; eg, so one can just say "@:{foo,bar,baz}" without the ":{}".
- So eg the prior example replaces "{$foo,$bar,$baz}" as project/etc arg.
- Also, the arg to rename can be a tuple rather than a binary relation.
- So eg one can say "%:{x:y,a:b}" rather than "@:[bef,aft]:{[x,y],[a,b]}".
- But we still need an alternative when we want to define attribute lists
dynamically at runtime; so we prob want "Set of Name"<->"emp rel" mappers.

* The generic reduce op (and meta) should be virtual/overloaded, so syntax
is the same whether the inputs are an Array or Set or Bag etc.

* POSSIBLE PARADIGM SHIFT!
Consider that most of the DB programming that is done on the internet is
extremely trivial, and developers are currently doing it using SQL plus
some other language such as PHP/Perl/Ruby/etc.
- So how much would these developers save if they had a simpler way?
- This could be a very interesting and strong business case for Muldis D.
- While I am keeping my focus on doing databases well as the priority, I
can see the use case also that people may actually want to write simple
apps and have them written entirely in one easy to use language, where say
the web app code and the database code are seamlessly integrated, rather
than the SQL-foo impedance mismatch.  So maybe for the many common simple
web/etc apps, one can just use Muldis D for the entire app, so the likes of
Perl/PHP/Ruby/etc are just cut out of the picture.
- While I designed Muldis D to be possible to write a full app in, I
assumed that most people wouldn't do this and would combine with another
language like Perl/Ruby/etc.  But maybe I assumed wrong.
- Maybe for the many very simple apps, using Muldis D for the whole thing
actually *is* the preferential option.
- I think I'm going to have to start pushing this angle more.  Don't just
compete with SQL, but also work to eat PHP's lunch, without doing PHP's
numerous lameness (including baking the kitchen sink into the language
itself).  So push that I make simple apps simpler, but without losing
actual power or flexibility.
- Of course, Muldis D would absolutely not have web stuff in its core, so
an extension would be needed for the web stuff.

* BUT SEE ALSO
http://www.infoworld.com/d/application-development/introducing-opa-web-dev-language-rule-them-all-172060
as it seems that Opa makes many of the same claims as the above paragraph,
such as one language to do the database and server and client side web,
where the DBMS and web server are built-in.  However, Opa's database is
apparently hierarchical, and it has 1 implementation, written in Ocaml.
A commentor says its like ASP.NET WebForms, that being bad.
Fortunately then that Muldis D doesn't actually concern itself with
generating HTML; Muldis D is only meant to replace the SQL+{PHP/Perl/etc}
pair and not also HTML/etc.
Actually, http://opalang.org/ shows examples that look like PHP when that
is mixed into HTML, and Opa calls itself "the cloud language"; different goals really.
See also http://www.infoworld.com/d/application-development/10-programming-languages-could-shake-it-181548
for multiple language mentions.

* This might have similarities to what I'm doing: https://dsl-platform.com .
At first glance seems to have a lot in common actually.  Also a lot not.

* Refactor, and establish as distinct concepts, Muldis D stores that are
code-only, which we will call "modules", versus those that are primarily
for data, which we will call "depots".
- The "modules" are where any code that should be shared by multiple
databases would live, and include all system-defined code plus all
user-defined code that is packaged and useable like system-defined code.
- The "modules" represent all functionality that is either traditionally
built-in to a DBMS, whether considered part of the Muldis D core or
implementation-specific, or is providable by third-party exten (including
Postgres' "extension" concept) or is conceivably low-level enough that it
would likely be implemented at least partially in a manner external to the
Muldis D environment so its internals aren't visible to the user.
- The "modules" *can* be an external declared dependency of a depot (and
of each other), that is, something which must be present in order to
interpret/use the depot, so like the system itself, that is ostensibly
this is an exception to the rule about depots must be self-contained.
- The modules are written physically differently on disk, at least with a
different primary keyword, so they can't accidentally be used as depots.
- The modules are always readonly to the DBMS.
- Modules are introspectable by system catalog but that is readonly.
- Modules are all individually versioned as per Muldis D itself and all
entities using such must declare the dependency using full version names,
so to ensure the dependent, especially a depot depending on it to be
understood, is interpreted correctly.  So when a depot or module is
declaring its language version, it is declaring a list of all determinant
modules with the same level of detail.
- The "depots" are the only stores that are possibly writable by the DBMS,
when mounted as such, and are what are normally considered "the database",
and are where all user data lives.  These can also contain code and often
do, and only code living here can be updated at runtime.
- In theory or practice a depot could be entirely devoid of code and just
contain data, in which case unless its declared database type is "Database"
it *must* depend on a (readonly) module to define its constraints/etc.
A big advantage of that is to support the common paradigm where data and
code *are* separate, which is frequently the case.  Even more so when
implementing Muldis D over simple data stores without the native concept
of stored user code.
- Maybe we should consider separate code and data files to be the
recommended default way of doing things, and that putting code in data
files be the less common option, so more like typical programming and
further distanced from the paradigm that Muldis D has long had.
If nothing else it would probably be simpler to start implementing Muldis D
if we assume that is the normal way of doing things.
- Ostensibly a primary difference between a module and a depot is that a
module is written as a plain text code file in exactly the same way that
programmers normally write code such as Perl or C or whatever, done
externally to the DBMS in a text editor with direct access to the
filesystem, and then this is considered static in running programs, while a
depot is read and edited from within the DBMS at runtime, using
data-definition statements.
- The point of this new module emphasis is to reduce the problem of having
to duplicate or alias type definitions and such to make multiple depots
work together; for example, often the types shared by a database and an
application might be in a module so each of the latter doesn't need their
own copies.
- Muldis D will provide 2 completely distinct namespaces for modules and
depots, where routines/types in modules are referenced in the manners
typical for programming languages, often fully-qualified, though I suppose
modules, depots can optionally choose to create synonyms within themselves
for such things outside themselves (only to modules) for brevity.
- Modules always declare their own namespaces as per typical programming
languages and they do not vary by or can be controlled by DBMS users.  So
such naming will need to be managed by external convention such as that
the modules declare their own versions with distinct base names that
indicate their namespace, ala Perl modules.  Only depots are under the
user's control for what base namespace they mount into.
- Modules would still support "data" sections, but these would be readonly
and intended for static data resources of nontrivial size, which would be
more efficiently maintained that way than in the "catalog", theoretically.
- For various reasons, including security, with searched namechain
invocations, modules are always searched before depots.
- Lets say for good compromise on various things that various shorthand
syntaxes for operators, such as using non-foo() notation, are only
available for invoking those in modules maybe and that otherwise there is a
different qualification for module and depot thing invocation.  Maybe.
- For data sections of both modules and depots, we could consider something
like an "inclusion node", which may appear anywhere a value node may appear
and pulls in that value from an external source, typically another file,
and then the semantics are as if it were defined inline instead.  This is
kind of like C's include or maybe PHP's include or something or other.
An inclusion node would be formatted like a tuple literal and its details
would be implementation-specific, like specifiers for depot mounts etc, but
in the default case would probably just point to a file in the filesystem.

* Add core support for an analogy to SQL TABLESPACES, with that support
somewhat resembling the depots concept in form in that it has
implementation-specific parameters such as filenames/paths/etc.  The idea
is users can help optimize some things say by storing different parts of a
depot in different file layouts as would be more optimal for that section
of the depot, eg code vs data, and particularly insert-only data versus
frequently updated or deleted data.  But it is important to note that the
depot is still generally the scope of self-definedness or ACID, and so only
DBMSs that support this across multiple tablespaces may use the Muldis D
analogy to said, and our "tablespace" would map directly to theirs.
- With this concept supported, we theoretically could relax certain
requirements that "temporary relvars" need to be defined by existing in
"temporary depots".  Or caches.  This matter requires more thinking.
In fact, multiple DBMSs already seem to use tablespaces for temp stuff,
including Postgres and Oracle.
- We might repurpose the term "subdepot" for the tablespace analogy, or
call it "storage space" or "storage pool" or "pool" etc.
- See also the TTM list thread circa 2011 Nov 16, and "Fragmentation" of
http://www.softwaregems.com.au/Documents/Sybase%20GEM%20Documents/ .
- Note that Postgres tablespaces are actually not disposable; that is,
losing one is significant damage to the whole database that takes some
non-clean effort to recover from; so just saying you will put transient
data in a tablespace on unreliable storage won't actually work
(clearly this is a missing feature in Postgres).

* NEXT PRIORITY...
Rework routine headings/definitions, especially functions:
1.  Remove native concept of optional parameters; this lack can be worked
around in various ways such as with wrappers or multiple routine versions
or polymorphism or priming (partial function application) or system design that emphasizes greater
specialization or fewer parameters.
2.  Make public routine parameter aliases a wrapper thing.  Each routine
has exactly 1 native name for each of its parameters, which is the only
name shared by the routine's internals and its users, and both sides have
their own independent alias namespaces.  For any function that explicitly
declares itself to be symmetric or commutative or associative or whatever
in its routine heading, it *must* use the positional names "0","1",etc as
its native parameter names.  Or for that matter, we could say that in
general any routine intended to be used infix must have exactly 2
parameters, with positional names, and any with more than 2 would not be
used infix.  The routine-internal alias namespace is simply ordinary
expression node aliasing, such as [numerator ::= "0"], or that's what it
is fundamentally, anyway, though syntactic sugar may be provided for this
kind of aliasing.  The routine-external aliasing would involve auto-gen
wrappers, in a manner of speaking, that are like the tuple RENAME operator,
or certainly the semantics are the same.  There would also be syntactic
sugar provided for defining sets of external parameter aliases along with
a routine definition.  To be clear, while synonym aliases for whole
routines don't add routine definitions, parameter aliases do add routines.
3.  Change the normal format for tuple-type and relation-type to look just
like a routine definition, sort of, in that the attribute list is declared
with the same appearance as a parameter list.
For example:
    Dict ::= relation-type (word : Text, definition : Text) key (word)
        where (<boolean-valued-expr>);
4.  Make all functions without exception have exactly 1 actually defined
parameter, which is a tuple-typed positional parameter, so that the
functions' conceptually defined named parameters are actually attributes of
this tuple.  And so all functions will take 1 argument and produce 1
result.  The result type does *not* have to be a tuple (this is important)
and is typically the normal result type the function otherwise has.  We now
will always use a tuple type definition to define a function's fundamental
named parameter list, meaning its parameter names and their declared types.
But an added benefit of the change is that we can define what
*combinations* of named arguments are valid for the function in order for
the function to produce a normal result; a predicate or contract for the
combined argument list is now defined quasi-declaratively as a
boolean-resulting expression in the tuple type constraint.
Theoretically, the parameters' tuple type can now just define all the
allowed public-side aliases itself, or any other variations of parameters
including optional or what-have-you.  But in practice for most functions
we will still just want exactly 1 variant defined, so that the body of the
function only has to deal with 1 variant.
In practice there will exist an automatic RENAME-alike operation that maps
the tuple of a caller with 1 set of argument names to the names that the
called function actually wants, and this would probably be auto-generated
per spot of invoking code for efficiency.
Using tuples like this would enhance polymorphism, as now a lot of the same
logic can be shared, and in particular, dispatch would always be determined
using a single actual tuple parameter; the automatic generated code that
dispatches a virtual call can simply invoke "isa" once per candidate to
find an implementer.  So then the logic becomes a lot more like the
multi-sub/method dispatch logic of Raku, wherein variants of the same
virtual routine can now have different numbers of or names for parameters.
No change to the empty-string-named expr node; it is the function-result.
Inside the routine body, the tuple argument as a whole is like an inner
expr node, in that one can optionally name it (but using : rather than ::=)
and if they don't then a name will be automatically generated, and then for
all practical purposes it would not be explicitly referenceable; regardless
of that naming, all of the tuple's attributes can be referred to using the
special leading-dot syntax, such as ".numerator"; the leading dot syntax is
no longer a shorthand into the "0" or "topic" argument.
Examples:
    function (ResType <-- ArgsType) : ...;
    function (ResType <-- args : ArgsType) : ...;
    function (ResType <-- tuple-type (x : Text, y : Int) where ...) : ...;
    function (ResType <-- x : Text, y : Int) : ...;
Still todo, how to distinguish 2nd from a single named arg in 4th?
Still todo, sugar for auto-declaring auto-nodes from args, eg so we can
just refer to "x" and "y", though the 3rd/4th syntax may just do that
because we declared the tuple type inline.
Tuple selectors need to have special expr node syntax to avoid chicken-egg.
Tuple attribute accessors don't need special syntax, strictly speaking,
because functions don't have to return tuples.
5.  Making procedures also have exactly 1 tuple-typed positional parameter
should theoretically also work, thanks to pseudo-variables, and because
even subject-to-update parameters are always passed values,
a type constraint or virtual dispatcher can include those too.
However, in the general case, we would expect procedures to have *two*
predicates, because there may be values allowed for output that aren't
allowed for input, and vice-versa, in theory.
Related issues are still marking params/args as subject-to-update or not.
Or we could just conceptualize that the single procedure tuple parameter is
*always* subject to update fundamentally, as a function's is read-only, and
any further markings on its attributes, probably only happening if declared
inline, the "&" or absense of said would be more for optimization and
adding of constraints that variables or non-variable exprs are allowed.
Where the 1st and 2nd formats are used, there can also be a separate
declaration list to indicate ro/rw or alias-exprs etc.
I would expect global-params to remain separate from the parameters-tuple.
6.  We should be able to retain existing signature/etc syntax a large part.
7.  See also what Raku does about routine parameters.

* Do away with TREAT as seems to be useless now.  See TTM chat c2011.06.11.

* Recheck my terminology.  Perhaps "consume" is better than "compose" when
referring to what one does with roles/mixins.  See Perl 5.15.3 perlootut.

* Perl 5.16.0 introduces the __SUB__ token, which is a ref to the current
subroutine, so Perl now has a direct counterpart to rtn(), which is nice.

* Bring back, with enhancements, the "transition constraint" support
removed in version 0.140.0 as I changed my mind again on its utility.
As per my comments to the TTM list on 2011 Oct 16, I consider the primary
benefit of transition constraints to protect the explicitly recorded
history of auditing databases from being changed, eg such as with a
"you may only insert" transition constraint.  The transition constraints
can also be used on their own for simpler business rules.
So this restored feature expressly is pure functional / declarative and is
*not* replaced by stimulus-response rules.

* NEXT PRIORITY...
Make the system catalog into something much closer to a *concrete* syntax
tree-like-thing.  See various following TODO items for details.
Mostly do this as its own spec release with minimal dialect/etc/other chgs,
that is, dialect changes to fill in new slots not needed, but anything the
catalog would break could be altered.
Includes:
1.  Get rid of the "scm_" prefix for cat attrs, since we're more formally
storing concrete syntax anyway, so treat those as normally significant.
Also update or remove/move SCM section in Basics.pod since we aren't really
differentiating between "metadata" and normal code anymore.
2.  Add Bool-typed scm_foo cat attr everywhere we have a "name" attr that
says whether the name is user-specified or compiler-generated.  See below.
3.  Refactor the scm_vis_ord and scm_comment attrs as described below.
4.  Refactor the FooSelExprNodeSet types to merge most of them into one
that just stores an ordered list of Name attrs; eg, [rank|vis_ord]:name
pairs; the existing ArySel would be the closest fit.
5.  Add attr for func-invo etc that specifies whether prefix or infix or
postcircumfix etc form was used, where applicable.
So ultimately distinctions of func-invo-alt-syn are recorded in catalog.
6.  Likewise for := op, to say if "x :=foo y" or "x := x foo y" used,
and so on concerning meta-ops, maybe.
7.  Add attr to other expression kinds saying which forms were used,
eg for if-then-else, whether that or ??!! used.  Or for relation literals,
whether ordered or named form used.

* Maybe add Pair and Dict generator types while we're at it; Pair and Dict
have the same binary heading like "(key : Universal, value : Universal)"
where Pair is the tuple type and Dict is the relation type; and "key" is
also defined to be a unary key.
Consider redefining Array to be a subtype of Dict where the key is
a nonnegative integer and its values are dense; in other words, the new
Array is the same as the old one but renaming "index" to "key".
Then you can use a lot of the same operators on Array and Dict like Raku.
Theoretically we could also make Bag a subtype of Dict where its "value"
is a positive integer; so then "key" is its payload rather than "value";
this then would also be like Raku and it might be worth it.
But if we do this then we'll have to relook at the set membership/etc ops.
Maybe while we're at it, rename Set's attribute to "key" for consistency.
- Note: The Postgres hstore type may be relevant for a nested one of these,
where I think the keys and values are all text.
- Note: The "hyper" meta-op would be particularly for Dict then, such that
when we say "foo >>op<< bar", then foo/bar are both Dict and the result is
also a Dict, where "key" match the "key" of both inputs and "value" is the
result of applying "op" pairwise to the "value" of foo/bar.  This would
just work for Array and Bag ostensibly.  What to do when
"foo^{key} != bar^{key}" is an open question though; the simplest answer is
that the result just has the set-intersection of the input keys, like a
join would produce; or we might want to define some "outer" version?.

* NEXT PRIORITY...
Reduce the general options for concrete value literals to have just the
simple ones.  For any given "x:y:z", remove all "y" but for Scalar where it
is necessary; people can wrap a literal in an explicit TREAT assertion/etc
otherwise if they really want to.  Also remove all "x" where possible, so
just the plain "z" is the only option, in general.  So then, to write an
integer/rat/text literal, the only option then is to say "42/3.25/'hello'";
you can't say "Int:42/Rat:3.25/Text:'hello'" any more.  So cascade these
simplifications, and we also free up the ":" mostly for other uses.  This
particularly applies to Plain_Text, but we'll also simply the Perl-STDs where
possible, which is easier in Raku.
Unlike "[$|%|@]:..." for generic value literals, only %|@ without the colon
are in use, I believe, as prefix operators, meaning cast-tuple-as-relation
and vice-versa, but there is no colon-less $ prefix in use nor does it make
sense for any similar purpose.
So, use $ for name literals, that is, "$foo" means "Name:foo"
and then say "$$foo" means "NameChain:foo" (doubleup for chain).
And then we're a long way towards being able to ditch the postcircumfix
syntaxes for eg projection since, say "r keep {$foo,$bar}" is terse enough.
Then come up with something for rename, maybe a set of name-pair literals.
After this, keep postcircumfixes rare and common, like for array elements.
In fact, we could just say foo[x] and bar{x} are then array/dict lookups.
Or the dotted forms are elem lookups and no dots are for slices, like Perl.
And then "<expr>.attr" is then its own thing rather than being a shorthand
for "<expr>.{attr}" although it may still be a function shorthand.

* Consider having unary virtual functions for some common kinds of casts,
such as "?" for boolean, "+" for number, "~" for char str.  This terse
syntax would always deal in just base-10 for numbers-with-char.  As for
whether to produce an Integer or Rational or etc from a string, the parsing
rules would be like for Plain_Text, eg, presence or absense of a radix point
or a division/fraction slash, etc.
Casting to the same type produces the same value as the input.
The functions might be transitive, or whatever the term, so eg saying
"~somebool" produces the same result as "~+somebool" and so on.
Likely booleans would translate to integer 0|1 or string '0','1'.
Likewise in reverse, any number equal to zero, or any string that casts as
a number equal to zero, would become bool false, and otherwise bool true,
at least speaking for strings that successfully numify.
So 0 and 0.0 and '0' and '0.0' are false, and other numifyable are true.
Attempts to numify a string that isn't a number will throw an exception.
Attempts to boolify any string will succeed, where the above zeros plus
the empty string are false, and other strings are true; to be specific, any
string that would successfully numify would use numeric rules for truth,
and any string that doesn't successfully numify would be true unless empty.
ON THE OTHER HAND, it would probably be much better and more predictable
and extensible to forget about transitivity, and so, boolifying a string
will only produce false for the empty string and true for all other strings
and to get the '0'=false behaviour you have to say ?+str; MUCH BETTER.
Unlike Raku, the prefix !foo will *not* be system-overloaded to mean
!?foo as doing so would likely introd accidental logic errors of implicit
casting when one considers ! is widely used when just bool inputs expected.
Similarly, unlike Raku, no other ops will implicitly cast like infix ~,+.
If this is adopted, "so" will therefore be virt rather than a "not" mirror,
though for the boolean composer it becomes the same as otherwise.
ON THE OTHER HAND, since there is so little standardization and wildly
different expectations on what strings are considered true or false, it
would seem best for strings to *not* provide system-defined terse bool
casting at all, and instead force people to be explicit, which would
typically mean either going by way of a number, either from a numify cast
of the string or a length-test of the string, or some explicit boolean test
like comparing the string against a list of values.  Similarly, a boolify
of many other things like collection values is best done by way of some
intermediary like a number such as a length or elements count test.
So eg the so/not-empty prefix for a relation/array is then ?#r or !#r for
consistency with "? #r" or "! ? #r", and #?r or #!r is probably invalid.

* Grammar spec related to separating/trailing chars like ',' or ';' could
look like this:
  <foolist> ::=
    <open>
      [<item>? <sep>]* <item>?
    <close>
... which says every item may have a trailing separator, and every item
must have a trailing separator except the last/only one, and any item may
have a leading separator or separators may appear several in a row; the
semantics are that extra separators do not introd any implicit extra items.
The above would be for a 0..n list; here is a 1..n list:
  <foolist> ::=
    <open>
      [[<item>? <sep>]+ | <item>]
    <close>
... but there are probably cleaner versions of both.

* The SQL-92 Standard CREATE ASSERTION is like a generalized db-level CHECK
constraint but few SQL DBMSs support it.  Note this in the spec.  RDB from
DEC had it, but they got bought by Oracle when DEC went out of business.

* FYI, PgOpenCL exists, run intensive db tasks in the GPU.

* See http://vitessedata.com for beta (2014 Oct 28) of Postgres fork that
uses LLVM as backend for better performance of some things.

* See http://people.umass.edu/klement/russell-imp.html for free restored
copy 1920ish "Bertrand Russell: Introduction to Mathematical Philosophy".

* Note that Perl module PQL::Cache released at the end of 2014, which has
some semblance to Set::Relation but has DBIx::Class like query API and it
intentionally has no non-core Perl dependencies.

* ALSO...
Get rid of the Set.new() options in Raku and generally update the
Perl-STDs to use just Array/Seq/arrayref and mostly not set/bag/hash/etc,
partly for code brevity but particularly to preserve the visual order of
elements from source to catalog and back again.  Also add scm_vis_ord to
the catalog or change some catalog types to record order.

* Note that Perl 5.15.4+ fleshes out / completes the support for having
Unicode in identifiers, to treat them properly, such as not ignore the
UTF-8 flag; it also fleshes out support for the null character "\0" so that
eg regexes with them work properly.  See perl5154delta.

* Also add to Perl-STD explicit support for some more Perl modules whose
objects we would implicitly treat as built-in scalar values.  We already
have BigInt, BigRat etc, but add Ingy's "boolean" (which is expressly
intended for cross-language interchange in its POD) and Juerd's "BLOB".

* Maybe also but probably not yet be concerned w comments in Perl-STD/etc.

* NEXT PRIORITY...
Reformat all declarations, materials/subdepots particularly, to be of the
format "name ::= kind ..." rather than "kind name ...".  Similarly, we may
be able to just nix the "subdepot" keyword so a subdepot is then declared
as just "foo ::= {...}", and a material as "foo ::= kind ...".
So the name on the left of the ::= is no longer part of the material node
itself but rather is part of the larger thing into which the material node
is composed; the material node is now just eg ['function', <payload>].
Material nodes themselves just declare anonymous entities.

* Rename the "subdepot" concept to "schema", even if this term is broad
enough to include what Oracle calls "package" and not just SQL "schema".

* Example of function in Perl with context ...
The FunctionSet tuple:
    ['cube','opt comment of FunctionSet',<Function>]
The Function tuple:
    ['opt comment of Function',<heading>,<body>]
... and so on.  The comments are first so they're like leading comments
as is common with whole-routine definitions and such, and they tend to be
terser besides.

* ALSO...
Change function bodies from {...} to (...).  And make "..." expr/stmt kind.

* Likewise make a code comment a stmt/expr kind or otherwise provide for
specifying where it goes visually in code in a statement position.
Consider reworking scm_vis_ord to be external for some things it describes,
eg mapping an order to a declared name.  In fact, if this is done, then it
becomes much easier to add/remove/reorder code pieces because their
sequence numbers are stored separately and so code diffs on the system
catalog itself may not show much in the way of spurrious diffs, especially
if the mapping is simply an array_of.Name.
Consider pulling out code comments in a similar fashion, just putting them
as their own named (as stmts/etc are named) code bits, which are then
associated by name with other code bits, and are listed in the vis-ord too.
In fact, then where comments are physically visible and what things they
are semantically connected to are then not joined at the hip.
The comment names are optionally user-specifiable too, like with statemnts.
For example:
    cmt_on_x ::= C:'This roxors!';
... or:
    comment cmt_on_x ::= C:'This roxors!';
... and other details still to fill in like saying what it applies to.
Maybe a new infix-op-bind-like syntax will fit the bill for association.
Also make blank code lines or visual dividing lines recordable in syscat.
See also Postgres "COMMENT ON ..." syntax.

* Note: Postgres $$...$$ delimiters are actually general string delimiters
and can be used anywhere '...' are, not just function definitions.  They
also generalize to heredocs, eg, $foo$...$foo$ so maybe Plain_Text should
support this kind of thing too, maybe.  For that matter, I should have a
function that takes a string as input and outputs some delimiter suitable
for quoting it without escapes because it doesn't occur in the string.
This, as well as other escaping functions, would be used by a code gen.

* Don't use any quote-like characters for delimiting code comments after
all, but instead use nonidentical multi-symbolic-character delimiting
tokens, such as a "/* */" pair as in C/etc.  The tokens would be ones not
valid as normal operators, and they would be visually "heavy" rather than
"light" such as quote-like string delimiters are.  To make it easier to
comment-out code blocks that may already contain comments or have literal
strings that contain comment delimiters, the comment delimiters may be
extended to an arbitrary length using repetition, where both ends must
match.  Raku is used as inspiration for this.  We would presumably use a
bracketing character for the extender, for example "/*{ }*/" or "/*{{ }}*/"
etc.  This means there is NO ESCAPING in code comments for the comments.
On the other hand, maybe some certain escaping is still needed such as for
whitespace like linebreaks and indentation, as with text/etc literals, to
control exactly what is captured, or we have a version of comment for each.
Apparently both Tutorial D and SQL support /* */ block comments too.
All Muldis D comments are block comments on purpose; there are no line
comments, because of a desire to make it line-ending-chars-generic.

* Invent some lightweight syntax for the general case of infix operators,
whose use is optional for system-defined infixes, so users can define infix
operators or invoke their routines infix, and the parser will interpret the
infix-using code correctly in a context-free environment where the parser
doesn't have access to other user-defined-stuff definitions, mainly telling
routines apart from variables, since the latter don't have special markings
on purpose.  This syntax will need to be something that users or Muldis D
are unlikely to want to use for some other purpose.  By "lightweight" we
mean something that is visually small but also short to type.
One possibly best choice is the GRAVE ACCENT; for example:
    x `foo` y `bar` z
... or:
    [`foo`] ...
... for user-def reduction, which couldn't be "[foo()]" for arry confusion.
But then we'd need something else for code comments, though code
comments are probably much more amenable to having double-delimiters;
or better yet, lets use single-quotes for comments now, same as for Text or
Blob, and have some kind of prefix like with a Blob to know its a Internal_Comment.
We could similarly use that for user-defined prefix operators sans parens,
where context (terms-in-a-row/etc) could differentiate from infix, although
the value of this is lower relative to foo() compared with supp infix.
We would probably never support unary postfix or circumfix operators,
save the few built-in special syntaxes.
Lets just say that there is no part of the catalog for declaring routines
as infix but that rather any user-defined (or system-defined routine can be
used both "foo(x,y) and "x `foo` y" iff it has exactly 2 positional
parameters that are mandatory (names are "0" and "1") and so any other,
optional, parameters may only be caller-specified in the "foo(x,y)" form.
The system catalog would specify if that a routine *invocation* is foo() or
infix etc but the routine *definition* knows no difference; both forms use
exactly the same choices of routine names, whether alpha or symbolic.

* A tangent to the above is that where appropriate we can take any function
of 3+ parameters which conceivably would be made easier to use in a
ternary+ infix format and make a set of dyadic functions that break it
down, kind of like how the creation of a distinct "assuming" function
allowed numerous 3-arg functions to generalize into a pair of 2-arg.

* Note, apparently Haskell has no prefix operators except for unary minus,
and normally all of its infix operators are symbolic where normally all
alpha operators must be used in "foo()" notation.  Also, you can take a
foo() operator and use it infix using backquotes, eg "x `foo` y", meaning
my thought of doing this has a precedent.  Haskell says you can take a
symbolic infix and make it prefix using parens, eg, "x + y" -> "(+) x y".

* Plain_Text Syntax/parsing cluster:
- A general rule is, the parser has zero knowledge of what specific
operators or vars/etc exist, and must be able to derive a concrete syntax
tree using just basic grammatical knowledge, such as knowing the difference
between whitespace and alphanum and symbolic and quoting and bracketing
characters, and so the parsing rules are based on those restrictions.
In particular, the behaviour of a parser or how a piece of code is parsed
does not vary depending on what user-defined entities are declared at the
time, which should aid on security and predictability and simplicity.
There are only a small amount of disambiguating rules that say certain
tokens are system-defined operators, so otherwise no knowledge.
The hardest work then, in the face of Unicode, is specifically identifying
what characters count as alpha or symbolic; eg what are Greek letters?
Generally speaking we would whitelist; any characters or groups we don't
whitelist into a particular category are disallowed from appearing outside
of a quoted context.
- Remove dash from bareword ident, leaving just alphanum and underscore;
people can quote names with dashes when they want them.
- There are no postfix operators; eg, ++ is prefix.
- Namespace-qualified/dotty operator calls must be in foo() syntax; plain
prefix or infix operator calls must be unqualified.
- Whitespace affects parsing.  Any consecutive 2 alphanum or 2 symbolic
tokens must have intervening whitespace.  Quoting '"` or bracketing (){}[]
characters count as neither alphanum nor symbolic in general.  Whitespace
may be omitted between any 2 of these distinct things in general: alphanum
tokens, symbolic tokens, quoting or bracketing characters.
- Default to treating any bareword run of 1+ alphanums as a var name or
foo(), treat any bareword run of 1+ symbolics as a non-foo() operator.
- All bareword symbolic tokens are plain infix/prefix op calls.
- All grave-accent-delimited (`) tokens are plain infix/prefix op calls.
- All double-quoted (") tokens are var/etc references or foo() calls.
- All single-quoted (') tokens are string value literals.
- All namespace-qualified/dotty tokens are var/etc refs for foo().
- All tokens followed by an open-paren with no ws between are foo() calls
if the token is bareword alphanum and a plain prefix op if symbolic.
- Any code enclosed by a pair of bracketing chars is treated as a var/etc,
except when they contain only a symbolic token and no whitespace, in which
case they instead are taken as an extension of said symbolic token, eg [+].
- When 2 or more plain op calls appear consecutively, all but the first are
taken as prefix ops; the first op is taken as an infix op if it follows a
var/etc and a prefix op if it doesn't.  Eg "x + -y".
- When a symbolic token appears between 2 alphanum tokens, the symbolic
token is taken as a plain prefix op if there is no whitespace between it
and the following alphanum and there is whitespace between it and the
leading alphanum, while it is treated as plain infix if there either is or
is not whitespace on both sides, or there is whitespace just following it.
- When disambiguation isn't present, sequences of bareword alphanums are
interpreted as alternations of var/etc and plain infix op calls, always
ending with the former, like "var op var op var" or "op var op var".
- Certain bareword tokens are special-cased to always be interpreted as
prefix ops, namely {not,so} (there might be more).
- All operators with more than 2 parameters must be called in foo() syntax.
- All prefix ops have 1 param named "0", all infix just 2, named {"0","1"}.
- All bareword prefix ops are symbolic except for a small number of special
cases such as "not" and "so"; all other bareword alphanum ops are infix.
- The only infix syntax taking more than 2 inputs is special syntax with
its own kinds of parse nodes, not op calls, such as ??!! and if-then-else.

* We have just a small number of precedence levels, here from highest to
lowest, loosely speaking:
  - base literals or selectors or delimited whatevers or foo() etc
    - for foo() there must be zero whitespace (or unspace) before opening paren
    - includes "." of tup.attr etc since "." changes how its RHS is parsed, its not a normal infix
  - why-binding infix ::?= (binds tighter than ::=)
  - what-expr-binding infix ::= (binds to innermost expression, use parens to force looser)
  - general symbolic prefix (only bareword symbolic)
  - general symbolic infix (only bareword symbolic)
  - general alpha/other prefix (bareword alpha or tilde-quoted)
  - general alpha/other infix (bareword alpha or tilde-quoted)
  - short-circuiting / ??!! / if-then-else / given-when-def
  - assignment := (both base statement and any derived meta-op eg :=+)
  - statement-binding infix ::&= (because plain ::= binds to lvalue expr if used there)
Note that tilde-quoting should have an impact on precedence, for principle
of least surprise, otherwise users have to guess if something is symbolic
or not to determine precedence.  Also security implications
for lookalike graphemes.

* Pegex grammar should work at some level midway between a pure tokenizer
and the resulting AST.  It should form trees where easy to do without
non-trivial lookahead.  In particular, sequences of barewords and etc where
interpretation of what is a pre/infix op and what is a var/expr/etc depends
on the number of barewords and what they're next to, should be output as a
flat list and no attempt to form a tree in the grammar.  Also, the Pegex
grammar perhaps should capture everything, including ws, just in case.

* IDEA: Add a concept kind of like a distinct depot/package catalog but it
is system-generated and not directly editable by the user; in here would
live copies of all of the canonical type definitions, especially scalar
types, from both the system and user namespaces.  The copies in this
package are the ones cited by a ScalarWP value in the low-level type system
as being the scalar type name.  When there is just one normal copy of the
type def, it is more of a redirection.  When multiple user depots declare a
copy of a type and then the mounting commands further specify that they are
to be considered the same data types, then just a single copy exists in the
new sys-gen package for both, rather than one for each.  This has various
benefits like saving a chicken and egg problem concerning say transient
values in the system when various depots that may declare types for it can
come and go.  It means the low level type system type name for an
(immutable) ScalarWP value never has to be changed for the life of the DBMS
process no matter what happens with the depots user types are declared in
or as copies come and go.  For that matter, we can just identify this copy
of the type name with a plain integer, this separately being mapped to
namechains for the various normal copies it is aliased with, so we get
memory efficiency too.  This all probably has wider design implications or
can inspire other changes.

* Consider the provision of a limited search path functionality for
user-defined functions and types so that they could be invoked tersely or
unqualified like builtins, particularly so reasonable infix syntax or
singleton types could be supported.  The fully-qualified invocation of such
a floating name would have another special name token, in the same way that
"par" and "nlx" etc are special.  Maybe "shp" (SearcH Path).  So, in the
system catalog, all unqualified names "foo" become explicit shp-prefixed
names "shp.foo", which includes references to all system-defined routines
as typically used.  On a tangent, we can revise how NameChain are expanded
in the system catalog, due to wanting to make it as close to the user's raw
syntax as possible, so eg not expanding unqualified to sys.*; if necessary
for ease of scanning the catalog, we might store 2 versions of a namechain
together, where one is expanded and the other isn't.  A "shp.foo" would be
interpreted as follows: 1. first all system-defined possibilities would be
explored and exhausted; 2. then user-defined possibilities that are
siblings of the invoker or siblings of ancestors from closest to farthest,
and then failure.  To be specific, in a given "shp.foo.bar.baz", just the
"foo" is looked for as siblings of ancestors, and then the first one that
is found would then be dug into as usual, to succeed or fail, same as if
one had said "par" instead.
A "shp" can be included along with other special chain
elements so to further customize the search path so that say #2 can be
followed relative to a specific place in the namespace rather than from
where it is actually invoked; for example "fed.lib.db.foo.bar.shp.baz" but
that this example wouldn't actually work, so details still to figure out;
on the other hand, we could say that this last feature would be unnecessary
because the existing APMaterialNCSelExprNode feature would make stuff work.
Or possibly APMaterialNCSelExprNode/etc could be employed with an expanded
role to translate shp.foo to abs-paths which then could be used as usual.
And so, the parser doesn't have to resolve paths so much from unqualified
to qualified, this functionality being pushed back, but instead the parser
still has to resolve what a reference is to, whether a function or a type
or a data-entity etc so users will still have to provide enough clarity in
their syntax to disambiguate this.

* Note that Postgres and MySQL both support an UPDATE statement extension
where you can mention multiple tables besides those being updated, where
semantics are that all of the tables are joined with the one being updated
and so you can use data from other tables in the SET clause.
This is a *very* useful feature and Muldis D should have an analogy,
or this feature would be helpful in implementing Muldis D features
like certain kinds of relational assignment involving self joined with foo.
As of Postgres 9.1 the UPDATE also supports the WITH clause.

* Note, http://facility9.com/2011/12/ten-reasons-postgresql-is-better-than-sql-server/
which gives some interesting details or explanations of Postgres (per 9.1)
features, including use cases for writeable CTEs, and compares with SQL
Server; a response also lists some of Postgres' weaknesses in comparison.
Also, re unlogged tables, and serializable transactions.

* Note, see http://en.wikipedia.org/wiki/Vector_clock and such things.

* Note that in Postgres, db encoding SQL_ASCII just means text has no
encoding and Postgres just treats it as bytes, so any values can be stored.
Other encoding choices are basically constraints to ensure your
data is valid according to that encoding.

* On 2011 Oct 17 I made a comment on Andrew Dunstan's blog that in the
context of foreign keys, making changes to parent and child
tables simultaneously in a single statement is much better than trying to
determine correct order of operations.  In response Andrew said "That might
be possible in 9.1 with writeable CTEs, but I happen to be on 9.0 with this
client, and the constraints are not deferrable."

* Note that Postgres databases default to SQL_ASCII (7-bit) encoding when
an explicit encoding (such as UTF8) is not specified upon their creation.
"The SQL_ASCII setting behaves considerably differently from the other
settings. When the server character set is SQL_ASCII, the server interprets
byte values 0-127 according to the ASCII standard, while byte values
128-255 are taken as uninterpreted characters. No encoding conversion will
be done when the setting is SQL_ASCII. Thus, this setting is not so much a
declaration that a specific encoding is in use, as a declaration of
ignorance about the encoding. In most cases, if you are working with any
non-ASCII data, it is unwise to use the SQL_ASCII setting because
PostgreSQL will be unable to help you by converting or validating non-ASCII
characters."

* ALSO...
Use colons to separate any kind of heading/body pairs, both materials and
values.  Take Relation now "@:[...]:{...}" as example to follow.
Also, routines now "function (...): (...)" or "updater (...): {...}" or
"procedure (...): [...]"; this for routines is inspired by Python.  This
then opens the door for routine body bounding chars to be opt sometimes,
and makes clearer where a heading ends and a body starts when there are
various extra heading clauses such as is-x or implements x.  Also consider
using ":" in other places where pairs are, maybe freeing up => for
something more specific; eg Python uses ":" in dicts rather than =>; or do
the opposite; keep "=>" for named param/attr/etc lists and use the ":" for
things like Bag literals or generic dicts that are binary relations ... use
one for atvl:atvl (bags/dicts), other for atnm:atvl (tuples, arg-lists).
Also consider using "::" for something, maybe type conversion, as Pg does.
Keep "::=" as for explicitly associating names with what they are naming.
DESPITE WHAT MNEUMONICS SAYS, lets use the : for name/name and value/value
pairs within delimiters also (unless <- still better for rename) and so
maybe the only place => is used is as a binary infix op to construct Pair
tuples, same as .. is an infix op to construct intervals maybe I guess.

* Have ";" as separator (opt lead or term) for both statements/vars/exprs
etc as well as whole materials.  This also comes together nicely for the
simpler routines that don't need to have bounders because they are just
single statements or expressions, for example:
    cube ::= function (Int <-- topic : Int) :
        topic ^ 3;
... and that's it.

* Remove the "var" and "attr" keywords or make them optional noisewords.
Simply having "foo : bar" in a procedure statement position should be
enough to know it is a variable declaration.
Likewise, "foo : bar" in a sca/tup/rel typedef can be known an attr def.
Then, other things can gain optional noisewords, such as "result" before
the type in function sigs, or "param" before a param in routine sigs,
or "expr" or "stmt" optionally before those things in a routine, etc.

* Update system catalog, if necessary, to support specifying where a named
expression, or a variable declaration, lives visually in a statement list.

* Consider dropping the special support for dot-accessors as their own
expression node kinds in the system catalog, meaning AccExprNodeSet, and
code can just use an ordinary function invocation on Tuple.attr() instead;
or that node kind can be downgraded to just be doing aliasing, like when
you write "a ::= b ::= c" (target is then just a Name, not a NameChain).
This change would improve internal language consistency.
As part of this change, the formal syntax "t.{x}" goes away so just "t.x"
remains.  Then "." becomes an ordinary dyadic infix operator in the context
of referring to a data entity, same as "."(t,x) where "." synonyms "attr".
But when referring to a routine/type the whole dotted name is a NameChain.
But if "." is just an infix operator, then "t.x" means
"attr(<value-of-t>,<value-of-x>)" so things aren't actually that simple.
Regardless, if there may be separate "." for relations or scalars then
"t.x" may actually point to a virtual op, only func forms disambiguate.

* NEXT PRIORITY...
Add support for material and parameter synonyms.  And change what params
any positional arguments implicitly go with from topic|other to 0|1|...
But don't actually change any routines/params until later, except adding
0|1 to all topic|other.

* Update the array-specific postcircumfix concrete syntaxes to make them
more generic such that the array index/es (what's inside the "[]") may be
any arbitrary value expression rather than having to be an integer or
interval literal.  But if nothing else changes, this means the slice will
have to be spelled like "ary[{x..y}]" rather than "ary[x..y]", but
individual element access like "ary.[x]" will still work.  But now you can
actually have the x,y variables rather than those having to be constants.

* Consider taking a more Raku like approach by turning ".." and its 3
friends into infix dyadic functions that take endpoint values and result in
interval values.  Then the surrounding curly braces are no longer needed,
and you can once again say "ary[x..y]".  Note that if ppl still want/need
delimiters for an interval, they can always use parens, like "(x..y)".
If we also redefine an MPInterval to be a set_of.SPInterval, then any
{x..y} would unambiguously mean either a set or MPInterval, but we may then
lose the shorthand "x" meaning "x..x", but this could be ok tradeoff.

* Consider also making the likes of "," and "=>" into dyadic functions
along the lines of Raku, though this would have further consequences.

* Add boolean monadic function "so"/"?" which returns its argument;
it serves as a useful noiseword in code, helping parity with "not"/"!".

* Have a Boolish mixin-union type which Boolean or Bit etc compose.
By having "so" and "not" in this union type, users can compose Boolish
into ostensibly less-Boolean-like types like numbers or strings and so they
can define implementations for "so" and "not" that are like the Perl
operators when one treats numbers or strings like booleans.
But the system-defined numeric and string types *won't* compose Boolish/etc
as we prefer stronger typing or more explicitness by default.
The details of this will require more thought, maybe more mixin types.

* Account for that we can't generally have a virtual N-adic operator that
accepts the empty set as input, where that is implemented by type-specific
N-adic operators that accept the empty set as input, because it wouldn't
know which type-specific N-adic to dispatch to for the empty set.
Instead, make the type-specific N-adic virtual with 2 implementers, one
taking just the empty set and one taking nonempty sets; the type-specific
0..N virtual will not implement the type-generic 0..N; rather the
type-specific 1..N will implement both 0..N virtuals directly; the
type-specific 0 will only implement the type-specific 0..N; there would
also be, as applicable, a type-generic 0 implementing just the generic
0..N.  Eg, so we have just Num-0..N, Num-0, Int-0..N, Int-0, Int-1..N at
least for ops that have identity values, and no -0 where there aren't.

* Consider creating an analogy to virtual routines that is to existing
virtual routines what domain-union types are to mixin-union types.  That
is, define a kind of virtual that declares what other routines implement
it, and so is not user-extensible.  This is essentially an alternative way
for users to write wrappers for related routines that dispatch on argument
types.  An example is we can have separate "ungroup" and "unwrap" functions
that take only nonempty relations vs those that take empty ones too; the
nonempty-only ones don't need the extra parameter to say what attributes to
extend with when the relation is empty.  Or maybe those should be normal
functions and better examples for the new kind of virtual will come around.

* Don't worry about declaring identity values somehow attached to dyadic
function definitions in the catalog; instead, use the virtuals mechanism
we have to just declare the triple {0..N,0,1..N}, um, or something.

* Make all system-defined functions generally return special values on
failure rather than throw exceptions.  For example, make division return
the NaN.DivByZero singleton and so on.  But functions whose whole role is
to assert, such as treat(), would still throw actual exceptions, and some
other kinds of problems may be better suited to thrown exceptions.  This
gives users the choice to either explicitly accept such situations or not.
If users don't handle such situations, then often-times they will
immediately get a type constraint violation exception (which is an actual
exception, not a special value), and the description of the exception
message is still informative enough, eg
"Nan.DivByZero isn't a value of type Integer".  This works because typical
system-defined type-specific functions will not accept special values as
input, even when they might return them, so in nested expressions we still
get exceptions in about the same places for the same reasons.
But now the Muldis D analogy of "@foo = map { $_.x / $.y } @ints" in Perl
won't itself throw an exception, if @foo is a Universal-array, but it will
throw an exception if @foo is an just-Integer-array.
One could say that a significant portion of the exception throw/catch
system has been made redundant by the type system.  For example, a routine
signature formally declares exception-like conditions as its result types.
I suppose one might say this could lead to action-at-a-distance problems
such as what exceptions are meant to help prevent in the first place,
but when code is written with fairly narrow declared types, then errors
would not tend to get very far before detection, I would think.
Now subject-to-update parameters of procedures can be tricker, because what
if the param decl type is Text|IOErr but the variable argument is a
just-Text var?  Is that something we would expect to fail at compile time
or just optionally at runtime only if a IOErr is to be returned?
Make sure *don't* call the special values "exceptions"; use something else?
See also the list of IEEE float special values, and Mathematica/etc such as
http://mathworld.wolfram.com/Indeterminate.html / etc.

* Support at least a base level of controlled override-overloading with
virtual operators, meaning where multiple impls overlap in their domains.
Normally, if several implementation signatures match the arguments to the
virtual, it is undefined which one is called, and in general it is onerous
to determine which one is more "specific" than another to pick that.
The base proposal is that the virtual operator itself can name, or be its
own, default implementor, where this default is invoked if all of the other
implementors don't match the arguments, rather than there being a type
constraint violation.  Actually, best for a virtual to not be its own
default, so users are able to always invoke the default implementation
directly without worry of it being overridden by something.
A natural extension to this is that in any cases where an implementer of a
virtual is itself a virtual, it can do likewise, providing a default.
In this way, on an operator-by-operator basis, we can support a hierarchy
of sorts like in a multiple-inheritence OO system.
This idea still needs thought to flesh out details of course.

* Demote the numeric operators that are more statistics-oriented from the
language core into a new Statistics extension or some such.  Specifically
this means these 5 in [Numeric|Rational|Integer]: range, frac_mean, median,
frac_mean_of_median, mode; and these 2 in Integer: whole_mean,
whole_mean_of_median.  Also, this "mean" is "arithmetic mean" (division of
sum); there is also "geometric mean" (root of product), etc.  After the
demotion, this set of ops can be changed or expanded to be something more
appropriate for statistical applications; some yet-missing SQL-standard
functions like pop-etc can then come in also.  Now these core-removed
functions are just shorthands for not-too-complicated expressions that
users can define for themselves with core ops, so they're not really
missing anything important if they only get the core.
For example, the current (arithmetic) mean is just:
    arith_mean ::= function (Rat <-- topic : bag_of.Rat)
        ([+]topic / #+topic)
... and geometric mean is something like:
    geom_mean ::= function (PRat <-- topic : bag_of.PRat)
        ([*]topic ** (0 - #+topic))
... but any vers in the dedic Statistics could be impl more efficiently.

* Note that general case of "median" is "quantile" (median is 2-quantile).

* Drop special entity name embedded support for inline type declarations
like "foobag : bag_of.Foo"/etc; instead, this syntax is demoted to a
dialect-specific thing that is just sugar for something like "foobag :
relation-type Bar { attr value : Foo, attr count : PInt, primary-key {
value } }".  That way, we can always point to a specific material that
actually exists when asked what is the declared type of "foobag", and also
we are psychologically more free to just declare things as relation types
anyway, and the added flexibility that comes with that, such as in the
definition of the system catalog itself, and also then the concept of an
entity name chain is no longer overloaded.

* Generalize the Set/Array/Bag/Maybe-specific operators so that: 1. the
names of the value/index/count attributes can be specified with arguments
(that are optional, and default to the current ones if not given); 2. they
work with relations of arbitrary degree.  For example, merge the Counted
extension into Bag and call it Counted, and generalize Array into Ranked
("Ordered" is already taken and best left as is) which also absorbs the
ranking and quota functions from Relation.pod, and generalize Set into
Relation.  The Counted|Bag is then any 1+ degree relation with a
positive-integer typed attribute C that has a key (or superkey) on all of
the attributes except for C; it is treated as special by the functions,
which are analogies to general relational functions that work as normal on
all attributes but C and merge C.  The Array|Ranked is then any 1+ degree
relation with a nonnegative-integer typed attribute I that has a key on I
and is further constrained that "max(r{I})+1 = #r"; I is treated as special
by the functions.  The Maybe is then any relation with a nullary key.  With
these generalizations, some concrete syntax like .[N] will just compile
into special cases such as assuming certain special attribute names, and
you can use the foo() syntax when that isn't the case.  After these
generalizations, some Counted|Array|Maybe|etc functions can be core and
others can be pushed into extensions, as is appropriate.  After these
generalizations, we may or may not still have named Array|Bag|etc types,
which will probably keep their definitions, as special cases of the
generalized where the attribute names match the canonical ones.  Also
rename "index" to "rank" in Array perhaps.  After the
generalizations, the distinct usefulness of Set would decrease somewhat.
Note: For a generalization of Maybe, consider the Zoo name, inspired by
Database Explorations that discusses MD's canonical missing info solution,
or alternately call it C01 in the spirit of D0C0/D0C1/D0.
Still in question is what if anything to change about [S|M]PInterval/etc.

* Consider removing MPInterval as a s-d type and rename SP to "Interval";
then, either one can just use "Interval" as a "Set" element to get the same
effect, or a relation over "Interval" can at least be demoted from core.

* Add official support for functions/expressions to be able to do some
things that they otherwise couldn't, such as have side-effects or be
quasi-non-deterministic.  To be specific, add support for side-effects that
occur external to the current in-DBMS process, such as output via some
side-channel like STDERR or a message queue, which can be used for
debugging a function.  But any such functionality can't directly affect the
current process, and in particular it can't affect the
function/expression's result value.  On the other hand, it is acceptable
for something to cause the function/expression to abort with a thrown
exception, since this isn't changing the result value.  There should be
metadata for any function which does or might do something like this, to
declare the fact.  In addition, we could support a limited form of
non-determinism, such as allowing a rand() or now() function that does
affect the calling function/expression's result, but that this is
constrained to be mutually deterministic within the whole of a single
Muldis D multi-update-statement.  That is, given the same arguments (or
none), now() would always return the same value within a
multi-update-statement, and might only change between different
multi-update-statements, and rand() likewise.  This might also give some
support for partial-sort functions, as long as they are consistent within
multiple calls in the same multi-update-statement.  Once again, such things
would need to be tagged with metadata.  Normal deterministic functions
always have the same result no matter how far apart.

* Make autonomous transactions / in-DBMS processes not so much startable
directly by a process but rather that the kernal/etc process always does it
directly and any other process asks to have such done by sending a message
to the kernal.  Similarly, DBMS-clients just become message passers, and
they start a process the same way as internally, by sending a message to
the kernal/etc to please call this procedure for me, and the result to the
client is also a message.  This also generalizes the stateful/stateless
thing and streaming/cursor or not thing.  Now also tied into this is
stimulus-response-rules, in that all stimuli are messages.  The kernal can
also initiate messages, such as this depot did mount, or whatever.

* Note that Oracle's autonomous transaction support looks like this:
    create procedure foo as pragma autonomous_transaction; begin ... end;

* Quoth http://ledgersmbdev.blogspot.ca/2012/09/or-modelling-interlude-postgresql-vs.html :
"A simple description of the difference [between MySQL and Postgres] is:
MySQL is what you get when application developers build an RDBMS.
PostgreSQL is what you get when database developers build an application
development platform."

* For real work projects in the short term where one would conceivably use
a Postgres enum, I should just use a "text check value in ('foo',...)"
instead, as it is much easier to manage/change those types.  For example,
if we want to temporarily add extra enum values outside normal range for
testing so the system ignores those values, eg status_code='WASQ'.

* Consider relaxing the restriction of how much of a depot must be defined
just in terms of itself.  So, for example, only a depot's data types (and
dbvar) must be defined wholly internally to the depot.  But any routines in
a depot may invoke routines outside of the depot if the former aren't used
in the definition of a data type or dbvar.

* Consider adding some way of generating a type specification from a value
of that type and consider having something like a system catalog which
describes the actual database value rather than a prescribed database type,
such as to help introspection of a database whose declared type is just
'Database'.  The MST thing of TTM may tie into this.
See also how the "Pick" DBMS works, or something.

* Add a scm_foo to the system catalog next to any place that declares a
DBMS entity name, particularly an expr/var/material, to indicate whether
the declared name is considered explicitly user-specified or parser-gen.
There may be more than 2 possible values (making this an enum rather than a
Bool) that relate, say, to distinguishing explicitly named but inlined
items versus explicitly named and not inlined items.  The sys-cat might
restrict based on this such that it doesn't allow certain references to
entities whose names are marked parser-generated, because any generated
source code would have to make the references visible.  A related
implication is that any entity names marked as generated are not sacred and
are free to be automatically renamed by different catalog-updating actions
such as source code optimizers.  Maybe also have something to distinguish
things declared in positional format so "0"=> etc don't appear, maybe.

* Numeric updates ...
See http://archive.adaic.com/standards/83lrm/html/lrm-02-04.html .
Excise the M;N format for bases 17..36 leaving just 2..16, absolutely.
Use "#" as separator rather than ";".
Write M as a base-10 integer rather than a single character.
These are more like Ada "based" literals then, read better, frees up ";".
So 16#FF is an integer, 16#'FF' is a blob.
Maybe also add Raku inspired commalists, like this:
60#[43,5,12] (integer); no good reason for a blob analogy.

* Define some generic framework for units and measures, so to make it
easier to perform the large fraction of math involving such things, and it
can be more strongly typed and bug free.
The core of the system would have the domain-union type "Measurement" (or
more general), whose composing types should be named after the kind of
thing being measured (eg, "Mass" or "Duration") or the names of the
relevant units (eg, "Kilograms" or "Seconds"), and there would likely be a
variety of subtypes that are union types themselves for further
categorization.  These likely all scalar types.  Subcategories include:
1. Single-unit (eg, just seconds) or multi-unit (eg, any/all of YMDHIS).
2. Approximate (carries amount plus margin/sigfigs) vs exact (no margin).
3. Semantics, such as where versus howmuch, or continuous v discrete field.
4. What can be added or differenced or multiplied or divided etc, either
two of the same measure or a measure and a bare num, what result type is;
we may possibly have a separate domain-union type for each of those.
- When units are always directly convertable, they can be possreps of the
same scalar type, such as {seconds, minutes, hours}?.
- When not directly convertable, should be separate types, such as either
seconds vs months or ...
- For simplicity, we could consider all units flat, such that rather than
adding multiple dimensions orthogonal to everything else, we would just
have MetresPerSecondSquared etc types, this especially because many
plain-sounding units are actually defined as such, eg "Watts"="Amps*Volts".
- With this very basic structure, nearly all the complexity would be in the
operators, and generally one would explicitly define separate functions
for every kind of unit-measures they wish to pair up; eg, dividing distance
by time to get speed would be "(MetresPSecondSq <-- 0:Metres, 1:Seconds)"
but of course that func could overload the generic "/" virtual, +just work.

* To simplify the units and measures framework to not have to deal with
margin/sigfigs directly, we could have a framework for general inexact math
that tracks margin/sigfigs, but this is probably no more complicated than
supporting rationals when we have integers.

* IN PROGRESS ...
Rewrite/update anything talking about matters affected by process isolation
to both declare that Muldis D is generally orthogonal or agnostic to such
matters and makes no guarantees in general that any routine, even a recipe
or updater/function invoked by one, will see a consistent view of the
database during its execution, and generally remove "atomic" terminology,
and rename "nested transaction" to some other terminology.  Rather, any
guarantees of serializability of a recipe/etc will need further work by
users such as to explicitly configure their isolation or locks or whatever
as appropriate, and of course everything's affected by what DBMS you use
and what concurrency models it supports, such as locking or MVCC.  Likewise
the model being used affects when conflict errors may manifest, eg at
commit or earlier, or when/if user tasks will block, or how complicated it
is to resolve or avoid a conflict.  Matters of the concurrency model or
isolation are best not legislated by Muldis D but be left up to the
implementations and users.  Muldis D just has to require that the database
is always in a consistent state on statement boundaries et al.

* Define how one can split a Plain_Text depot into multiple text files since
you would conceptually put an entire potentially large program in one.

* Tweak the STD dialects to account for defining system modules with them.

* Update STDIO.pod and Cast.pod concerning the Text types split.

* PACKAGE:
- Support variant of "<[ a..z A..Z _ ]><[ a..z A..Z 0..9 _ - ]>*" nonquoted
name strs that's more liberal "<[ a..z A..Z 0..9 _ - ]>+" for just atnms,
possrep names, param and arg names, so any of "-foo", "3", "-4" can be bw.
- Update system catalog and grammars to add lightweight aliasing support
for whole materials, as a new "synonym" (name?) material.  These have no
mutual order but the actual non-synonym target is the "primary" name.
Grammar can be "synonym foo of nlx.lib.bar" et al in general form, or
"function foo|bar|baz (...) {...}" where original is the first one "foo"
and the other synonyms all live in the same subdepot, and in particular the
others are "not" inner materials of "foo".
- Also add [Integer, Rational, Boolean], make [Int,Rat,Bool] into synonyms.
- Likewise (and necessarily), subdepots themselves can have synonyms.
- Also update tuple (and by extension) database types to add attribute
synonyms which semantically are lightweight virtual attribute maps that
simply make 2 attributes always-identical so only one ever needs storing
and no map function is required.  Not the same as material/sdp synonyms.
- Update system catalog and grammars to add support for routine parameter
aliases, built-in to the definitions of the routines; all names for a param
are defined in an array, that ordering being source-code-metadata, and the
first item in the list being the "primary" name.
Grammar can be "function foo (Int <-- topic|0 : Int, other|1 : Int) {...}".
This is not supported for param names in generic expr context except for
the shorthand "=>foo", so "=>1 is allowed".
- Change grammar so any number positionals supported for both s-d and u-d,
always map to "0","1".."N" and *not* "topic","other".
Also, any ".foo" now is short for "0.foo" rather than "topic.foo".
- Consider changing param names of special routines like value-filter etc,
or at least change any "topic" to "0" (other "1") so it works with ".foo".
- Update the documented signatures of all system-defined routines to use
the updated grammars reflecting the above additions.  Add param aliases of
"0" and "1" for every "topic" and "other" respectively, keeping said old
names too, and add other aliases as appropriate.  Add routine synonyms for
every distinct way of spelling a routine that rtn-invo-alt-syn provided, so
one can then always use that spelling in "foo(...)" plain-rtn-inv syntax;
update all routine docs so that the "also known as" comments no longer
mention any declared synonyms, no longer mention anything as "C<foo>" but
rather just anything as "I<foo>".
- Just stick to that, basically, leave anything else such as Unicode or
rtn-invo-alt-syn alone/not-removed for this release.

* Consider adding a midweight version of virtual-attribute-maps which is
like the fullweight version but that it expressly maps 1 attr to 1 attr; it
still uses a map function but that is no longer Tuple<--Tuple.

* Demote the "[array|set|etc]_of" types from a special concept knowable by
the backend (and explained in Basics.pod), where you can essentially use
some data types without them being declared as system catalog materials, so
that instead actual s-c materials *are* required; this syntax will remain
only as a dialect feature which is a shorthand for inline type definitions;
eg, these are now all equivalent:
    - param : set_of.Foo
    - param : relation-type { over tuple-type T { value : Foo } }
    - param : set-type over Foo
... or we might consider more material kinds specif to [set|array|etc]-type
so to help preserve the user's syntax and be more compact, maybe, but those
6 or so could probably be repr by single m-k which has an enum type attr.
Also thanks to the change about replacing N-adic with dyadic s-d routines,
and its precedent, there is less need for "foo_of" shorthands anyway.

* Externalize all the details of character string repertoires or encodings
from the Muldis D core, such that say all the details of Unicode become
part of a Muldis D extension instead, and maybe ASCII likewise.
More plans pending.

* Considering the following items where non-ASCII chars are much more
pervasive (though strictly optional), replace the "op_char_repertoire"
pragma with a pragma that affects all non-quoted code in general, including
all nonquoted (but not quoted) entity names.  The options would be, at
least, the 3: ASCII, Unicode_9.0_canon, Unicode_9.0_compat.  There
would separately be options for each kind of quoted character string:
quoted entity names, texts, comments; see later TODO item about this; as
per that, all of these could be part of a single pragma.  A simpler
implementation could support only ASCII across the board as literal
characters, while non-ASCII data could be supported as escape sequences.

* Look at this for a long list of Unicode gotchas / false assumptions, etc:
http://stackoverflow.com/questions/6162484/why-does-modern-perl-avoid-utf-8-by-default

* Enhance the cat-type/syntax for defining tuple types as attr lists (and
by extension, relations and scalar possreps) to let users provide an
optional hint for the order that tuple/sca-pr/etc attributes should be
consulted when doing an equality test between 2 tuples/etc so to direct the
DBMS to do the least expensive comparisons first, eg integer attributes,
prior to more expensive ones, eg blob attributes; since the test
short-circuits, and assuming the vast majority of compares would return
false, this should aid performance in a clean way without users resorting
to overload operators or something for performance reasons.  This is a
separate hint from that garnered by marking relation attrs as key attrs,
and could work within that eg to suggest order within multi-attr keys.
Other areas in the language could probably be assisted by hints also.

* Consider support for functions that aren't fully deterministic but for
which it is reaonable to cache their results.  For example, a function ...
THIS THOUGHT REMAINS UNFINISHED.

* Consider making the generic equality test operator virtual, so
user-defined scalar types can explicitly define it for themselves (or it is
generated if they don't explicitly say otherwise), but it is still
system-defined for tuples and relations.  The semantics of this operator
then are treated not so much as "is same" but rather as "is substitutable",
which is how the system would treat it.  This still needs a lot of thought.
To be more specific, "=(Int,Int)" and "=(Int,List)" are system-defined and
can't be overridden, but "=(List,List)" isn't at that level.
Perhaps the answer is for "=" to be defined at the lower type level only
and "=(Any,Any)" be undefined and a type-mismatch error as TTM suggests?
And rather certain other operators are more universal and over which "="
can be defined?  Maybe have multple =-like ops for different purposes.

* Consider adding native concepts of "value instance identifiers" (or
substitute "occurrence" or "sample" etc for "identifier") where these are
analogous to Perl "references", and provide system "functions" for
obtaining the VII of values, like Perl has "ref".  Note that given the same
instance X, multiple vii(X) must give the same result.
Especially in a system where the generic equality test operator is virtual,
VII can provide an implementation-influenced baseline something or other.
THIS THOUGHT REMAINS UNFINISHED.

* Taking further the idea of how various Text subtypes are defined, eg that
normalization is required, so that the Text generic equality operator would
"just work":  Consider defining that formally a Tuple or Relation etc value
has a canonical form in the low-level type system, meaning the member
attributes and tuples are always sorted in a specific system-defined way,
and so any operators that are sensitive to the low-level type system would
be fully deterministic, and the regular "=" operator would just work
without that having to be virtual or have special cases.  The only wrinkle
with that then relates to scalars and their multiple possreps.  Of course,
this is just the canon, but most normal operators wouldn't be sensitive to
this and so tuples/relations are still effectively unsorted, and more
important, an implementation doesn't actually have to store them sorted
normally, same as the "NFD" character strings don't actually have to be
stored that way by the system.  The canonical order is as follows: 1. Two
Int always sort as is normal for integers; 2. Two List always sort as is
normal for comparing strings, by comparing their elements pairwise; 3. An
Int always sorts before a List.  Further to this canonicalization, we no
longer offer multiple forms for Tuple and Relation; now, the payload of
each has 2 elements, heading and body, where the heading is sorted by
attribute name String and each tuple-body corresponds as usual, and a
relation body is a list of 1 element per tuple-body, and the elements of
that list are sorted by tuple-body List value.  So now the main thing to
figure out how to work this is in regard to multiple scalar possreps, so
that "=" has the correct semantics at the user level.  Ostensibly the
solution is quite simple, which is to mandate that a specific possrep is
the scalar type's canonical low-level form, and that moreover the name of
this possrep must be the empty string, meaning that if you actually want
that possrep to have a different name you must have a second possrep whose
form is identical to it.  A consequence of this is that, in the low-level
type system, every scalar type only has 1 possrep, and hence the possrep
name doesn't actually have to be stored, so then a scalar value then just
becomes a 2-element payload where the first is the scalar type name and the
second is a tuple value payload.  A benefit of all this canonicalization is
also that certain operations like a function to extract "a" tuple from a
relation can be fully deterministic, or similarly that we have a way of
finding a default "total sort order" for the entire type system that is
fully deterministic, even if not generally useful.  The function for this
low-level sorting, that works on Universal, is *disjoint* from the ordering
routines that users normally deal with, and is just intended for use in
canonicalization of List etc for "="; it is a plain real function like "="
and not a virtual like "<=>" etc.  Regardless, this whole paragraph
requires more thought.

* Update the Plain_Text grammar to split up the "Name_payload" or its parts
further so that, rather than just the 2 "[|non]quoted_name_str", there is
at least the additional "nonquoted_rtn_invo_name_str" which is only allowed
to be used in a routine invocation context like <op><unspace>(...), with
trailing parenthesis, and not in a context lacking trailing parenthesis.  A
"nonquoted_rtn_invo_name_str" is a nonquoted string containing no
whitespace and, in addition to all the chars nonquoted_name_str allows,
also many other symbolic chars such that wouldn't confuse the parser, so
bracketing chars would likely be disallowed, at least as leading or
trailing characters in the string, and trailing colon could be disallowed,
and leading comma or leading => etc.  The idea here is that people can then
write "+(foo,bar)" for addition or "++(foo)" for increment, or "=(foo,bar)"
for comparison, "@(t)" or "%(r)", or ":=(target,value)" for assign.
In this case, if infix ops are allowed, they'd have to have mandatory
surrounding whitespace.
We also generally have to revisit Unicode for what is allowed in bareword
variable/etc names such as non-Latin or accented letters in general.  The
parser would have to use Unicode character classes in its definitions,
then.  Look at what Raku does for some guidance.
As per another change, also assume that the idea of the internal catalog
no longer using Unicode for sys-def entity names is no longer true.
So Muldis D would then much more be Polish notation (with parens) by
default, and it should be much easier to just use the whole language that
way when it is more terse like this.  Supporting polish without parens
would be up to rtn-inv-alt-syn replacemnts while above is in plain-rtn-inv.
See also the 2nd(+?) next TODO item on splitting rtn-inv-alt-syn.
Also add yet another nonquoted...name_str that is just for use with
attribute/param/arg names and is only slightly less restrictive than the
old nonquoted_name_str in that it also allows strings of just or leading
digit chars; this is mainly so one can write positional params wo quotes.
Maybe just this last one can be added ASAP, and the other wait longer.

* Consider creating a branch of the Muldis D spec (and of the Muldis D
Manual) which retains all of the current spec features, and subsequently
strip out the whole rtn_inv_alt_syn catalog abstraction level in trunk so
that we can more radically evolve the language design at the more
fundamental level which plain_rtn_inv has access to, without worrying about
clashes or the complexity of a dozen-plus-precedence-level grammar.
Ideally the more fundamental level can evolve to the point that a
lot of what rtn_inv_alt_syn offers is no longer necessary in practice
with regards to making the code more terse.  The branch would merge in the
more fundamental changes with the old retained rtn_inv_alt_syn to see how
they might look together, or show how the new is absorbing the old; ideally
their differences would reduce over time without th branch losing features.
In the interest of marketing, the reduced trunk would retain all or much of
the example code using the then-removed features, as well as gain ones
using not yet specced features.  Each examples section would potentially be
split in 2, with the normal "Examples" just using the reduced spec features
and a new "Potential Future Examples" having anything not yet specced.
Also, the 3 Dialect files wouldn't actually lose the rtn_inv_alt_syn
precedence level but rather it would be made impotent as the grammar would
just define it as a non-proper superset of plain_rtn_inv for now; mainly
the change is that the 2 main pod sections "FUNCTION INVOCATION ALTERNATE
SYNTAX EXPRESSIONS" and "IMPERATIVE INVOCATION ALTERNATE SYNTAX STATEMENTS"
would be removed, or alternately stripped down to collection of "Potential
Future Examples" sections with a bit of commentary to explain if needed.

* The new version may be a lot easier to learn, considering that SQL + many
other C-like languages actually don't have too many non "f()" format ops.
Perhaps the main use of rtn_inv_alt_syn later is for people that want their
code to look like math/logic/etc exprs rather than named function calls.
IDEA:  Split rtn_inv_alt_syn into 2 abstraction levels where the lower one
has just 1-2 dozen or so plain prefix/infix ops such as
[:=, =,≠,!=, <,>,≤,<=,≥,>=,--,++, not,!,and,or,xor, +,-,|-|,*,/, ~, @,%,#]
and few are allowed having modifiers or that aren't in most languages.
Likely disallowed in lower level are [<=>,abs,div,mod,exp,^,**,log], the
other math ops, all other or Unicode variants of logic ops, all hyper-ops
including hypers of := or !, practically all relational/set/array/etc ops
including membership or sub/super tests.  As a middle-ground, for which we
could probably have a middle-third level from the split, are all the
postcircumfix ops that do restricted-to-constants shorthands of the likes
of array element access, projection, rename, un/group, un/wrap etc.
Things like the full set of infix logic ops are reserved for highest level,
and likewise for majority of Unicode ops and their ASCII-symbolic versions.
Now assuming we get generic <sym-op>(...) in plain-rtn-inv, and so
"+(foo,bar)" etc is an option, then we should reprioritize the above 3
post-split levels so that a level adding just postcircumfix syntax for
project/group/ary-acc/rename/etc should be the lowest additional level, so
one can be able to say "foo{...}" without also needing support for foo+bar.
Maybe call that new lowest "rtn_inv_pcfx_alt_syn".  Making postcircumfix
the lowest alt syn is also fitting because just it is like some of the
lower levels such as code-as-data where using some syntaxes make certain
inputs hard-coded, such as the attr names or interval-endpoint-flags,
versus those taking variables in the the more verbose generic syntaxes.
Presumably all levels higher than rtn_inv_pcfx_alt_syn are plain infix
or paren-less prefix with fully-variable arguments like generic functions.

* Consider making ASCII lookalikes for as many Unicode symbolic operators
as possible; for example:
    - join: ⋈ -> |X|
    - semijoin: ⋉ -> |X
    - semidiff: ⊿ -> |>
    - diff: ∖ -> \
Well it's a thought anyway, though may not be worth the trouble in general.
If the backslash is allowed to be used this way, then we'll have to put
limits on its other uses for starting escape sequences, otherwise work out.

* Maybe this isn't feasible, but ...
Consider formally making every function map 1:1 from a tuple input to a
tuple output; it declares exactly 1 parameter that is a tuple type and its
result declared type is a tuple type.  Consider making every updater
formally do something analogous, such as having exactly 2 tuple-typed
parameters where only 1 is subject-to-update.  A recipe is like that but
has 4 tuple-typed parameters, 2 like updater and 2 global alias analogies.
A virtual attribute map kind of resembles this already.
Doing this would require making tuple attribute accessors special, their
own expression/etc node kind and not just a function ... though they kind
of are already as an alternative; also, variable assignment would have to
be a special node kind and not just an updater; in both cases, to save
their definitions from being mutually recursive.

Note that the first relational database system was the IBM IS/1, in 1970-2,
and the second one was IBM Peterlee Relational Test Vehicle (PRTV); the
latter's command language was Information Systems Base Language (ISBL).

----------

* In all 3 STD.pod, add code examples for each of these 4 material kinds:
scalar-type, domain-type, subset-type, mixin-type.

* In all 3 STD.pod, complete the description text, defining interpretation
in Plain_Text and structure in the 2 Perl-STD, for each of these 7 material
kinds: scalar-type, tuple-type, relation-type, domain-type, subset-type,
mixin-type, subset-constraint.

* In all 3 STD.pod, populate the entire pod sub-section for each of these 2
material kinds, to provide concrete grammar, description text, and code
examples: distrib-key-constraint, distrib-subset-constraint.

----------

* Eliminate the simple monadic postfix special syntax category.  Convert ++
and -- into simple prefix ops, because an expression with
that in it is no longer end-weighted, and it would be less likely to
confuse people into thinking the op is variable increment rather than just
returning a result.  Removing the category also simplifies the parser as
there are no longer pre vs post precedence conflicts, and helps open the
door to the parser being more generic.  Simply eliminate postfix "!"
factorial or change it to prefix "fact".

* Update Basics.pod or other places to distinguish between the 2 main ways
that a type can be infinite, such as with "outwardly infinite" and
"inwardly infinite"; the later is when any 2 values have an infinite number
of others between them, so eg a time-of-day type could be infinite in the
inward sense but not in th outward sense; th result type of sin() likewise.
Also, the singleton types -Inf, Inf only refer to outwardly infinite types.

* Change the basic exception throwing mechanism from a function/procedure
to its own expression/statement node kind.  Call the new node kind "fail"
or "failure" or "throw" or "raise" something.  The "fail" node has a child
expression node or references a variable node which defines an Exception
value.  Simply evaluating a "fail" expression node will throw the exception
so a "fail" expr node is expected to only be the child of a short-circuit
expression like ??!!.
- Add a "fail" term, which throws a generic/default Exception value,
and/or a tight-binding "fail" prefix-keyword which takes an Exception arg;
that term/prefix is the concrete syntax for the new fail node.
- The "assertion" function can then go away; instead of writing
[$foo asserting $foo != 0], say [$foo = 0 ?? fail !! $foo].
- Add a few simple functions that each result in a kind of generic
Exception value.  At least have a niladic one for the most gen exception.
Then one could write [<cond-expr> ?? gen_exception() !! <expr-when-ok>].
- The treated() function then is just a wrapper over ??!! + isa.
- The fail() procedure will go away, replaced with a term/keyword also,
which maps to the "fail" statement node.
- Maybe use 'fail' for niladic term and 'raise' for prefix term?
- New keyword speelings:
    - failure
    - raised <expr>
    - fail
    - raise <var>
- Maybe alternatively, make an assertion into a lexical entity that is like
an expr node but doesn't have its own node name, and so is always used
either inline or offside, the main point being that users don't have to
come up with another node name when the node represents the same value as
another node and should naturally just have the same name.
Example:
    foo ::= ...
    asserts bar( foo )
    baz( foo )
... here, the assertion only happens when baz() is going to be evaluated;
the spelling is "asserts" since it should be an adjective.
- There also needs to be a version that can assert multiple exprs.
- Or actually, the ??!! version may still be better?
- Naming the "duplicate" isn't actually that hard; just use a leading
underscore, eg:
    _foo ::= foo asserting bar
    _foo ::= bar ?? foo !! failure
... so maybe that's best?
- A BIG THING TO CONSIDER HERE IS, HOW DO FUNCTIONAL LANGUAGES MAKE
ASSERTIONS ON COMBINATIONS OF ARGUMENTS ... OR IS THE ANSWER THAT ALL
FUNCTIONS HAVE EXACTLY ONE ARGUMENT?  SEE WHAT HASKELL/ETC DOES.

* Change generic assertion mechanism from a function/procedure to its own

* Add support for materials to have aliases.  But this kind of alias would
be simple, just an alternate unqualified name that exists in the same
namespace and is for the same material.  Aliases would be declared with an
"aliases" attribute, typed set-of-Name, held directly in the same catalog
types that have "name" attributes; for example, add it to the "FunctionSet"
type.  So, R.count becomes a simple alias for R.cardinality, and we can add
a whole bunch more aliases, so to make it friendlier for people who prefer
to call routines with foo(x,y) syntax rather than alternate symbols.  A
common use could be to provide both "prefix" and "infix" reading names,
such as both "product" and "multiply", and especially to give shorthands.
Example: "function product|multiply|mul (Int <-- x : Int, y : Int) {...}".
The first one in the list is the primary name, remainder are the aliases.
Or actually, it would probably be better for FunctionSet et al to *not*
internalize aliases, but rather have each alias exist as a separate
material which cites what it aliases.  And then that version could exist in
any public namespace (usually nlx), and not just the same subdepot as what
is being aliased.
The SYNONYM schema object of Oracle and other dbs corresponds to this, and
maybe "synonym" is what I should call mine too, being what the specific
material kind is called, leaving "alias" as a more generic term.
Even if we have separate synonym materials for routines/etc, one can still
declare them bundled into their originals like in the above foo|bar example
as that would just be a dialect shorthand but produce separate materials.
Also useful in support of users having their own home subdepots which have
aliases to the things they use, without them having to know where they are.
Add alias for every 'op' node 2nd element for a routine, meaning eg add
"+" and "⋈" as aliases, and so then a Muldis D parser can then produce
calls to those, as if one said `"+"(4,5)` or `"⋈"(foo,bar)`, and so we can
better remember the individual syntactic choices that the users made.
But then, how do we deal with the idea of making logical-not into a meta-op
so that there is no actual is_not_same|"≠" function etc; how do we
preserve user's individual syntactic choices then?  So think about that.
While SQL synonyms can also be used for relvars, mine would probably only
be used for materials - types, routines, stim-resp-rules, themselves, etc;
perhaps leave relvar aliases to be handled by virtual attributes.

* With the improvements from having aliases or supporting "+"(x,y) etc, and
other language improvements, it becomes a lot more feasible for users to
settle for users to be satisfied with "plain_rtn_inv", that being
sufficiently terse, and so there is less need for "rtn_inv_alt_syn" to be
implemented or available.

* Maybe also treat material names like `function "infix<+>" (...) {...}` as
special such that if a parser encounters a random "foo + bar" then it would
parse it as if it were `"infix<+>"(foo,bar)` maybe I guess.  But if this is
going to work in a general sense, including for user-defined things, then
general format rules have to be set out for the parser so that if it sees
anything like X, without knowing what ops are declared, then it treats it
as an operator rather than some other construct.  On the other hand, we're
sure to run into trouble in trying to support non foo(x,y) syntax for
user-defined operators (besides those overloading system-defined virtuals),
and so better off just not doing this period; "infix<+>" is not special.

* Add support for routine parameters to have aliases, that is, for a named
parameter to be able to bind with a named argument where the argument may
have several possible names.  One use for this would be to support
parameters where it is desired to refer to them within their routine using
one name, but to use a different name in the argument, such as because the
latter is shorter or reads better (the Raku spec should have some
examples of this).  Another use for this is to provide better support for
mixtures of arbitrary numbers each of positional and named routine
arguments; any parameters that would be reasonable to have a positional
argument would have 2 names, where one is an integer and one is text.  All
Muldis D grammars would be updated to no longer consider 'topic' and
'other' as special, which is a contrived notion, and instead consider
'0','1',... special.  And so, for all system-defined or user-defined
routines, any `op(foo,&bar,baz)` would be parsed into the same thing as
`op("0"=>foo,&"1"=>bar,"2"=>baz)`, and `.name` would be `"0".name`.  Now it
will so happen that "topic","other" will be commonly used in parameter
names, typically paired with "0","1" but we can now be a lot freer to name
parameters something more descriptive, such as "addends", and not
artificially make them topic/other simply so they support positional
syntax.  An idea for declaration syntax when aliases exist is to use the
"|" char; eg `function foo (Int <-- topic|"0" : Int, other|"1" : Int)`.
Of course, this complexity is only in param lists; arg lists are unchanged
and still are plain tuples with a single name per attr/arg.
For simplicity, a single param name will be more important than the others,
and only that would be its "expression node name" or "variable name" within
its routine, by which it must be referenced; therefore, the current
system catalog for declaring parameters can remain unchanged, and new
rtn-decl-type rtn-heading-attrs can be added to declare aliases.
Largely for flexibility, and correctness where they don't make sense,
parameters will never automatically have a number alias, but rather only
when the routine definer explicitly gives it one.
Of course, these aliases only apply to regular params, not global params.
One result of this change is that the Muldis D grammars will no longer
consider positional ro and rw args in separate spaces such that they can
appear in either order; now all positional args must be in the correct
mixed relative order, as there is only one "0", not one per ro and rw.

* Add special syntax for more ops:
    - ?#foo - "has 1+ elements" - is_not_empty(foo)
    - !#foo - "has zero elements" - is_empty(foo)
    - foo :=!# - assign_empty(&foo)
... and maybe rename underlying routines in the process.

* Update the mixins feature to add support for mixins that define
attributes that types can compose, whereby we support some approximation of
"specialization by extension" while still actually being just
"specialization by constraint".
Maybe also it could be said ...
A primary purpose of mixins is to help with managing software reuse, mainly
when multiple types have a number of attributes in common, a mixin can
define these and then the multiple types can compose that mixin.  A mixin
or type that composes a mixin can both add additional attributes of its own
to what the mixin defines, and the composer can add extra constraints over
the composed attributes like forcing a subtype.
Maybe also do ...
Support delegation / 'handles'; for example:
    - Name explic delegate to Text attr
    - maybe Blob, Text explic delegate to String attr
    - a ColoredCircle would delegate to both Color and Circle attrs?
This will all take some work to get right; not /all/ Rat/etc can be subst.
Probably *only* those operators that Rational/etc explicitly declares can
be delegated to Rat/etc by TAIInstant/etc.

* Replace many N-adic routines with dyadic ones, specifically
those whose definition is a repetition of a dyadic operation (so, 'sum' or
'join' etc yes but 'mean' no), which users then can invoke by way of a
reduction function if they want N-adic syntax.  Also let system catalog
store more information such as whether or not functions are commutative or
associative or idempotent or symmetric etc; likewise, the function def can
store what the operation's identity value is, if it has one, as meta-data,
useable when comm/assoc; the reduction func can read this using a
meta-programming function or something.  Reduction will fail if used on a
base func that doesn't define an identity if given an empty list.
The point of this change is to make the common dyadic case of N-adic
operators simpler, and also set a foundation for user-defined operators
that provide more information such that a compiler can be more effective
in optimizing them, or something.
The explicit/normal way, then, to indicate in code whether you want the
parser to produce a reduce op wrapper call rather than nested direct
invocations in the system catalog, is to just invoke the reduction
operator directly and explicitly pass an operand list; but the reduce op
would have special syntax, taking normal collection exprs, such as:
    [+] {5,23,5}
    [~] ['hello', 'world']
    [join] {order,inventory}
    [*] {1..5}
... or something.  Not using that would parse into nested dyadic calls
instead though the compiler can still rearrange.
Once we do that, its also simple to add hyper-operators, though arguably
these are redundant with 'map' or 'extension' etc.
Or this would be better for simplicity, given it won't be used as often,
and any dyadic infix function at all may be used, spelled the same way:
    reducing + {4,23,5}
    reducing ~ [...]
    reducing join {...}
    reducing * {...}
    reducing <nlx.lib.myfunc>(a=>3) {...}
... and so the regular operators can be parsed as usual.
Or maybe:
    reduced {4,23,5} using +
    reduced {...} using <nlx.lib.myfunc(a=>3)
... but that might have an end-weight problem?
Or, still go symbolic like the first one, but use prefix notation so that
it works well with both symbolic and wordy or inline-defined operators:
    []+ {5,23,5}
    []~ ['hello', 'world']
    []join {order,inventory}
    []* {1..5}
    []<nlx.lib.myfunc>(a=>3) {...}
Another consideration is that, when combined with routine synonyms that are
symbolic, the plain_rtn_inv alone would let you do this:
    reduce( <"+">(), {5,23,5} )
    reduce( <"~">(), ['hello', 'world'] )
    reduce( <join>(), {order,inventory} )
    reduce( <"*">(), {1..5} )
    reduce( <nlx.lib.myfunc>(a=>3), {...} )

* Furthering the above, add somewhat generalized support for what Raku
calls "meta" operators, at least in that we define and exploit several.
The general reducer above would be one of these.  Another is the negated
relational, whose syntax is putting ! or not- in front of any Bool-resu op.
Another is the assignment, putting := in front of any function.
For !, we can then eliminate all the "not" variants of any Bool-resulting
functions, so eg "x != y" parses into "not(is_same(x,y))", same as if
they had said "!(x = y)".  As for the old intended purpose of all the not-
variants, which is to preserve the user's intent of how code should look,
we could simply have an alias for the not() function which is what is
parsed into when != is used, and the old not() is just parsed into when the
separate prefix op is used.  On the other hand, while lots of not- variants
would go away, we'll keep the alias-but-param-order-reversed dualities such
as less-than/greater-than and sub-superset; unlike these, what we're
eliminating would not result in losing track of which args are lhs/rhs.
A related change is infix ops like ≠ or ⊈ would parse into not(foo()) even
though they don't have the !; these would be aliases for the combos, same
as Raku has != as an alias for !==.
For :=, we can eliminate all the updaters that are just shorthands for
doing an op and assigning the result to one of the args.  And so a
"foo :=union bar" would parse to "assign(&foo,union(foo,bar))".  Once
again, an alias for assign() can exist which such combos are parsed into,
where the regular assign() is used when users write "foo := foo union bar".
Of course, despite Muldis D requiring operator combos where singles used to
work, we assume that implementations will be smart enough to, say, use a
single "!=" or "insert into foo ..." etc when it sees the combination, so
there is no performance loss.
Probably, any meta'd operator would have the same precedence as the base
operator that it is modifying.
Adding the hyper-meta may not be useful since we already have map()/etc;
or alternately it might be useful in avoiding some uses of map or extend
or substitute etc where users are just adding/defining one attr.
Or maybe hyper-meta would only be useful with Set/Array/Bag because the
general map/extend/etc would require naming the attribute explicitly.
As for ASCII vs Unicode etc, that preference is never encoded in the system
catalog, so when code would be generated from the system catalog, it would
be up to the generator's configuration for which versions are used.

* Add hyper-meta in a more general fashion, as per the Ranked general type
of which Array is a more specific kind.  The hyper-meta is fundamentally
associated with the join operator, because it typically involves taking 2
relations, joining them on one set of same-named attrs (exactly 1 usually),
and then taking another set of *same-named* attrs and applying the hypered
op pairwise and deriving a single replacement set of those attrs with the
results.  The argument attrs would be renamed distinct first.  For example,
given 2 relations A{key,value,x} and B{key,value,y}, where we assume that
"key" is a unary key of each relation, the expression
"A >>+<< B" is roughly like this code:
  with (
    a ::= A{%others_a<-!key,value}{value_a<-value}
    b ::= B{%others_b<-!key,value}{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr{<-%others_a}{<-%others_b}
And the result is a relation with heading {key,value,x,y} but of course
with the more typical case the inputs and output are just {key,value}, in
which case that simplifies to:
  with (
    a ::= A{value_a<-value}
    b ::= B{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr
A variant taking a relation and a tuple would be like the >>+>> /etc form.
We might have variants for join vs union etc or generalize this further so
that bag/counted variants of relational ops can be defined using this
generalized hyper in combination with the regular relational ops, maybe.

* About extra metadata in the system catalog for functions/etc, see
http://www.postgresql.org/docs/9/static/extend.html for some ideas, such
as 35.13.x on Pg's use of COMMUTATOR and NEGATOR where function pairs
declare their complement operator.  The first pairs up "<" and ">" say (and
"+" pairs with itself) while the second pairs up "<", ">=" (dbl-chk that).

* Note that Pg exts are like Muldis D system modules in what they do, such
as that they add types and routines etc to the language.

* Change multi-update to be a sequence of statements rather than a set, and
explicitly allow the same target to be used more than once ... this could
be the case anyway thanks to virtual relvars etc.

* Move or adapt more Text functions into Stringy.
- Fundamentally all Stringy funcs work on Text in terms of the
"maximal_chars" possrep; this will just work correctly for when all
func args are of the same Text subtype, such as Canon etc.
- The Stringy/Text ops are analogous to Rational ops such that it is like
doing fraction math.  catenation() is like sum(), replication() is like
multiply, a substring test is related to difference/subtract (maybe "?~"
and "!~" might work as infix ops for something?).
- Move cat_with_sep to Stringy; semantics are clear cut and generalizable.
- has_substr ought to work with Stringy no problem from the Text and Array
perspectives, but Blob presents an issue purely concerning bit alignment,
such as whether we're searching on bits or on octet/etc alignments.

----------

* Update the virtual attributes maps so there is a way to manually specify
a reverse function, as meanwhile all the virtuals don't have to be either
read-only or updatable due to an automatically generated reverse function,
which might vary by implementation, which may be considered broken.  Note
that the reverse functions might have to be defined as per-tuple
operations, separately for insert/substitute/delete.

* Add new "material" kinds that define state constraints (address as simple
nlx.*.data.*), like type constraints but ref in reverse.

* Update the "material" kinds that def stimulus-response rules / triggered
routines so that they work for more kinds of stimuli, and maybe change the
keywords.  The material kind has 2 main attributes, where the "stimulus"
defines what to look out for and "response" defines what to do when the
former is sighted.  Some possible keywords for the first are "stimulus",
"cause", "when"; for the latter, "response", "effect", "invoke".

* Add new "material" kinds that define descriptions of resource locks that
one wants to get, starting with basic whole dbvar, relvar locks (address as
simple fed.data.foo.*, as well as simple relvar tuple locks (addr as prior
plus lists of values to match like with a semijoin); leave out generic
predicate locks at first but note they will be added later.
Update the system catalog concerning managing shared|exclusive locks or
looking for consistent reads between statements, etc.

* Large updates to docs concerning transactions and resource locking.
Note:  Supposedly PostgreSQL and MySQL use read-committed isolation by
default while SQLite provides serializable.

* Rewrite the "Exception" catalog type so it can carry metadata on what
kind of exception occurred, not just that an exception occurred.

* Also study SQL concept of conditions and handlers, looks sort of like
something between exception handling, signals; or it is their exceptions.

* Also adapt something like Postgres' LISTEN/NOTIFY/UNLISTEN feature, which
is an effective way for DB clients to be sent signals, such as when a
database relvar has changed.

* Use a conceptual framework for database transactions that is strongly
inspired by how distributed source-code version control systems (VCSs)
work, in particular drawing on GIT specifically.  The fundamental feature
of the framework is that the DBMS is managing a depot consisting of 1..N
versions of the same database, where every one of these versions is both
consistent and durable.  Each version is completely defined in isolation,
conceptually, and so any versions in a depot may be deleted without
compromising each of the other versions' ability to define a version of the
entire database.  It is implementation-dependent as to how the versions are
actually stored, such as each having all of the data versus most of them
just having deltas from some other version; what matters is that each
version *appears* to be self-contained.  Every version is created as a
single atomic action, and it is never modified afterwards, though it may be
later deleted (also an atomic action).  Every in-DBMS user process,
henceforth called "user", has its own concept of the current state of the
database, which is one of the depot's versions that is designated a "head".
A user's current head is never replaced during the course of the in-DBMS
process unless the user explicitly replaces it, such as by either
performing an update or requesting to see the latest version (the latter
done such as with an explicit "synchronize" control statement).  Therefore,
each user is highly isolated from all the others, and is guaranteed
consistent repeatable reads and no phantoms; they will get repeatable reads
until they request otherwise.  The framework has no native concept of
"nesting transactions" or "savepoints" or explicit "commit" or "rollback"
commands.  Rather, every single DBMS-performed parent-most multi-update
statement (which is the smallest scope where TTM requires the database to
be consistent both immediately before and immediately after its execution),
is a durable atomic transaction all by itself.  The effect of a successful
multi-update statement is to both produce a new (durable) version in the
depot and to update the executing user's "head" to be that new version (the
prior version may then be deleted automatically depending on
circumstances); a failed multi-update statement is a no-op for the depot,
and the user gets a thrown exception.  A depot's versions are arranged in a
directed acyclic graph where each version save the oldest cites 1..N other
versions as its parents, and conversely each version may have 0..N
children.  A child version has exactly 1 parent when it was created as the
result of executing a multi-update statement in the context of the parent
version; the parent version is the pre-update state of the database and the
child is the post-update state of the database.  A child version has
multiple parents when it is the result of merging or serializing the
changes of multiple users' statements that ran in parallel.  One main
purpose of tracking parents like this is for reliable merging of parallel
changes, so that the intended semantics of each change can be interpreted
correctly, and potential conflicts can be easily detected, and effectively
resolved.  More on how this works follows below.  Note that versions simply
have unique identifiers to be referenced with and there is no implied
ordering between them if they are generated as serial numbers or using date
stamps, though versions with earlier date stamps are given priority in the
case of a merge conflict.  So a multi-update statement is the only native
"transaction" concept, and it is ACID all by itself.  Now, the
multi-statement "transactions" or concepts of nested transactions or
savepoints would all be syntactic sugar over the native concept, and
basically involve keeping track of versions prior to the head and
optionally making an older one the head.  This framework uses the VCS
concept of "branching" (which is something that GIT strongly encourages the
use of, as GIT makes later "merging" relatively painless) as the native way
to manage concurrent autonomous database updates by multiple users.  By
default, when no users have made any changes to the database, a depot just
has a "trunk", and its childmost or only version is called "master"; every
database user process' "head" starts off as the "master" version when that
process starts.  Each (autonomous) user process that wants to update the
database will start by creating a new branch off of the trunk, and
subsequent versions of theirs will go into that, rather than into the trunk
or some other branch.  The trunk is shared by all users while each user's
branch is just for that user, as their private working space.  Note that,
unlike a VCS in general where branches can become long-lived and interact
with each other independently of the trunk, the framework instead follows
the typical needs of an RDBMS, which espouses a single world view as being
dominant over any others, and expects that any branches will be very
short-lived, not existing for longer than a conceptual "database
transaction" would; only the trunk is expected to be long-lived.  (This
isn't to say that a DBMS can't maintain them long term, but one that acts
like a typical RDBMS of today wouldn't.)  Note that the final action on a
branch that involves merging into the trunk, this would be perceived by all
other DBMS users as all of the changes wrought by the branch being a single
atomic update, though the user performing it may see several steps.

* Flesh out matters related to starting or communicating between multiple
autonomous in-DBMS processes, in general, besides the special case about
sequence generators.

----------

* Add to Routines_Catalog.pod and other files
definitions of any remaining routines, eg String routines, that would be
needed so that for all system-defined types all the necessary
system-defined routines would exist that are necessary for defining said
types, especially their constraint or mapping etc definitions.  So in
String.pod we need [catenation, repeat, length, has_substr] etc.
Also add "is_coprime" or GCD or LCM or etc which are used either in the
constraint definition of Rat or in a normalization function for Rat; see
also "the Euclidean algorithm" as an efficient way to do the calculations.

* Consider adding type introspection routines like: is_incomplete() or
is_dh() or is_primitive|structure|reference|enumerated etc.  Or don't
since one could look that up in the system catalog.  But more tests on
individual values might be useful, or maybe we have enough already.

* Add ext/TAP.pod, which is a partial port of Perl's Test::More / Raku's
Test.pm / David Wheeler's pgTAP to Muldis D; assist users in testing
their Muldis D code using TAP protocol.  The TAP messages have type Text.

----------

* Add concept of shallowly homogeneous / sh- relation types to complement
the deeply version, and named maximal types like SHRelation, SHSet,
SHArray, etc to complement the DH/etc, and sh_set_of/etc to complement
dh_set_of/etc; but not sh-scalar or sh-tuple as the concept doesn't make
sense there.  Then update functions like Relation.union/etc to take
sh_set_of.Relation rather than set_of.Relation, which more formally defines
some of their input constraints.

* Consider adding an imperative for-each looping statement; the main
question here is whether it should work on any (unordered) relation or just
on an Array (in which case it iterates through the tuples in sequence by
index); the question is what tasks the for-each would be used for; perhaps
both versions are useful; presumably the main reason to have for-each at
all is when I/O is involved and some derivative needs to be output either
where order matters or where order does not matter; but perhaps only a
routine is needed here such as a catenate function plus normal I/O output.
The question also is what tasks would an imperative for-each be needed for
that functional constructs like the list-processing relational functions
can't better be used for those tasks instead.

----------

* Add a round-rule param to rat division, I suppose, since in general we'll
need it if we want to maintain a rational radix through every op (+,-,*
will already do so when all their args are in the desired radix).

* Add explicit support for +/- underflow, +/- overflow, NaNs, etc.
I'm inclined to think +/- zero is unnecessary when we have underflow and
can be confusing anyway (just a single normal number zero is better).
I'm not sure if +/- overflows are useful or if infinities cover them for
our purposes.  How this would work is that we define a set
of scalar singleton types, one for each of the special values.  Then we
define extended versions of the Int, Rat, etc types where the extended
types are defined in terms of being union types that union the regular
numeric types with the special singleton type values.  This approach also
means just one each of +Underflow, -Overflow, etc is needed and is a member
of extended Int or Rat etc.  Consider using the existing names "Int"/"Rat"
with the versions that include these special values, and make new names for
the current simpler versions that don't, such as "IntNS" (int no specials),
"RatNS", etc.  Either way, it is useful to support the full range of values
that a Raku numeric can support, or that an IEEE float can support,
without users necessarily having to define it themselves.
IDEA:  Maybe make all normal math/etc ops work with the extended versions
(those with NaNs, infinities, etc) and in situations where users don't want
those special values they just use a declared type excluding them, and then
the normal type constraints will take care of throwing exceptions when one
divides by zero for example.

* Flesh out Interval.pod to add a complement of functions for comparing
multiple intervals in different ways, such as is-subset, is-overlap,
is-consecutive, etc, as well as for deriving intervals from a
union/intersect/etc of others, as well as for treating intervals as normal
relations in some contexts, such as for joining or filtering etc, as well
as a function or 3 to do normalization of Interval values.
Maybe the type name 'Range' can be used for something.
Maybe the type name 'Span' or 'SpanSet' can be used for something;
there are Perl modules with those names concerning date ranges.
Input is welcome as to what interval-savvy functions Muldis D should have.

* Consider renaming Interval to Range, even if that is less specific,
for brevity, and also so we have a word that looks less like Integer
or that an Int abbreviation would be less ambiguous.
- Update, upon discussion, I decided to stay with "interval", which both
Hugh, Philip, Derek agree is the best term, with none arguing differently.

* Flesh out some window/partition funcs, which are kind of like a
generalization of aggregation/reduction functions.  A window()/partition()
wrapper func is like the summary() wrapper func but it has the same number
of output tuples as input ones; when wrapping an agg/reduc func, all output
tuples have the same value per tuple in the same group; when wrapping a
window/partition-oriented func, such as rank(), each tuple in the group
gets or can get a different value.
See these:
- http://www.postgresql.org/docs/9.0/interactive/tutorial-window.html
- http://www.postgresql.org/docs/9.0/interactive/functions-window.html
- http://www.postgresql.org/docs/9.0/interactive/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS

* IN PROGRESS ...
Add Bool-resulting relational operators EXISTS and FORALL, that provide
"existential quantification" and "universal quantification" respectively,
these being useful in constraint definitions.  See TTM book p168, pp394-5
for some info on those.  Also add analogies to Perl's List::MoreUtils
operators any(), all(), notall(), none(), true(), false(); some of those
may be the same as EXISTS/FORALL.  Also add an EXACTLY operator like the
Tutorial D language has, and a one() op that is between any() and none().
Maybe some pure boolean ops can be added analogous to the above also; eg
any() an alias for or() and all() an alias for and().
is_(any|all|one|none|notall|etc)_of_(restr|semijoin|semidiff|etc)
source is any|etc matching|where|etc filter|etc
ADD RELATIONAL OPERATORS THAT COMBINE BOOL OPS ADDED IN 0.80.0 WITH
RELATIONAL MAP/RESTRICTION/ETC AND ... The new functions are modelled after
some in Perl's List::MoreUtils module.
That is, add prefix ops exactly|all|any|one|none|etc
which take a relation and result in True or False depending on what that
relation's cardinality is.  In some cases, an extra arg is needed:
    - exactly((s⋉t),n) = (#(s⋉t) = n)
    - none((s⋉t)) = exactly((s⋉t),0) = !#(s⋉t)
    - any((s⋉t)) = !exactly((s⋉t),0) = ?#(s⋉t)
    - all((s⋉t),#s) = exactly((s⋉t),#s) = (#(s⋉t) = #s)
    - notall((s⋉t),#s) = !exactly((s⋉t),#s) = (#(s⋉t) != #s)
    - one((s⋉t)) = exactly((s⋉t),1) = (#(s⋉t) = 1)
OR MAYBE THESE AREN'T ANY MORE USEFUL THAN THEIR EQUIVALENT EXPRS.

* Consider adding sequence generator updaters|procedures in Integer.pod.

* Consider adding random value generators for data types other than integer
and rational numerics, such as for character strings or binary strings.

* Consider analogy to SQL's "[UNION|EXCEPT|INTERSECT] CORRESPONDING BY
(attr1,attr2,...)", which is a shorthand for combining projection and
union, that takes a list of attributes and unions the projections of those
attributes from every input relation; so this means, as with join(), that
the input relations don't need to have the same headings.

----------

* In Plain_Text, consider further changes to how character escape sequences
in strings/etc are done.  For example, whether the simple escape sequence
for each string delimiter char may be used in all kinds of strings (as they
are now) or just in strings having the same delim char as is being escaped.

* IN PROGRESS ...
Update the STD dialects to support inline definition of basic
routines (and types?) right in the expressions/etc where they are used,
such as filter functions in restriction() invocations, so many common cases
look much more like their SQL or Perl counterparts, or for that matter, a
functional language's anonymous higher order functions.  This syntax would
be sugar over an explicit material definition plus a FooRef val selection,
which means the inner def effectively is an expression node, and users can
choose to name or not name the FooRef selecting node as normal with value
expressions.  It is expected that the materials could be decl anonymously
and names for them (the inn.foo, not the FooRef's lex.foo) would be
generated as per inline expression nodes etc.

* Further to the previous item, add some special syntax, similar to how one
references a parameter to get its argument's value, which can see into the
caller's lexical scope.  This would be sugar over declaring parameters with
the same name and having the caller explicitly pass arguments to it,
without having to explicitly write that.  Generally this syntax would only
be used with inline-declared routines.  But similarly, add some special
syntax allowing one to essentially just write the body of a routine without
having to explicitly write its heading / parameter list, which is useful
for routines invoked directly from a host language, where said parameters
are attached to host bind variables.  Now one still has to say what the
expected data type is for these bind variables, but then the explicit
syntax for such Muldis D routines is more like that of a SQL statement you
plug into DBI or whatever, without the explicit framing.  May not work
anywhere, but should help where it does.  Maybe use $$foo rather than $foo
to indicate that the 'foo' wasn't explicitly declared in the current
lexical scope and we are referring to the caller or a bind variable.  Or
rather than $$foo, have something like "(param foo : Bar)" for an
expression-inline parameter definition and use, where the part after the
"param" has all the same syntax as an actual param list; this is the one
for host language bind parameters.  Actually that might be useful by
itself.  Similarly "(caller foo)" would be the look to parent Muldis D
lexical scope, or $$foo would just do that maybe, unless this should have
an explicit type declaration still.  Note, if same inline-declared host
param used more than once, you just need "(param foo : Bar)" form once and
other uses can just say foo as per usual; in fact, it must be this way.

* Consider in all STD adding a new pragma that concerns whether data in
delimited character string literals is ASCII or Unicode etc.
Example Plain_Text grammar additions:
            <ws>? ',' <ws>? str_char_repertoire <ws>? '=>' <ws>? <str_cr>
    <str_cr> ::=
        '{' <ws>?
            [<str_cr_describes> <ws>? '=>' <ws>? <str_char_reper>]
                ** [<ws>? ',' <ws>?]
        <ws>? '}'
    <str_cr_describes> ::=
        all | text | name | cmnt
    <str_char_reper> ::=
          ASCII
        | Unicode_9.0_canon
        | Unicode_9.0_compat
Example Plain_Text code additions:
    str_char_repertoire => { text => Unicode_9.0_canon,
        name => Unicode_9.0_compat, cmnt => Unicode_9.0_compat },
    str_char_repertoire => { all => ASCII },
Of particular interest is the Unicode canonical vs compatibility, that is
NFC|D vs NFKC|D; it is generally recommended such as by the Unicode
consortium to use canonical for general data but to use compatibility for
things like identifiers or to avoid some kinds of security problems; see
http://www.unicode.org/faq/normalization.html.  Note that compatibility is
a smaller repertoire than canonical, so converting from the latter to the
former will lose information.  The text|name affect how delimited char
strs that are Text|Name are interpreted, and the effects are
orthogonal to whether characters are specified literally or in escaped
(eg "\c<...>" form); canonical will preserve exactly what is stated (but
for normalization to NFD) and compatibility will take what is stated and
fold it so semantically same characters become the same code points (like as
normalizing to NFKD).  The suggested usage is compatibility for Name to
help avoid security or other problems, and canonical for Text; as for
comments, I currently don't know which is better.  If ASCII is chosen, the
semantics are different; with both Unicode any input is accepted but folded
if needed; for ASCII, it is more likely an exception would be raised if
there are any code points outside the 0..127 range in character strings.
The 'all' is a shorthand for giving the same value to all 3 text|name|cmnt
and is more likely to occur with ASCII but it might happen otherwise.
An additional reason to raise this feature is to setup support for other
char sets in future, such as Mojikyo, TRON, GB18030, etc which go beyond
Unicode eg no Han-unification (see http://www.jbrowse.com/text/unij.html +
http://www.ruby-forum.com/topic/165927) but type system also needs update.

* Update HDMD_Raku_STD.pod considering that a 2010.03.03 P6Syn update
eliminated the special 1/2 literal syntax for rats and so now one writes
<1/2> instead (no whitespace allowed by the '/'); now 1/2 could still work
but now it does so using regular constant folding and so having a higher
precedence op nearby affects its interpretation.

* Update HDMD_Raku_STD.pod considering names of Perl collection types,
such that "Enum" is the immutable "Pair" and "EnumMap" was renamed from
"Mapping", and "FatRat" is now the "Rat" of unlimited size, etc.

* Consider using postcircumfix syntax for extracting single relation
attrs into Set or Bag etc, meaning wrap_attr; eg "r.@S{a}", "r.@B{a}".
Now that might not work for Array extraction, unless done like
"(r.@A{a} ordered ...)" or some such, which isn't pure postcircumfix,
but that may be for the best anyway.

* Consider adding concrete syntax that is shorthand for multiple
single-attribute extractions where each goes to a separate named expression
node (or variable) but the source is a single collection-typed expr/var.
Or the source could be a multiplicity as well, or mix and match.
The idea here is to replicate some common idioms in Perl such as
"(x, y) = @xy[0,1]" or "(x, y) = %xy{'x','y'}", this being more useful
when the source is an anonymous arbitrary expression.
Proposed syntax is that, on each side of the "::=" or ":=", the source and
target lists are bounded in square brackets, indicating named items assign
in order, and syntax for collections supplying/taking multiple items are
ident to single-attr accessors (having a ".") but that a list is in the
braces/brackets; for example: "[x, y] ::= [3, 4]",
"[a, b] ::= t.{a,b}", "[c, d] ::= ary.[3,5]".  This syntax would
resolve into multiple single-attr accessors when app in system catalog.
The assignment variants of the above would naturally fall out the ability
to have arbitrary expressions on both sides of the ":=", so what you do is
have an array-valued expression on both sides, eg "[x,y] := [y,x]" works
because "[...]" is an array literal now.
We can overload ".[]" for tuples in general so they extract like projection
but return an array rather than a tuple, so we can then say
"[a,b] ::= t.[a,b]" or even "t1.[x,y] := t2.[a,b]" to multi-substitute,
that being a shorthand for "t1.x := t2.a, t1.y := t2.b".  We can't do that
for general relations though since the array subtype of rel is using it.
This mechanism also provides a general way for a function to have multiple
ord retv; eg, "[x,y,z] := foo(...)"; like Perl's "($x,$y,$z) = foo(...)".
A variable (or subject-to-update parameter), "bar", may be aliased using
"foo ::= bar" such that "foo" is an expr node, but like all named exprs in
procedures, "foo" is conceptually reevaluated per mu-statement.
Ordered tuples can be used instead of arrays, and in fact might be a better
solution for multiple reasons.  To do this, just say "%:{x,y,z}" rather
than "[x,y,z]"; the former is shorthand for '%:{"0"=>x,"1"=>y,"2"=>z}'.

* In Plain_Text, consider loosening the grammar regarding some of the normal
prefix or postfix or infix operators so that rather than mandating
whitespace be present between the operators and their arguments, the
whitespace is optional where it wouldn't cause a problem.

----------

* Restore the concept of public-vs-private entities directly in sub|depots.

* Restore the concept of "topic namespaces" (analogous to SQL DBMS concept
of "current database|schema" etc) in some form if not redundant.

* Update the system catalog to deal with database users and privileges etc.

----------

* IN PROGRESS ...
A Muldis D host|peer language can probably hold lexical-to-them variables
whose Muldis D value object is External typed, and so they could
effectively pass around an anonymous closure of
their own language.  Such a value object would be a black box to the host
and can't be dumped to Muldis D source code.

* IN PROGRESS ...
Fully support direct interaction with other languages, mainly either peer
Parrot-hosted languages or each host language of the Muldis D
implementation.  Expand the definition of the "reference" main type
category (or if we need to, create a 5th similarly themed main category) so
that it is home to all foreign-managed values, which to Muldis D are simply
black boxes that Muldis D can pass around routines, store in transient
variables, and use as attributes of tuples or relations.  These
of course can not be stored in a Muldis D depot/database, but they can be
kept in transiant values of Muldis D collection types which are held in
lexical variables by the peer or host language; that language is then
really just using Muldis D as a library of relational functions to organize
and transform its own data.  We also need to add a top level namespace by
which we can reference or invoke the opaque-to-us data types and routines
of the peer or host language.  This can not go under sys.imp or
sys.anything because these are supposed to represent user-defined types and
routines, which in a dynamic peer language can appear or disappear or
change at a moment's notice, same as in Muldis D; on the other hand, types
or routines built-in to the peer/host language that we can assume are as
static as sys.std, could go under sys.imp or something.  This also doesn't
go under fed etc since fed is reserved for data under Muldis D control and
only ever contains pure s/t/r types.  Presumably this namespace will be
subdivided by language analogously to sys.imp or whatever syntax Raku
provides for calling out into foreign languages co-hosted on Parrot.  Since
all foreign values are treated as black boxes by Muldis D, it is assumed
that the Muldis D implementation's bindings to the peer/host language will
be providing something akin to a simple pointer value, and that it would
provide the means to know what foreign values are mutually distinct or
implement is_same for them.  One thing for certain is that every
foreign value is disjoint from every Muldis D value, and by default every
foreign value is mutually distinct from every other foreign too, unless
identity is overloaded by the foreign, like how Raku's .WHICH works.
The foreign-access namespace may have a simple catalog variable
representing what types and routines it is exposing, but to Muldis D this
would be we_may_update=false.

* IN PROGRESS ...
About External type ... update Perl_STD and
Raku_STD to add a new selector node kind 'External' which takes any Perl
value or object as its payload; this is treated completely as a black box
in general within the Muldis D implementation.  For matters of identity
within the Muldis D envirnment, it works as follows:  Under Raku, the
Perl value's .WHICH result determines its identity.  Under Perl, if the
value is a Perl ref ('ref obj' returns true) then its memory address is
used, and this applies to all objects also (since all refs are mutable,
this seems to be the safest bet); otherwise ('ref obj' is false) then the
value's result in a string context, "obj", is used as the identity; the
mem addr and stringification would both be prefixed with some constant to
distinguish the 2 that might stringify the same.  By default, an
External supports no operators but is/not_same.

----------

* Add new "FTS" or "FullTextSearch" extension which provides weighted
indexed searching of large Text values in terms of their component tokens,
such as what would be considered "words" in human terms.  This is what
would map to the full text search capabilities that underlying SQL DBMSs
may provide, if they are sufficiently similar to each other, or there might
be distinct FTS extensions for significantly different ones?

* Add new "PerlRegex" extension which provides full use of the Perl
regular expression engine for pattern matching and transliteration of Text
values.  Maybe the PCRE library can implement this on other foundations
than Perl itself if they are sufficiently alike; otherwise we can also
have a separate "PCRE" extension.  Or the same extension can provide both?

* Add new "RakuRules" extension which provides full use of the Raku
rules engine for pattern matching and transliteration of Text values.

* Add new "PGE" or "ParrotGrammarEngine" extension, or whatever an
appropriate replacement is, for pattern matching and transliteration of
Text values.  This and "RakuRules" may or may not be sufficiently similar
to combine into one extension.

* Add functions for splitting strings on separators or catenating them with
such to above extensions or to Text.pod as appropriate.  Text has one now.

* Update or supplement the order-determination function for Text so that it
compares whole graphemes (1 grapheme = sequence starting with a base
code point plus N combining code points, or something) as single string
elements, rather than say comparing a base char against a combining char.

* Add new "Complex" extension which provides the numeric "complex" data
types (each expressed as a pair of real numbers with at least 2 possreps
like cartesian vs polar) and operators.  Note that the SQL standard does
not have such data types but both many general languages as well some
hardware CPUs natively support them.  Probably make "Complex" a mixin type
and have the likes of "RatComplex" and "IntComplex" composing it.  Note
that a complex number over just integers is also called a Gaussian integer.
A question to ask is whether a distinct "imaginary" type is useful; some
may say it is and Digital Mars' "D" has it, but I don't know if others do.
In any event, complex numerics should most likely not be part of the core,
even though their candidacy could be considered borderline; for one thing,
I would expect that most actual uses of them would work with inexact math.

* Add other mathematical extensions, such as ones that add trigonometric
functions et al, or ones that deal with hyperreal/hypercomplex/etc types,
or ones with variants of the core numeric types that propagate NaNs etc.

* Consider adding a sleep() system-service routine, if it would be useful.

* Add multiplication and division operators to the Duration types; these
would both be dyadic ops where the second op is a Numeric.

* Consider adding a Temporal.pod type specific to representing a period in
time, maybe simply as an alias for 'interval_of.*Instant' or some such.
See also the PGTemporal project and its 'Period' type.

* Flesh out "Spatial" extension; provide operators for the spatial data
types, maybe overhaul the types.

* Consider another dialect that is JSON ... like HDMD in form, but stringy.

* Fundamentally, a Muldis D DBMS API or client-server protocol has a
command pattern where the request is Muldis D code and so is the response.
This is like the relationship between Javascript and JSON, where the data
is expressed in the same syntax as code.  Plain_Text by default.
Actually, this brings up an interesting thought in that a DBMS shell could
be analogous to writing HTTP requests manually like telnet to port 80.
But a different Muldis D dialect would be optimized for machine-to-machine
like client-server stuff.
If we ignore parameter binding, a programmer API could also be a function
or procedure call that takes Muldis D code as a single argument and has
Muldis D code as the result; the argument is code defining a tuple value.

* Mention in the DBMS or learn from "Cego" (http://www.lemke-it.com/).

* Apparently Postgres has no built in scheduler feature, and
PgAgent is designed to cover that need.

----------

* Add one or more files to the distro that are straight Plain_Text code like
for defining a whole depot (as per the above) but instead these files
define all the system entities.  Or more specifically they define just the
interfaces/heads of all the system-defined routines, and they have the
complete definitions of all system-defined types, and they declare all the
system catalog dbvars/dbcons.  In other words these files contain
everything that is in the sys.cat dbcon; anything that users can introspect
from sys.cat can also be read from these files in the form of Plain_Text
code, more or less.  The function of these files is analogous to the Raku
Setting files described in the Raku Synopsis 32, except that the Muldis D
analogy explicitly does not define the bodies of any built-in routines.  An
idea is that Muldis D implementations could take these files as is and
parse them to populate their sys.cat that users see; of course, the
implementations can actually implement the routines/types as they want.
Note that although this Muldis D code would be bundled with the spec, it is
most likely that the Plain_Text-written standard impl test suite will not.
Note that these files will not go in lib/ but in some other new dir.  Note
that it is likely any implementation will bundle a clone of these files
(suitably integrated as desired) rather than having an actual external
dependency on the Muldis::D distro.  Note that some explicit comment might
be necessary to say there are no licensing restrictions on copying this
builtins-interfaces-defining code into Muldis D implementations, or maybe
no comment is necessary.  Probably a good precedent is to look at what
legalities concern existing tutorial/etc books that have sample code.

* Create another distribution, maybe called Muldis::D::Validator, which
consists essentially of just a t/ directory holding a large number of files
that are straight Plain_Text code, and that emit the TAP protocol when
executed.  The structure and purpose of this collection is essentially
identical to the official Raku test suite.  A valid Muldis D
implementation could conceivably be defined as any interpreter which runs
this test suite correctly.  This new distro would be a "testing requires"
external dependency of both Muldis::D::Ref_Eng and any Parrot-hosted language
or other implementation, though conceivably either could bundle a clone of
Muldis::D::Validator rather than having an actual external dependency.
This test suite would be LGPL licensed.  This new distribution would have a
version number that is of X.Y.Z format like Muldis::D itself, where the X.Y
part always matches that of the Muldis D spec that it is testing compliance
with, while the .Z always starts at zero and increments independently of
the Muldis D spec, as often there may be multiple updates to ::Validator
for awhile between releases of the language spec, and also since .Z updates
in the language spec only indicate bug fixes and shouldn't constitute a
change to the spec from the point of view of ::Validator.
